[
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Measuring Cognitive Control in Vision-Language Models",
    "presenter": "Dezhi Luo",
    "poster_id": "A1",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Cognitive control refers to the ability to flexibly coordinate thought and action in pursuit of internal goals. A standard method for assessing cognitive control involves conflict tasks that contrast congruent and incongruent trials, measuring the ability to prioritize relevant information while suppressing interference. We evaluate 108 vision-language models on three classic conflict tasks and their more demanding \"squared\" variants across 2,220 trials. Model performance corresponds closely to human behavior under resource constraints and reveals individual differences. This results indicate that some form of human-like executive function—albeit limited—may have emerged in current multi-modal foundational models."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Situational Dimensions that drive Event Boundaries",
    "presenter": "Gabriel Arthur Daniel Kressin Palacios",
    "poster_id": "A4",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Humans maintain an internal representation of the agents, locations, goals, and causal relationships of their present situation. In the study of event cognition, two central ideas are that (i) people automatically segment continuous streams of experience into discrete event representations, and (ii) boundaries between events correspond to moments of prediction error from active event models. Here, we asked whether event boundaries could be reliably predicted directly from discontinuities in a small set of event dimensions, such as locations and goals within narrative text. We defined dimension rating criteria using event boundaries in an initial story and then applied these criteria to map discontinuities and predict event boundaries in a held-out natural text. Our decision tree model predicted the held-out event boundaries above chance. These preliminary results suggest that the detection of simple discontinuities, such as changes in location or agents within a narrative, provides a concrete and interpretable model of event boundary generation."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "The Influence of Expectations on Recurrent Processing During Challenging Image Recognition",
    "presenter": "Paulo A. Ortiz",
    "poster_id": "A5",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Recent work on challenging visual conditions has highlighted the role of recurrent processing in supporting robust perception. Predictive processing frameworks propose that prior knowledge plays a role in disambiguating sensory input, potentially reducing the need for recurrence under strong expectations. In an object recognition paradigm, we examined whether expectations influence neural processing of challenging and non-challenging visual input. Using recent adversarial attack techniques, we generated a set of perceptually challenging stimuli. We then created perceptual expectations for some stimuli by presenting them in a predictable order. Preliminary behavioral data suggested that stronger expectations enhance both the accuracy and speed of categorical judgement. We will present MEG data that will elucidate how expectations modulate temporal signatures linked to recurrent processing. This work aims to shed light on how predictive mechanisms shape the neural dynamics of robust perception."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Predictive Coding algorithms induce brain-like responses in Artificial Neural Networks",
    "presenter": "Dirk C. Gütlin",
    "poster_id": "A6",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "This study investigates whether predictive coding\n(PC) inspired deep neural networks can serve as\nbiologically plausible models of the brain. We\ncompared two PC-inspired training objectives - a\npredictive and a contrastive approach - to a\nsupervised baseline, using a simple recurrent neural\nnetwork (RNN) architecture. Our results show that,\ncompared to Supervised or Untrained models, the\nPC-inspired models exhibited more key signatures\nof PC. This includes mismatch responses (MMR),\nformation of prior expectations, and learning of\nsemantic representations. These findings indicate\nthat PC-inspired models can capture important\ncomputational principles of predictive processing in\nthe brain, and serve as a promising foundation for\nbuilding biologically plausible artificial neural\nnetworks."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Response Strategy Reverses Serial Dependence: From Attraction to Repulsion",
    "presenter": "Aviel Sulem",
    "poster_id": "A2",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Perceptual decisions are substantially biased by recent stimuli—a phenomenon termed serial dependence. Yet, what determines whether this bias is attractive (toward the previous stimulus) or repulsive (away from it) remains unclear. Previous studies focused on task structure and did not consider how individuals’ response strategies might influence bias. Here, we show that response speed—whether natural or instructed—critically shapes the direction of serial dependence. In a two-face discrimination task using a uniform morph continuum between two female identities, slower responders showed attraction, while faster responders showed repulsion. To test causality, we repeated the experiment and manipulated strategy within subjects by instructing participants to prioritize either speed or accuracy. The more participants sped up, the more repulsive their bias became. Repulsion in fast responses appeared consistent with a shift toward long-term representations—specifically, stronger attraction toward the nearest face prototype and reduced influence of the previous stimulus. These findings demonstrate that response strategy modulates serial dependence, likely by altering the balance between short- and long-term history."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Dynamics of neural representations that support generalization under continual learning",
    "presenter": "Daniel L. Kimmel",
    "poster_id": "A3",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Abstraction and generalization are essential for flexible decision-making in novel situations. Recent work in humans and monkeys has shown how abstract variables are encoded by the representational geometry of single-neuron population activity. However, these observations are typically made after learning has converged, leaving open the question of how these representations form. To address this question, we developed a factorized model of temporal abstraction that builds on the successor representation. The model disentangles the contributions of different levels of abstract learning—from stimulus-response associations to generalizable task schema—in the form of a *factorized prediction error* that relates the change in relational knowledge to a predicted change in representational geometry on each trial. We fit the model to the behavior of human participants performing a context-dependent decision task during fMRI. The model captured the learning dynamics at multiple timescales, including the increasing contribution of generalization as participants transferred abstracted relational knowledge between novel task instances. In fMRI, BOLD activity in orbitofrontal cortex, hippocampus, and amygdala correlated more with learning attributed to generalization than to the other levels of abstraction. Moreover, the relative dominance of generalization over the other levels increased across task instances in entorhinal cortex, as well as orbitofrontal cortex and hippocampus. Finally, individual variation in the generalization neural signal correlated with behavioral performance on key trials that required relational knowledge. Our findings align with recent proposals for how the brain generalizes abstracted knowledge to current task-relevant states. Our approach offers a computational framework for probing the dynamics of representational geometry under continual abstract learning."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Characterizing ADHD-Related Inhibitory Control Deficits via Multidimensional EEG Signatures",
    "presenter": "Negin Gholamipourbarogh",
    "poster_id": "A7",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Attention-Deficit/Hyperactivity Disorder (ADHD) is a neurodevelopmental condition characterized by persistent difficulties in attention regulation and behavioral control. Although electroencephalography (EEG) has provided valuable insights into the neural correlates of ADHD, capturing the full complexity of its underlying brain dynamics requires more advanced analytical approaches. In this study, we employed non-negative tensor decomposition to examine multi-dimensional EEG data collected during a Go/NoGo task. This method revealed distinct neural signatures in individuals with ADHD, particularly involving posterior alpha and theta oscillations during early attentional engagement and later stages of inhibitory control. These findings challenge the traditional focus on fronto-central theta activity and underscore the potential relevance of posterior oscillatory dynamics in the development of more targeted neurofeedback interventions for ADHD."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "From seeing to seeking: belief-based exploration in gamified environments",
    "presenter": "Thirza Dado",
    "poster_id": "A8",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Contemporary neuroscientific theories postulate that experience is shaped not only by the world as it is, but also by our beliefs about it. When beliefs conflict with incoming sensory input, this mismatch should ideally trigger belief updating. However, research on confirmation bias shows that we often discount information that contradicts our prior beliefs. Here, we study whether information sampling after belief formation is biased toward confirmation. To this end, we designed a gamified experiment in which participants decide which of two planets contains more (or fewer) stones. After an initial impression that sets their prior belief, participants continue exploring --- first by re-viewing the same visual input, then by actively seeking new evidence. This design allows us to examine how initial beliefs, along with their associated confidence, shape subsequent information gathering across different modes of interaction. This work investigates how we form, update and act on our beliefs, bridging perceptual and action-based exploration."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Valence Computation as Higher-order Inference via Conceptual Self-Processing",
    "presenter": "Yuyue Jiang",
    "poster_id": "A9",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Predictive processing models suggest that emotions arise from the hierarchical computation of prediction errors across signals. However, this framework alone cannot account for the subjective, evaluative quality of emotional experience. Here, we extend predictive processing by integrating higher-order theories of emotion, proposing that emotional valence emerges from value judgments grounded in conceptual self-processing over sensory and contextual representations. This framework offers a mechanistic account of how subjective emotional experience arises through inferences about the dynamic interplay between world- and self-models within a shared computational architecture."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Time-Resolved EEG Decoding Reveals a Flip from Enhanced Expected to Unexpected Action Outcomes",
    "presenter": "Kirsten Rittershofer",
    "poster_id": "A10",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "It is widely proposed that our brains use expectations about what is likely in order to perceive, but it remains unclear how exactly they shape perception. Bayesian accounts propose that perception is biased towards expected events, ensuring quick and veridical experiences, whereas cancellation accounts argue that unexpected inputs are perceptually prioritized because they are informative. Here, we tested a recent proposal reconciling these views, which suggests that expectations initially bias perception towards what is expected, followed by reactive enhancement of only particularly surprising inputs that are informative for model updating. Using time-resolved decoding of EEG (electroencephalography) data, we provide evidence for this account - enhanced neural representations of expected action outcomes early in time, which later reversed to favour unexpected outcomes. These results suggest that expectations make perception both veridical and informative by exerting distinct influences across time."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Modality-Agnostic Representations are Widespread Across the Cortex",
    "presenter": "Mitja Nikolaus",
    "poster_id": "A11",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Humans are able to perform tasks that require manipulation of inputs regardless of how these signals were perceived by the brain.\nThis can be achieved thanks to representations that are agnostic to the stimulus modality.\nPrevious work that attempted to localize such modality-agnostic representations has not yet led to conclusive results, with different studies proposing varying sets of candidate regions. These analyses have largely relied on relatively small-scale fMRI datasets with predefined sets of stimulus categories.\nIn our work, we leveraged a new large-scale multimodal fMRI dataset of 6 subjects watching both diverse images and short text descriptions of such images to localize modality-agnostic representations. To this end, we performed a searchlight analysis with decoders trained by mapping brain activity patterns to the latent space of pretrained deep neural networks.\nWe identified regions in which it is possible to decode both stimulus modalities in a modality-agnostic way (i.e., with a single decoder applied to brain responses from images or text).\nWe found that large areas of the brain contain modality-agnostic representations, particularly in the left hemisphere.\nOur study highlights the importance of naturalistic stimuli and large-scale datasets for insightful analyses of representations in the human brain."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Surprising narrative events elicit convergent responses in the brain, subjective reports, and large language models",
    "presenter": "Ziwei Zhang",
    "poster_id": "A13",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Linguistic surprise occurs when incoming linguistic information violates expectations formed from prior context. For example, when we hear a story, we are surprised when unfolding events do not align with our expectations. Here we ask whether large language models (LLMs) represent event-level surprise similarly to humans. To measure LLM surprise in two stories, we asked an LLM to generate text predictions as increasing amounts of context were revealed. For each story event, we operationalized LLM surprise as the dissimilarity between LLM’s internal embeddings of the predicted and actual text. We measured human surprise during the same events with self-reported ratings and predictions of a brain-based model of surprise applied to fMRI data. LLM surprise was significantly correlated with self-reported and brain-predicted surprise across events. This suggests that LLMs and humans predict the same events as surprising. Our findings highlight LLMs’ potential in modeling human surprise to narrative events."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Suboptimal human decision-making reflects an efficient information bottleneck on inference",
    "presenter": "Jacob A Parker",
    "poster_id": "A12",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Human decision-making behavior varies widely across individuals and task conditions. This variability is often interpreted as a variety of suboptimal inference strategies, but the principles that govern these suboptimalities are not well understood. We propose that one major source of variability in suboptimal decision-making reflects a specific form of bounded rationality that involves capacity-limited inference. We developed and used new theoretical and empirical approaches to study capacity-limited inference based on the information-bottleneck framework. These approaches allowed us to relate the amount of information used (capacity), to the effectiveness with which it was used (accuracy), by individual human subjects performing a variety of inference tasks. We found that substantial variability both within and across subjects reflected optimal capacity-accuracy trade-offs. Strikingly, the same capacity-accuracy tradeoffs were evident among those using heuristic (biased) inference strategies, which inherently failed to maximize performance for a given level of information use but nonetheless appeared to be implemented in a similarly capacity-limited manner. The results imply that human inference reflects consequential, and flexible, capacity limitations that impose structure on suboptimal choice behavior."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Prefrontal Representations During Learning Reflect Probabilistic Computations Across Domains",
    "presenter": "Fahd Yazin",
    "poster_id": "A15",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "The prefrontal cortex (PFC) is thought to represent abstract forms of cognitive maps or internal models during tasks. These representations could be specialized structures suited for distinct domains of experience (e.g., people vs places). Alternatively, they could represent domain-general processes rather than structure, suited for inference across domains. Here we tested these competing accounts using a learning task where human participants learned probabilistic cognitive maps in an unsupervised manner, across three domains, while performing rule classifications. During spatial, social and sequential learning, we found that the structured 1D map representations are formed in the entorhinal cortex but not in midline PFC. Instead, the PFC performs probabilistic inference, abstracting out the underlying probability distributions. Specifically, the ventromedial PFC computes data likelihood under different models, updating them through experience akin to a Bayesian learner. The anteromedial and dorsomedial PFC represent (angular) directional changes and transition distances respectively, within this abstract probability space. These findings were seen during inference as well on unseen exemplars. These results suggest that the midline PFC might be performing a domain-general computation on learned cognitive maps - probabilistic search."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Approximate Bayesian computation with a complex internal model naturally combines probabilistic inference and heuristics",
    "presenter": "Ádám Koblinger",
    "poster_id": "A14",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Proposals differ on how the brain accounts for the uncertainty of perceptual variables – either by representing them as probability distributions that explicitly encode uncertainty in their width (Knill \u0026 Pouget, 2004), or by exploiting the correlation between the uncertainty of one variable (e.g., orientation) and the value of others (e.g., contrast), using the latter’s point estimates as heuristic proxies (Bertana, Chetverikov, van Bergen, Ling, \u0026 Jehee, 2021). The two approaches offer distinct advantages – probabilistic representations provide superior data- and memory-efficiency, while proxy-based strategies impose substantially lower computational demands – and each\nhas its proponents, depending on which advantage is considered more relevant to brain function (Barthelme \u0026 Mamassian, 2010; Meyniel, Sigman, \u0026 Mainen, 2015; Koblinger, Fiser, \u0026 Lengyel, 2021). Rather than strictly contrasting these hypotheses, we follow a normative perspective and argue that both strategies can emerge naturally in a unified framework when time-evolving approximate inference is optimized to solve realistic tasks involving the joint estimation of multiple interacting variables. We formalize this idea by modeling behavior as the output of an ideal observer that combines approximate probabilistic perceptual representations with fast, coarse proxy information – yielding a flexible hybrid approach. Through simulations, we show that the model adaptively relies on proxies to compensate for the coarseness of approximate inference. Finally, by directly comparing the model’s output to empirical data, we demonstrate that observed behavior qualitatively aligns with the predictions of this hybrid model."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Bayesian modeling reveals distinct priors for tactile and proprioceptive localization",
    "presenter": "Hüseyin Orkun Elmas",
    "poster_id": "A16",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "When a mosquito lands on your finger, swatting it requires your brain to calculate its location in the external space, which depends on the body’s 3D posture. Two competing computational hypotheses explain how the brain solves this challenge: the integration hypothesis, where tactile signals are transformed into spatial coordinates by integrating touch and posture information; and the cueing hypothesis, where touch merely cues a location on the body whose position is specified via proprioception. If touch merely triggers proprioceptive localization (cueing hypothesis), both localizing touch and body parts in space should rely on the same Bayesian computations, with identical prior expectations about the mosquito’s spatial location; if they involve different computational processes (integration hypothesis), the underlying priors might differ. Twenty-one participants localized their fingers via proprioception or touch in nine hand positions. We compared Bayesian model variants with different parameter sharing structures and quantified the overlap between these processes. Models allowing different prior distributions between modalities provided the best fit for most participants. The distances between fitted spatial priors showed 15 out of 19 participants had significantly different prior distributions across modalities. Our findings provide computational evidence that tactile localization involves different processes beyond those used in proprioceptive localization, providing evidence against the cueing hypothesis."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "The effect of task switching on cognitive fatigue",
    "presenter": "Nastasia Klevak",
    "poster_id": "A17",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Little is known about how task switching moderates the relationship between cognitive effort and cognitive fatigue. To that end, we developed a novel paradigm using an objective measure of cognitive fatigue—participants' willingness to spend money on rest following periods of cognitive exertion. Across the experiment, epochs characterized by poor performance (“low-efficacy”) were followed by significantly increased cognitive fatigue. This effect was potentiated when subjects anticipated an upcoming task switch. Crucially, the impact of undergoing a task switch on subsequent performance also depended on prior task efficacy: switching into a new task improved performance after low-efficacy epochs but impaired it after high-efficacy epochs. These results use an objective measure to replicate prior findings that cognitive fatigue worsens after low-efficacy tasks. Further, they demonstrate an intriguing role for task-switching in cognitive fatigue. Task-switching is costly to performance only during high-efficacy periods; in low-efficacy periods,  undergoing a switch enhances performance. Given the relationship between performance and cognitive fatigue, this also suggests that switching into a new task while in a low-efficacy state improves fatigue. While these relationships must be further examined, our findings implicate strategic task switching as a potentially effective intervention for managing performance declines and fatigue during cognitively demanding tasks."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Sequence Models for By-Trial Decoding of Cognitive Strategies from Neural Data",
    "presenter": "Rick den Otter",
    "poster_id": "A18",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Understanding trial-by-trial variability in cognitive strate-\ngies during decision-making remains a challenge. We\ncombine hidden multivariate pattern (HMP) analysis with\na structured state space sequence (S4) model to decode\ncognitive operations from EEG data. Applied to a speed-\naccuracy trade-off (SAT) task, HMP identified an addi-\ntional Confirmation operation in accuracy-focused trials,\nbut not in speed-focused trials. Our S4 model predicts the\nprobability of the Confirmation operation occurring at the\ntrial level. We use this to show that there are speed trials\nwhere the Confirmation operation does occur, and accu-\nracy trials where it does not. This operation correlated\nwith higher accuracy and EMG-indexed changes of mind.\nThe introduced method offers a new way to detect and\nunderstand cognitive strategies in a data-driven manner."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Metacognitive insight into decision caution",
    "presenter": "Yvonne F. Visser",
    "poster_id": "A20",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Perceptual decisions are accompanied by metacognitive experiences, such as the sense of confidence, that closely follows the speed and accuracy of each decision. Confidence can inform a general sense of performance to facilitate strategic adaptation of decision-making. Metacognitive insight into specific latent decision process parameters, however, could improve such adaptation, because it would allow decision-makers to pinpoint the source of errors and adapt accordingly. Here, we assessed insight into one key parameter generally thought to be under strategic control, the decision threshold. Participants decided on the direction of a random dot motion (RDM) stimulus in two conditions (cautious versus impulsive instructions). After each decision, they rated their sense of caution. As expected, decisions were faster and less accurate in the impulsive than in the cautious condition, and metacognitive ratings of caution were sensitive to these conditions. Modeling indicated that caution ratings reflect genuine insight into the state of the decision boundary, as opposed to other latent parameters or simply tracking response times. A hierarchical DDM will be used to asses this relationship on a single-trial basis."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Pupil dilations reflect surprisal during rapid auditory belief updating",
    "presenter": "Robert Baumgartner",
    "poster_id": "A19",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Pupil responses indicate modulations of belief updating by the locus coeruleus noradrenaline arousal system during explicit decision-making. In the present study we show that pupil dilation is sensitive to ongoing surprisal during rapid, online auditory perception. A group of 22 participants indicated the final direction of auditory motion sequences with random length and directional change points in a two-alternative forced choice task design. The ongoing pupil traces were aggregated in “low” and “high” surprisal conditions derived from Bayesian model predictions. This resulted in a positive relationship between high surprisal and pupil size from 571 – 1249 ms after sound onset. We conclude a mediatory role of the arousal system for implicit belief updating during ongoing perception."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Neural correlates of prediction during naturalistic event perception",
    "presenter": "Yuye Huang",
    "poster_id": "A21",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Prediction is a critical ability for human beings and other intelligent agents. Classic reinforcement learning studies have extensively examined neural signals relating to prediction error in constrained laboratory settings, but much less is known about how people make predictions in the course of real-life events. In real-life events, predictions are complex: they combine information from multiple sources, have multiple possible outcomes, and range over a longer timescale. In this study, we leveraged narratives to study real-world prediction processes. Participants in our study made natural-language predictions during movie watching. We quantified these predictions in three different ways (accuracy, variety, and confidence), and used these variables to model functional MRI data collected as other participants watched the same movies. We found that fluctuations in prediction accuracy were tracked by activity in the default mode network, a system responsible for retrieving and integrating past information to simulate potential futures."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Estimating the Synaptic Efficacies of the Drosophila Optical Lobe Full Connectome with Predictive Coding",
    "presenter": "Rintaro Kai",
    "poster_id": "A23",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "The neural circuitry of the drosophila brain is moderately complex and has long served as a model in neuroscience research; however, many existing models of the Drosophila brain rely on extreme simplifications or biologically implausible assumptions.  \nRecently, Drosophila has gained further attention because the drosophila full connectome has been revealed.  \nIn this study, we constructed a biologically plausible autoencoder for visual processing using the complete connectome of the drosophila brain.  \nBy computing prediction errors between two anatomically closely related visual neurons, we implemented predictive coding.  \nWe also built an autoencoder with a randomly initialized connectivity matrix and found that it is harder to train than the model initialized with the real connectome.  \nThese findings suggest that the original connectome already has mechanisms akin to predictive coding.  \nWe hope that, in the future, initializing models with real connectome data will show biological characteristics that would not be shown in other initializations."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Homeostasis After Injury: How Intertwined Inference and Control Underpin Post-Injury Pain and Behaviour",
    "presenter": "Pranav Mahajan",
    "poster_id": "A22",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Several arguments suggest that the brain might have a dedicated representation of the state of injury. This would provide an internal control system to modulate behaviour given the changed homeostatic priorities associated with injury, including pain, anxiety and mood changes appropriate to the need for heightened protection and recuperation during healing. Here, we propose a computational architecture for how this might be constructed, treating the injury as a partially observable Markov decision process (POMDP), and proposing a Bayesian decision-theoretic solution that combines inference with optimal control. We show how this offers an explanation of two core paradoxical observations: behaviours such as rubbing an injured area (conventionally viewed under the lens of gate control theory), and high propensity of transition to pathological chronic pain states. Overall, this provides a quantitative framework for mapping injury homeostasis to neural substrates, with potential for identifying novel chronic pain targets. Full paper: https://www.biorxiv.org/content/10.1101/2025.02.04.636410v2"
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Leveraging Multi-task Structure for Cognitive Flexibility",
    "presenter": "Xiaoyu K Zhang",
    "poster_id": "A24",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Cognitive flexibility requires both retaining past knowledge (stability) and generalizing to new tasks. While attention mechanisms supporting this tradeoff have been studied, the complementary role of environmental structure—richness and specifically connectivity—remains underexplored. We systematically examine how these factors affect performance in MLPs and in attention-based models. Our results show that richer, more connected environments enhance both generalization and stability, especially for attention models, highlighting the importance of architecture–environment interactions in multitask learning."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Testing the predictive processing model of placebo hypoalgesia using multivariate hierarchical models: evidence for precision weighted effects of expectations and Bayesian updating of expectations accounting for volatility",
    "presenter": "Arthur S. Courtin",
    "poster_id": "A25",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Placebo hypoalgesia is often explained by predictive processing theories, in which perception arises from a form of (approximate) Bayesian integration of expectations (prior) and sensory evidence (likelihood). However, few studies have formally tested this model and uncertainty remains regarding its implementation in the nervous system. \nHere, we use a probabilistic pain learning task and computational modelling to test a series of hypotheses about how healthy volunteers form and update expectations about the painfulness of upcoming thermal stimuli and how these expectations shape the way they perceive these stimuli. Of note, our models jointly account for all response types collected during the task, constituting a first step towards a comprehensive computational model of pain perception.\nOur results support the full Bayesian predictive processing model, in which 1) the update of expectations is calibrated on different sources of uncertainty (posterior belief variance, sequence volatility), which are tracked continuously by the agent, and 2) the effect of expectations on perception is proportional to the relative uncertainty of predictions and sensory evidence (precision weighting)."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Where you look in the past biases your social inference",
    "presenter": "Sangkyu Son",
    "poster_id": "A26",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Humans are adept at inferring others' intentions by watching their actions. What do they observe while making an inference, and how would observation influence the inference?  In this study, participants played an interactive pursuit game with a computerized opponent whose hidden intentions varied. Participants formed negatively biased inferences after the opponent behaved in an unexpectedly unfriendly way. However, it did not occur after unexpectedly helpful behavior. Such asymmetrical inference persisted even after the opponent's unfriendly behavior ceased, as long as it had been building up over time—a phenomenon of history and path dependence known as hysteresis. The inferential biases were associated with gaze selection: participants looked more at the unexpectedly unfriendly opponent, even though it increased variability in their control. These findings extend action-based inference models by incorporating what we choose to observe and accounting for asymmetrical hysteresis in social understanding."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Behavioral Dynamics of Cognitive and Metacognitive Conflict",
    "presenter": "Jintao Xing",
    "poster_id": "A27",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Cognitive conflict has been widely used to investigate cognitive control mechanisms. Recent research extended this to the metacognitive level by defining metacognitive conflict as the inconsistency between external feedback and confidence. The current work investigates interaction between both types of conflict using a modified Simon task, tracking how and whether cognitive and metacognitive conflict jointly shape cognitive performance and confidence. The results revealed significant but independent effects of adaptation to cognitive conflict and adaptation to metacognitive conflict. Model-based analysis further revealed opposite effects of post-decision biased accumulation: biased towards high confidence after incongruent trials and biased towards low confidence after metacognitive conflict trials. Together, these results highlight independent yet parallel mechanisms through which different types of conflict shape decision making.\n\nKeywords: Conflict; Confidence; Decision making; Metacognition"
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "How Does an LLM Process Conflicting Information In-Context?",
    "presenter": "Ivan Andre Naranjo Coronel",
    "poster_id": "A29",
    "topic_area": "Language and Communication",
    "abstract": "Large language models (LLMs) gain understanding from vast training datasets during the pretraining phase. Although prior research has examined how these models store knowledge, how they distinguish between accurate and false information in context is yet to be explored. In this paper, we presented LLMs with correct and false information in context and prompted them to discriminate between the two. To understand which model components carry out this ability, we performed activation patching. We showed in detail, how much different model components contribute to this behavior. Furthermore, we analyzed how prompt order and content affect our patching results. Overall, we reveal which model components separate factual from false information. We intend to advance this study by investigating how these results hold up under different influences."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
    "presenter": "Badr AlKhamissi",
    "poster_id": "A28",
    "topic_area": "Language and Communication",
    "abstract": "Large language models (LLMs) exhibit remarkable similarity to neural activity in the human language network. However, the key properties of language underlying this alignment---and how brain-like representations emerge and change across training---remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. Specifically, we find that brain alignment tracks the development of formal linguistic competence---i.e., knowledge of linguistic rules---more closely than functional linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting that the human language network primarily encodes formal linguistic structure rather than broader cognitive functions. Notably, we find that the correlation between next-word prediction, behavioral alignment, and brain alignment fades once models surpass human language proficiency. We further show that model size is not a reliable predictor of brain alignment when controlling for the number of features. Finally, using the largest set of rigorous neural language benchmarks to date, we show that language brain alignment benchmarks remain unsaturated, highlighting opportunities for improving future models. Taken together, our findings suggest that the human language network is better modeled by formal than functional aspects of language."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Vowel Duration Is Encoded Relative to the Contextual Speech Rate Throughout Cortical Auditory Processing",
    "presenter": "Mara Wolter",
    "poster_id": "A31",
    "topic_area": "Language and Communication",
    "abstract": "Across languages, listeners perceive durational features in speech relative to the syllable rate of the surrounding context. Despite the importance of this mechanism, it remains unclear how the cortex normalizes sound durations during speech perception. Here we used MEG to investigate rate normalization for vowel duration in German, where the contextual speech rate can decisively shift listeners’ word perception  (e.g., satt – ˈzat – “full” vs. Saat – ˈzaːt – “seed”). We found that rate influences evoked activity before, during and after the target vowel, across auditory, motor and inferior frontal regions. Vowel duration effects from ~130 ms post vowel offset always co-occurred with rate effects. This spatiotemporal overlap suggests that vowel duration is encoded relative to contextual speech rate throughout cortical processing, and that contextual rate normalization is an integral part of duration encoding."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "(Dis)similarities in Temporal Neural Dynamics during Reading and Listening",
    "presenter": "Jiawei Li",
    "poster_id": "A32",
    "topic_area": "Language and Communication",
    "abstract": "When reading or listening to a story, similar understanding occurs despite different input modalities—visual for reading and auditory for listening. However, how the brain converts modality-specific stimuli into semantic representations is unclear. To determine the timing of the underlying processes, we recorded EEG data while human participants read or listened to. We then used visual, auditory, and semantic embeddings in encoding models to predict brain activity to words. Models trained on the reading and listening data respectively revealed similar processing time courses, with significant encoding peaks around 250 ms. Encoding models trained on one modality and tested on another again had a similar time course, but unexpectedly with flipped results and negative predictions. These results suggest that reading and listening share a common temporal trajectory for encoding semantic representations, but the underlying mechanisms may differ."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Cortical representations are similar across English and Chinese for both concrete and abstract concepts",
    "presenter": "Mathis Lamarre",
    "poster_id": "A30",
    "topic_area": "Language and Communication",
    "abstract": "Concreteness, the degree to which a concept refers to a perceptible entity, plays a key role in psycholinguistic models of bilingualism. They suggest that concrete concepts are more similarly represented across languages than abstract concepts. Yet, most of these studies used only behavioral data and controlled stimuli. Thus, it is unclear how similarity of representations across languages relates to concreteness in the brain of bilingual speakers. To address this question, we analyzed functional magnetic resonance imaging (fMRI) recordings of Chinese-English bilinguals reading narratives translated in both languages using voxelwise encoding models. We used semantic features aligned between English and Chinese. We measured whether each voxel represents more concrete or abstract concepts in each language, and how similar these concepts are across languages. Our results show that similarity of representations across English and Chinese is high for both concrete and abstract concepts."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Uncovering Linguistic Representations in MEG Data Using Deep Learning",
    "presenter": "Nikola Kölbl",
    "poster_id": "A34",
    "topic_area": "Language and Communication",
    "abstract": "The ability to use complex language is uniquely human\nand underpins abstract thought, cultural transmission,\nand the structure of society. Understanding its neural\nbasis in naturalistic settings remains a major challenge.\nIn this study, we investigate whether word classes can\nbe decoded from low-dimensional MEG data recorded\nduring audio book listening. Using a minimalist modeling\napproach, we trained neural networks on individual\nMEG channels and identified peak classification performance\nover left frontal sensors, consistent with the involvement\nof Broca’s area in grammar and predictive processing.\nAs a proof of concept, we applied sequential\ndeep dreaming to reveal prototypical neural patterns for\nnouns and verbs. While the results demonstrate feasibility,\nlimitations due to data sparsity, class imbalance\nand single-subject design highlight the need for broader\nvalidation. Our approach represents a first step towards\ninterpretable decoding of linguistic structure from MEG\nduring natural, continuous speech comprehension."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants",
    "presenter": "Louis Jalouzot",
    "poster_id": "A35",
    "topic_area": "Language and Communication",
    "abstract": "We investigate optimal strategies for decoding natural speech from fMRI data with limited participants. Using data from Lebel et al. (2023) from 8 participants, we show that deep neural networks can effectively predict LLM-derived text representations with performance directly scaling with the amount of training data. Then, in this data regime, we observe that multi-subject training does not improve decoding accuracy compared to a single-subject approach. Furthermore, we find that our decoders better model syntactic than semantic features. Our results highlight deep phenotyping benefits and suggest multi-subject decoding needs more data per subject or a substantially larger cohort."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Multilingual Computational Models Reveal Shared Brain Responses to 21 Languages",
    "presenter": "Andrea Gregor de Varda",
    "poster_id": "A36",
    "topic_area": "Language and Communication",
    "abstract": "How does the human brain process the rich variety of languages? Multilingual neural network language models (MNNLMs) offer a promising avenue to answer this question by providing a theory-agnostic way of representing linguistic content across languages. We combined existing and newly collected fMRI data from speakers of 21 languages to test whether MNNLM-based encoding models can predict brain activity in the language network. Across 20 models and 8 architectures, encoding models successfully predicted responses in the various languages, replicating and extending previous findings. Critically, models trained on a subset of languages generalized zero-shot to held-out ones, even across language families. This cross-linguistic generalization points to a shared component in how the brain processes language, plausibly related to a shared meaning space."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Intermediate Layers of LLMs Align Best With the Brain by Balancing Short- and Long-Range Information",
    "presenter": "Michela Proietti",
    "poster_id": "A33",
    "topic_area": "Language and Communication",
    "abstract": "Contextual integration is fundamental to human language comprehension. Language models are a powerful tool for studying how contextual information influences brain activity. In this work, we analyze the brain alignment of three types of language models, which vary in how they integrate contextual information. Despite differences among models, we find minimal variations in their brain alignment. In line with previous research, middle layers consistently show the highest correspondence with brain activity. Interestingly, this alignment appears to strengthen with longer context inputs, pointing to improved sensitivity to extended linguistic information. To better understand how contextual integration affects brain alignment, we analyze the roles of short- and long-range context using variance partitioning. Our findings highlight a functional distinction between layers, suggesting a trade-off between retaining local detail and integrating broader context. This interplay may explain the robust alignment of middle layers with brain responses."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Not all induction heads are created equal",
    "presenter": "Eric Schulz",
    "poster_id": "A38",
    "topic_area": "Language and Communication",
    "abstract": "Induction heads are specialized Transformer heads thought to be central for in-context learning. These heads work by having a token $x$ attend to tokens that \\emph{succeeded} $x$ in the past,  allowing Transformers to predict repetitive structures in the input prompt. When training large-scale Transformer models on text corpora, multiple such heads emerge. In this paper we show that induction heads in a Large Language Model (Qwen2.5-1.5B) exhibit diverse and context-dependent strategies for attending to these successor tokens when there are multiple successor candidates that can be attended to. Some heads prefer to attend to the very \\emph{last} successor token, others to the very first. Some heads even \"learn\" to incorporate second-order contextual cues (e.g. what tokens preceded $x$) to attend to the successors that are actually predictive of future tokens. Overall, our findings show that induction heads are more sophisticated than previously believed,  implement context-dependent computations for predicting future tokens based on patterns observed in the past."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Temporal Straightening as a Predictive Mechanism in Human Language Processing",
    "presenter": "Jiaming Xu",
    "poster_id": "A39",
    "topic_area": "Language and Communication",
    "abstract": "Predicting what comes next is central to how humans process language, and to how artificial language systems, trained on next-word prediction, learn flexible representations that support diverse tasks. Despite the importance of prediction in both systems, the neural mechanisms underlying prediction in the human brain remain poorly understood. Inspired by the temporal straightening hypothesis from vision neuroscience, we investigated predictive representations in language processing from a geometric perspective. This hypothesis proposes that the brain transforms complex inputs to follow straighter temporal trajectories in representational space, enabling prediction through linear extrapolation. Here, we tested whether a similar principle applies to the human language system. Using fMRI data from subjects listening to a natural spoken narrative, we estimated representational timescale as a proxy for trajectory straightness across regions in the language processing hierarchy. We found that timescale increased in higher-order regions, indicating that neural trajectories become progressively straighter along the hierarchy. These findings offer a new perspective on predictive mechanisms in language, suggesting that temporal straightening may serve as a general organizing principle across different systems."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Neural Signatures of Argument Structure Constructions",
    "presenter": "Pegah Ramezani",
    "poster_id": "A37",
    "topic_area": "Language and Communication",
    "abstract": "Understanding how the brain processes language, particularly abstract grammatical structures like Argument Structure Constructions (ASCs), is a key goal in cognitive neuroscience. Exploring how the brain differentiates these constructions helps uncover the neural basis of language comprehension.\nTo investigate this, EEG data was recorded from 12 native English speakers as they listened to sentences representing each ASC type. Analysis of neural signals revealed distinct patterns linked to specific constructions. Significant differences emerged in comparisons between several pairs, especially between transitive and resultative and caused-motion and ditransitive. Other comparisons showed weaker or no differentiation. Machine learning classification supported these findings, identifying construction-specific neural signatures.\nAlthough individual variation existed, the Alpha frequency band consistently played the most prominent role in distinguishing constructions, followed by Beta and Delta bands, with Gamma showing minimal impact.\nThese results demonstrate that the brain processes grammatical constructions in distinct ways, challenging the notion of uniform syntactic processing. The findings highlight how neural oscillations, particularly in the Alpha band, are sensitive to grammatical patterns, deepening our understanding of the neural and cognitive architecture underlying language comprehension."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "The developmental trajectory and neuroanatomical correlates of speech comprehension",
    "presenter": "Jill Kries",
    "poster_id": "A41",
    "topic_area": "Language and Communication",
    "abstract": "Language acquisition spans early childhood into young adulthood. Neural mechanisms underlying this prolonged development, especially within naturalistic speech comprehension settings, remain understudied. Here, we used intracranial EEG recordings from 42 children with epilepsy while they listened to natural speech. We reconstructed acoustic and symbolic features from the neural data and found significant above-chance decoding in 77% of children. An ascending pattern of across-age feature decoding indicates later development of more symbolic features. The features seem to be co-encoded in temporal regions. These findings provide first insights into the language hierarchy development and the neuroanatomical correlates of speech comprehension throughout childhood."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Can LLMs inform us about predictive processing during natural listening in MEG?",
    "presenter": "Sahel Azizpour",
    "poster_id": "A42",
    "topic_area": "Language and Communication",
    "abstract": "The brain uses contextual information and prior knowledge to anticipate upcoming content during language comprehension. Recent research has shown that predictive signals can be revealed in pre-onset electrocorticography (ECoG) activity during naturalistic narrative listening, by building encoding models based on word embeddings from large language models (LLMs). Similarly, evidence for long-range predictive encoding has been observed in functional magnetic resonance imaging (fMRI) data, where incorporating embeddings for multiple upcoming words in a narrative improves alignment with brain activity. This study examines whether similar predictive information can be detected in MEG, a technique with higher temporal resolution than fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate that MEG captures pre-onset representations up to 1 second before word onset, consistent with ECoG results. However, unlike fMRI findings, incorporating future word embeddings did not enhance encoding in MEG, not even for one word into the future, which suggests that the pre-onset encoding may not reflect predictive processing. This work demonstrates that MEG combined with LLMs is a valuable approach for studying language processing in naturalistic narratives and highlights the need to study further what constitutes evidence for prediction during natural listening."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "The Representational Alignment between Humans and Language Models is implicitly driven by a concreteness effect",
    "presenter": "Cosimo Iaia",
    "poster_id": "A43",
    "topic_area": "Language and Communication",
    "abstract": "The nouns of our language refer to either concrete entities (like a table) or abstract concepts (like justice or love). Cognitive psychology has established that concreteness influences how words are processed. Accordingly, understanding how concreteness is represented in our mind and brain is a central question in psychology, neuroscience, and computational linguistics. While the advent of powerful language models has allowed for quantitative inquiries into the nature of semantic representations, it remains largely underexplored how they represent concreteness. Here, we used behavioral judgments to estimate semantic distances implicitly used by humans, for a set of carefully selected abstract and concrete nouns. Using Representational Similarity Analysis, we find that the representational similarity space of participants and the semantic representations of language models are significantly aligned and that both are implicitly aligned to an explicit representation of concreteness, which was obtained from our participants using an additional concreteness rating task. Importantly, using ablation experiments, we demonstrate that the human-to-model alignment is substantially driven by concreteness--not by other important word characteristics established in psycholinguistics, such as word frequency."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT’s Representations",
    "presenter": "Robin Sobczyk",
    "poster_id": "A40",
    "topic_area": "Language and Communication",
    "abstract": "We introduce Metric-Learning Encoding Models (MLEMs), a new framework to learn a feature-based metric explaining the geometry of neural representations. Applying MLEMs to BERT, we track various linguistic features (e.g., tense, subject number) and find distinct importance profiles across layers. For a given layer, feature importance ranking corresponds to a hierarchical geometry of representations. A univariate variant of our model reveals remarkable spontaneous disentanglement: in all layers, distinct neuron groups specialize in encoding single, specific linguistic features. MLEMs are more robust than popular decoding methods, offering a powerful tool for analyzing representations in artificial and biological neural systems."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Finding Modularity in Large Language Models: Insights from Aphasia Simulations",
    "presenter": "Jixing Li",
    "poster_id": "A44",
    "topic_area": "Language and Communication",
    "abstract": "Recent large language models (LLMs) excel at complex linguistic tasks and share computational principles with human language processing. However, it remains unclear whether their internal components specialize in distinct functions, such as semantic and syntactic processing, as seen in humans. To explore this, we selectively disrupted the components of LLM to replicate the behavioral patterns of aphasia--a disorder characterized by specific language deficits resulting from brain injury. Our experiments revealed that simulating semantic deficits akin to Wernicke’s aphasia was relatively straightforward, whereas reproducing syntactic deficits characteristic of Broca’s aphasia proved more challenging. These results highlight both parallels and divergences between the emergent modularity of LLMs and the human language system, offering new insights into information representation and processing in artificial and biological intelligence."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Temporal dynamics of natural sounds representation in the human brain",
    "presenter": "Marie Plegat",
    "poster_id": "A45",
    "topic_area": "Language and Communication",
    "abstract": "Acoustic and semantic representations involved in the temporal dynamics of the cerebral processing of natural sounds are often studied separately. As a consequence, we lack direct knowledge of how the human brain transforms complex acoustic waveforms into semantic representations of the acoustic environment. Here, we aimed to elucidate this process by predicting magnetoencephalographic (MEG) responses to natural sounds using acoustic, and semantic (text-based) models. Critically, we also consider two recently developed sound-processing convolutional neural networks (CNNs) that differ only in their loss function: CatDNN, which learns sound-event categories, and SemDNN, which learns continuous semantic embeddings (Word2Vec). We observe that DNNs better predict the dynamic MEG response, except at a long latency (800--1000 ms) where higher-level acoustics seems to dominate (auditory dimensions). Focusing on DNNs, we observe a potential switch from initial protoacoustic/categorical semantic representations (CatDNN, 250 ms) to more refined continuous semantic representations (SemDNN, 500--800 ms). Overall, our findings suggest limitations in the text-based modeling of the cerebral representations of natural sounds, and give a temporally resolved description of the cerebral dynamics of the acoustic-to-semantic transformation."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Many-to-Many, Yet Convergent: Insights into the alignment of Vision and Language Models",
    "presenter": "Zoe Wanying He",
    "poster_id": "A46",
    "topic_area": "Language and Communication",
    "abstract": "The “platonic representation” hypothesis holds that vision and language models converge on a shared conceptual space despite being trained on distinct modalities. Yet, much of the evidence for this hypothesis comes from one-to-one image–caption scenarios, where each image is paired with a single descriptive caption. This setup overlooks a fundamental reality: the mapping between images and language is many-to-many, as neither modality uniquely determines the other. In this work, we show that alignment between vision and language models also persists at a finer grain in such many-to-many contexts. Using a forced-choice “Pick-a-Pic” task, we find that human raters’ preferences for which of two images better matches a caption are mirrored in the learned embedding space across all vision-language model pairs. This evidence challenges the simplistic view of “one image, one caption” alignment and highlights that models capture finer-grained semantic distinctions akin to human preferences. Moreover, we demonstrate that averaging embeddings across multiple images and multiple captions referring to a shared concept yields significantly stronger alignment than individual image–caption pairs. While one might expect averaging to “blur” representational detail, our results reveal the opposite: aggregating multiple views appears to distill a more universal semantic core. Our findings ultimately reinforce the notion of a shared conceptual space across modalities, underscoring the importance of examining many-to-many correspondences to better understand how such models learn, represent, and unify semantic information."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Working Memory Reactivation Across Embedded Structures during Natural Language Comprehension",
    "presenter": "Jiaqi Li",
    "poster_id": "A47",
    "topic_area": "Language and Communication",
    "abstract": "A key feature of human language is recursion, involving hierarchical embedding of clauses– a process dependent on working memory (WM). During speech comprehension, listeners must maintain earlier linguistic elements for later integration. Even processing adjacent words into phrases requires WM resources (Desbordes et al., 2024), whereas the integration across embedded structures depends more on the flexibility of the WM system. However, the neural mechanisms underlying how WM supports the processing of complex recursive linguistic structures remain unclear. We constructed English sentences with embedded language structures and recorded magnetoencephalography (MEG) signals while 34 native speakers listened to these sentences. Neural decoding results demonstrate that during embedded structure processing, previously encoded information is stored in an activity-silent state until the non-adjacent verb of the main clause triggers reactivation. Source-level analysis reveals that the reactivation first occurs in prefrontal regions followed by activation in the temporal cortex. This study provides crucial insights into the temporal and spatial dynamics of WM functions required for unification operations across embedded structures."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Behavioral relevance of high-dimensional neural representations",
    "presenter": "Chihye Han",
    "poster_id": "A49",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "A common approach to understanding the organizing principles of neural representations has been to emphasize high-variance dimensions that correspond to interpretable features. Here, we investigate whether behavioral relevance is restricted to these interpretable dimensions or spans the entire spectrum of neural representations. Using fMRI data from the Natural Scenes Dataset, we tested whether humans could perceive coherent structure in image clusters formed along principal components of ventral visual stream responses, where explained variance decreases by orders of magnitude across principal-component ranks. In this initial study examining the first two decades of neural dimensions, we found that behavioral relevance extends throughout the entire range tested. These findings suggest that behaviorally relevant information in neural representations extends beyond the interpretable, high-variance dimensions emphasized in standard approaches and that comprehensive models of neural coding should account for the full range of dimensions."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Feature-based Modeling of Visual Attention in Autism:  A Large-scale Online Eye-tracking Study",
    "presenter": "Qianying Wu",
    "poster_id": "A50",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Atypical visual attention is one of the most reliable findings in autism spectrum disorder (ASD), with important implications for clinical screening and diagnosis. However, most findings rely on artificial stimuli and small samples, limiting generalizability. In this pre-registered study, we used webcam-based eye-tracking and  feature-based computational modeling  to characterize visual attention in a broad sample of 336 ASD participants and 304 neurotypical controls. Participants watched videos of group conversations that incorporated controlled social and nonsocial features. Compared to controls, autistic individuals showed reduced attention to speakers, increased sensitivity to distractors, and more frequent gaze shifts. Our study demonstrates the power of scalable online eye-tracking and modeling approaches for capturing individual differences in visual attention and advancing the understanding of heterogeneity in ASD."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "A Virtual Manipulation Task to Probe Human Interactions with Diverse Physical Objects",
    "presenter": "Giacomo Aldegheri",
    "poster_id": "A51",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Humans are able to successfully interact with objects with vastly different physical characteristics. For example, hard objects, such as boxes, and soft objects, such as cloth or rope, behave in entirely different ways when manipulated. We designed a controlled yet ecological behavioral task to probe how human subjects interact with objects with diverse physical characteristics. Subjects are shown, on a computer screen, an 'arena' including an object, a simple manipulator (a rectangular pusher) and a goal indicated by a location within the arena. Using the keyboard, they can rotate and translate the pusher, and their task is to use it to push the object into the goal. On each trial, the object to be moved belongs to one of three categories: box, rope, or cloth. This task allows systematically comparing human behavior across object categories in a simplified scenario, providing an ideal testing ground for computational models of object manipulation and intuitive physics."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Decoding Semantics: A Multi-Modal CNN as a Model for Human Literacy Acquisition",
    "presenter": "Tommy Clausner",
    "poster_id": "A48",
    "topic_area": "Language and Communication",
    "abstract": "While visually presented objects (e.g. a picture of a rat) and words (e.g. the word *rat*) appear perceptually different, they evoke a similar semantic activation in the human brain. A key question in understanding human reading acquisition is how semantic representations emerge such that visual object representations and written words are meaningfully linked. We used a convolutional neural network (CNN), trained such that both object images and written word stimuli activate the same output unit. Our findings indicate, that cross-modal semantic representations emerge gradually across layers. Using representational similarity analysis of the layer activations, we further were able to show, that incongruent information affects the network’s performance via interfering projections to a high dimensional space. This suggests that the acquisition of literacy can be modelled as the projection of object and word features, processed via the same neuronal substrate - the visual cortex - into a shared semantic space. Our approach offers a new avenue to uncover the neuronal substrate of human literacy acquisition by using representational similarity analysis to link representations in the CNN to brain imaging data."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Evaluating Model-to-Human Alignment on Image Occlusion",
    "presenter": "Ehsan Tousi",
    "poster_id": "A52",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Object recognition under occlusion is a significant challenge for both artificial and biological visual systems. We compared human performance with that of deep neural networks on a rapid object categorization task using systematically occluded natural images. Vision Language Models (VLMs) not only matched but exceeded human performance on heavily occluded images. In contrast, most Vision Only models showed steep performance drops. Confusion matrix analyses revealed that VLMs make semantically meaningful errors similar to humans, whereas Vision Only models show systematic biases toward specific categories regardless of input. VLMs' integration of linguistic context appears to enable more human-like inference of occluded object parts, suggesting a more object-centric approach compared to traditional pixel-based models. These results highlight the importance of multi-modal integration in developing more human-aligned visual recognition systems that maintain robustness under challenging viewing conditions."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Orientation Bias and Abstraction in Working Memory: Evidence from Vision Models and Behaviour",
    "presenter": "Fabio Bauer",
    "poster_id": "A53",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Remembering visual orientations involves systematic biases in human working memory. We tested whether vision models exhibit analogous orientation biases when exposed to rotated real-life objects and if they represent orientations independent of object identity. Using representational similarity analysis (RSA), we compare the representational geometries from eight vision models and human behavioral reports to theoretical patterns of orientation encoding  and bias. Our analysis differentiated between two representational domains: the 180° space (2-fold rotationally symmetric), and the 360° space (distinguishing 'up' vs. 'down'). To examine the extent to which  orientation representations generalized, we compared artificial neural network (ANN) activations within- and between-objects. We found that vision models showed orientation encoding in 180° space and exhibited a pronounced attraction bias, unlike the characteristic repulsion effects observed in human participants. Further, the vision models display limited 360° orientation encoding, with inadequate cross-object generalization. In contrast, human working memory reports readily reflected orientations in 360° in a generalized fashion. Thus, while contemporary vision models can represent stimulus-specific orientation information, they fail to replicate abstract object-independent orientation encoding and bias that humans effortlessly achieve. Our findings underscore critical limitations of current vision models for studying visual working memory processing."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Glimpse prediction fosters graph-oriented scene representations aligned with the ventral visual cortex",
    "presenter": "Sushrut Thorat",
    "poster_id": "A55",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Understanding how the visual system responds to natural scenes remains a central challenge in vision science. Research shows that the ventral visual cortex (VVC) encodes objects, textures, and the spatial and semantic relationships between them—forming a structured scene representation, akin to a scene graph. However, inferring such representations from images in an interpretable, image-computable way is still an open problem.\nWe propose glimpse prediction—predicting the upcoming visual input given an eye movement (saccadic efference copy)—as a training objective that encourages the emergence of representations with graph properties in artificial neural networks. A recurrent neural network trained with this objective learns spatial covariance between glimpses, across scenes (in‑weight learning) and in novel scenes (in‑context learning). Importantly, the model’s internal representations align closely with VVC responses to natural scenes (Natural Scenes Dataset), despite never observing the full scene or receiving explicit semantic labels.\nThus, glimpse prediction offers a principled route to building graph-oriented representations mirroring those in the human ventral visual stream. Combining interpretable concepts from cognitive neuroscience and image‑computable neuroconnectionist models, this work advances a comprehensive understanding of the visual system's response to natural scenes."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "How Decision-Process Information Shapes Inferences in Cooperative Interactions",
    "presenter": "Mrugsen Nagsen Gopnarayan",
    "poster_id": "A56",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Eye movements reveal attentional processes\nunderlying decisions, potentially enabling observers\nto infer hidden preferences in social interactions. We\ntested whether real-time gaze information improves\npreference inference during an iterative bargaining\ntask, in which 79 pairs of participants were assigned\nto either a control group or an attention group, the\nlatter having access to live eye tracking of buyers’\nfixations. Sellers adjusted subsequent offers based\non buyers’ response times. Buyers’ first fixations\nsignaled attribute importance, and sellers in the\nattention group were sensitive to this information.\nThis benefit, however, did not result in higher\nearnings than in the control condition. To\nunderstand how sellers learned buyers’ preferences,\nwe developed a Bayesian learning model for the\nseller, its results suggest that sellers make a trade\noff between maximizing utility and making offers\nthat reveal more information. These findings\nhighlight that real-time attentional cues can reveal\npreference signals but may be too complex to utilize,\ninforming our understanding of attention and\ndecision making in social contexts."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Five animacy dimensions and the CLIP model explain complementary components of visual representational dynamics and similarity judgments",
    "presenter": "Kamila Maria Jozwik",
    "poster_id": "A54",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Distinguishing animate from inanimate things is important for object recognition behaviour and animate and inanimate objects elicit distinct brain and behavioural responses. A recent study evaluated the importance of five object dimensions related to animacy (“being alive”, “looking like an animal”, “having agency”, “having mobility”, and “being unpredictable”) in brain representations and similarity-judgement behaviour. The study introduced a stimulus set that decorrelated these dimensions based on human ratings. Here, we ask: 1) to what extent one of the best computational models of vision (Contrastive Language-Image Pre-Training (CLIP) RN50) can predict dynamic human brain (EEG) and similarity judgement responses to this stimulus set and 2) what unique variance is explained by each animacy dimension ratings and CLIP. We find that CLIP explains a unique portion of the variance of similarity judgements, and a similar total amount of the variance as human ratings for each of the animacy dimensions. EEG responses are also predicted by animacy dimension ratings and CLIP to a similar extent. However, CLIP explains a unique portion of this variance at short latency (140-196 ms after stimulus onset), whereas “looking like animal” dimension rating explains unique variance at longer latency (239-301 ms after stimulus onset). We conclude that both human-generated multi-dimensional animacy ratings and the CLIP model explain unique components of visual representational dynamics and similarity-judgement behaviour and provide insights about specific dimensions of animacy that need to be better captured in future computational models of brain function and behaviour."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Comparing Object Selectivity in Visual Cortex and Topographic Deep Artificial Neural Networks",
    "presenter": "Davide Cortinovis",
    "poster_id": "A57",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "The occipitotemporal cortex (OTC) exhibits category selectivity, with specialized regions responding to specific object categories. Topographic Deep Artificial Neural Networks (TDANNs) have been proposed as mechanistic models of this spatial and functional organization. However, a direct comparison of the visual and semantic features driving functional selectivity in the two systems is lacking. We analyzed fMRI data from three participants viewing 200 images of distinct body parts and inanimate objects, and compared OTC selectivity with TDANN activations. Body-, hand-, and tool-selective regions all showed strong category preferences. TDANNs displayed similar, though weaker, selectivity with blurrier category boundaries, especially for tools. Texture scrambling revealed that TDANN selectivity partly relies on local features: body and hand selectivity persisted despite global shape disruption, while tool selectivity disappeared, possibly due to their higher similarity with the other inanimate categories. These results represent a first step toward better characterizing and comparing functional selectivity in visual cortex and topographic models."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Learning task-relevant visual features from large language model (LLM) embeddings",
    "presenter": "Michelle R. Greene",
    "poster_id": "A58",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Complete visual understanding requires an analysis of the environment’s features while considering the observer's goals. The visual system is not a passive feature extractor. Instead, perception constructs representations that are tailored to behavioral needs. While identifying task-relevant visual features in complex scenes is challenging, advances in AI may offer new opportunities to address these challenges. We asked observers to describe the same set of scenes with two different goals: to provide a general scene description or to describe the possible walking paths through the scene. We converted these descriptions to sentence embeddings and trained convolutional neural networks (CNNs) to learn the embeddings. We included the embeddings of scenes’ basic-level categories as a baseline. Using deconvolution, we generated activation maps to reveal the image areas that contain task-relevant semantic information. All networks generated maps that were unique from each other and to a CNN pretrained for scene classifications. The maps generated from the navigation task contained higher activation to the ground plane compared to the description map. We validated the image information by showing human participants (N=60) partial image views that contained either the top or bottom quarter of activated pixels for each network while they performed a three alternative forced choice (3AFC) task requiring either categorization or navigation. Higher activation pixels led to better performance, indicating that task-relevant scene features can be learned directly from written descriptions."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Distinct Computational Mechanisms Underlie Holistic Processing of Faces and Non-Face Line Patterns",
    "presenter": "Elaheh Akbari-fathkouhi",
    "poster_id": "A60",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Holistic processing is the tendency to perceive\nobjects as unified wholes. A hallmark is the\ncomposite effect—combining the top half of one\nobject with the bottom half of another creates a\nnovel percept that disappears when misaligned.\nAlthough traditionally considered face-specific or\nexpertise-based, recent findings show that even\nunfamiliar line patterns can be processed\nholistically, raising the question: Do these\nprocesses rely on similar or distinct mechanisms?\nTo find out, we used three convolutional neural\nnetworks (CNNs)—one trained on object\ncategorization, one trained on face identification and\none untrained—and tested them with faces and line\npatterns, mirroring human studies. The composite\neffect for faces emerged only in the face-trained\nCNN and was disrupted by inversion, suggesting a\nface-specific mechanism. In contrast, line patterns\nelicited a composite effect across trained CNNs\nregardless of inversion, pointing to a\ndomain-general process. Notably, holistic\nprocessing for faces peaked at later processing\nstages than for line patterns. Our results suggest\nthat distinct mechanisms underlie holistic\nperception for faces and line patterns in CNNs and,\nwe conjecture, also in the human brain."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Object Position, But Not Identity, Is Decodable During Object Permanence with MEG",
    "presenter": "Apurva Ratan Murty",
    "poster_id": "A59",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "When objects are dynamically occluded, we maintain knowledge of their existence—a phenomenon known as object permanence. Despite extensive research on object permanence, the nature of neural object representations under occlusion remains unclear. Specifically, does the neural code maintain perceptual-rich features, or does it shift toward a sparse, conceptual format? Here, we measured dynamic neural representations using MEG in an ecological valid setting. Participants viewed one of five objects (e.g. bike, chair) moving either unobstructed or behind a wall, with the knowledge that the object would either disappear or reappear, while performing a speed-change detection task. Using MVPA decoding, we were able to decode object identity when the object was unoccluded. Throughout occlusion, however, we could decode object positions but not object identity. Notably, when the object was predicted to disappear during occlusion, the neural position signal was traceable only until the moment of vanishing. Our findings suggest that during occlusion, neural signals may reflect simulated spatial information rather than detailed perceptual features, thereby placing important constraints on computational models of object perception in the brain."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Can Scene Graph Properties Explain the Neural Encoding Performance of Vision Transformers?",
    "presenter": "Helena Balabin",
    "poster_id": "A61",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Neural encoding models allow for the exploration of hypotheses about cognitive processes by linking brain activations to representations derived from large language or image models. However, such representations often remain poorly understood, limiting the interpretability of neural encoding models. Therefore, we set out to examine the effect of scene graph properties on image model representations and neural encoding performances to functional magnetic resonance imaging (fMRI) data from the Natural Scenes Dataset (NSD). Specifically, we used the overlap between the NSD and the Visual Genome to characterize each image using the number of relationships, objects and depth of the accompanying scene graph annotations. We found that relationships and depth measures could be decoded more accurately both from fMRI activations and from image embeddings compared to objects, aligning with an afforance-based scene perception approach."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Prior Scene Context Modulates the Dynamic Interplay Between Bottom-Up and Top-Down Neural Processes in Face Detection",
    "presenter": "Sule Tasliyurt Celebi",
    "poster_id": "A63",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Within a fraction of a second, we detect faces in our environment. How is this remarkably fast process implemented in the brain, and is it modulated by top-down mechanisms? Here, we used electroencephalography (EEG) to probe how prior scene context shapes temporal dynamics of neural face representations in natural settings. Participants viewed images of natural scenes containing a single face (on the left or right) that followed either a faceless preview (preview condition) or a gray screen (no-preview condition), while performing a face detection task (~10% foils). Using MVPA decoding, we were able to decode the face location (left vs. right) shortly after target onset. Critically, decoding accuracy of face location was initially higher in the preview condition, while the no-preview condition showed increased accuracy at later processing stages. Moreover, time-frequency analyses showed an enhanced decodability of face location in the preview condition in the alpha band (8–13 Hz), consistent with enhanced spatial orienting. Our findings suggest that prior scene context modulates face detection via distinct neural mechanisms that affect both bottom-up sensory integration and top-down spatial attention, thereby highlighting the dynamic interplay between contextual cues and neural processing. \nKeywords: face perception; time-frequency analysis; top-down processing; MVPA; EEG"
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "The human superior colliculus encodes looming- and object-related visual threat",
    "presenter": "Monica Thieu",
    "poster_id": "A62",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "The superior colliculus is an evolutionarily old midbrain structure involved in vision, attention, and motor control, enabling it to rapidly coordinate defensive responses to approaching threats. It remains unclear whether the human superior colliculus functions as a rudimentary threat detector, or if it uses highly processed information from cortex to facilitate threat processing. Here, we used convolutional neural networks and fMRI to characterize superior colliculus responses to naturalistic videos. We found that the human superior colliculus encoded visual looming and static object features, both of which were related to subjective fear ratings. Connectivity analyses revealed that looming and object-related signals in the superior colliculus covaried with a common network of regions including frontoparietal cortex, pulvinar, amygdala, and early visual, superior, and inferotemporal cortex. Object-related signals in the superior colliculus covaried more strongly with activity in the fusiform gyrus than looming-related signals, suggesting that static information about objects may reach the colliculus through cortical inputs. Together, these results characterize how the superior colliculus flexibly detects threats through its participation in distributed neural networks."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "An Ecological and Objective Neural Marker of Implicit Identity Recognition",
    "presenter": "Meike Ramon",
    "poster_id": "A65",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "We developed a novel paradigm to measure identity recognition implicitly - using Fast Periodic Visual Stimulation (FPVS) with electroencephalography (EEG). Uniquely, we used ecologically valid face images that represent the features characteristic of forensic case material. FPVS EEG data were acquired from a demographically diverse cohort of 16 students and 12 police officers with normal face processing abilities. We measured participants’ neural responses to a 1-Hz tagged oddball identity embedded within a 6-Hz stream of base images, using different types of face stimuli varying in image quality. Evidence of implicit recognition was found with high-quality mugshot, but not CCTV-like images. Our study extends previous research by demonstrating that unfamiliar identities can elicit robust neural signatures of recognition through brief, repeated passive exposure. The sensitivity of our method is valuable for applications where objective validation of face processing ability is crucial, including assessment of law enforcement experts (e.g., facial forensic examiners, Super-Recognizers) and eyewitnesses."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Deep Neural Networks Provide Insights into Distinct and Shared Selectivity for Faces and Bodies in Human Visual Cortex",
    "presenter": "Leonard E van Dyck",
    "poster_id": "A64",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Faces and bodies are key social stimuli processed by distinct functional networks in human visual cortex. However, growing evidence suggests systematic overlap between these networks, raising an important question: How segregated or integrated are face and body processing? Competing hypotheses propose fully segregated pathways, varying levels of integration, or a single multiplexed system. Here, we test these hypotheses using deep convolutional neural networks trained on object recognition. A functional localizer identified face- and body-selective units, as well as mixed-selective units that respond to both categories. Decoding analyses revealed that face- and body-selective units specialize in their respective domains, while mixed-selective units encode detailed information from both, supporting an integrative role. Finally, using fMRI encoding analyses, we found that these units account for unique variance in neural responses within both distinct and overlapping face- and body-selective cortical areas. Our findings suggest that face and body networks balance segregation and integration, supporting both fine-grained recognition and whole-person perception."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Isolating sparse, category-computing circuits in deep neural networks",
    "presenter": "Jeffery W. Andrade",
    "poster_id": "A67",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Humans  can recognize many object categories: what are the underlying computational routes from retina to category-level representations that enable this capacity? Deep neural networks are now remarkably competent at visual categorization, and as such can serve as a powerful model system for dissecting the hierarchical processing of visual inputs. In this work, we investigate the computations underlying individual category recognition in CNNs: are all unit-to-unit connections across the layers required to categorize an object, or might more dissociable sparse, modular circuits learned in the network support this task?  Extending work on CNN circuit extraction (Hamblin, Konkle, \u0026 Alvarez, 2023), we have developed a procedure for identifying functional subcircuits within Alexnet that are important for category discrimination. Our algorithm assigns scores to connections based on their estimated contribution to a category unit's activation pattern and prunes the lowest-scored connections up to a chosen circuit substitution accuracy threshold on an extraction imageset. We then evaluate the resulting circuits for function preservation on new images, and analyze the structure of the resulting category circuits. When pruning to an allowed small circuit substitution accuracy decrement, we find surprisingly sparse, substantially faithful circuits with an average circuit sparsity of 45.3% and an average circuit substitution accuracy of 90.9% that of the unpruned network. These results indicate that category-level representations individually depend upon relatively sparse subnetworks, suggesting a semi-modular neural code with significant, structured sharing of circuitry."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Neural representation of occluded objects in visual cortex",
    "presenter": "Fraser W Smith",
    "poster_id": "A66",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "The ability of the human visual system to recognize occluded objects is striking, yet current models of vision struggle to account for this successfully. Our goal here was to understand what best explains neural representations of occluded objects under more realistic occlusion i.e., when objects occlude other objects. In an event-related fMRI design, participants performed a one-back task while being presented with unoccluded objects, objects occluded by another object, or cut out by a corresponding object silhouette. Decoding analyses showed that EVC responses to occluded objects were better determined by the visible features whereas in IT inferred features better predicted the response. Projection analyses showed that the weights assigned to occluded objects in IT were predicted by independent categorization judgements. Our results demonstrate that IT better decouples responses to real-world occluded objects with robust representations evident across multiple competing objects."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Neural representations of implied motion in body-selective regions revealed by caption-based encoding models",
    "presenter": "Ryuto Yashiro",
    "poster_id": "A68",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Recent studies using encoding models that accurately predict brain responses to natural images from corresponding captions suggest that high-level visual regions encode the rich semantics of a scene, rather than merely responding to specific categories such as faces or bodies. Here we focused on the extrastriate body area (EBA) and investigated what semantic content is represented in this region by analyzing the relationship between the co-occurrence of multiple objects in large-scale natural scene captions and EBA responses predicted by caption-based encoding models. We found that responses in EBA as well as the fusiform body area (FBA) outside motion-processing regions exhibited high correlation with the perceived motion speed of a human body implied in an image, suggesting that these body-selective regions encode the implied motion of a human body inferred from the co-occurring objects in a scene."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Temporal misalignment in scene perception: Divergent representations of locomotive action affordances in human brain responses and DNNs",
    "presenter": "Clemens Georg Bartnik",
    "poster_id": "A69",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "The human visual system processes scenes with remarkable speed, enabling the extraction of essential information to navigate our surroundings in a single glance. To elucidate how the brain transforms visual inputs into neural representations of navigationally relevant information, we collected electroencephalography (EEG) responses to diverse indoor and outdoor scenes along with behavioral annotations of locomotive action affordances (e.g., walking, cycling), object annotations, and low-level image features to model distinct types of scene information. Using representational similarity analysis, we examined the neural representation of locomotive action affordances over time, their co-localization within scene-selective cortex, and their computational alignment with deep neural networks (DNNs). Our results show that locomotive action affordance representations emerge within 200 ms of visual processing, showing unique contributions to EEG responses at temporally distinct time-points from objects and low-level properties. Spatiotemporal fusion with functional magnetic resonance imaging (fMRI) recordings in scene-selective brain regions reveals that both the parahippocampal and occipital place region (but not the medial place region) contribute to locomotive action affordance representations, with a distinct temporal hierarchy between them. While DNNs align well with early EEG responses, they primarily capture low-level features and show limited alignment with affordance processing. These findings reveal a temporally distinct neural representation of action affordances and highlight a limitation of current DNNs in modeling affordance perception."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Risk-sensitive reinforcement learning processes in the human brain drive impulsive choice",
    "presenter": "Rhiannon L Cowan",
    "poster_id": "A70",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "We utilize standard and risk-sensitive reinforcement learning (RL) models to examine behavior and neural encoding of expected value and prediction error in 71 neurosurgical subjects during a risky decision-making task. We observed a behavioral trade-off between task performance and accuracy, specifically underpinned by negative prediction errors. Less impulsive choosers encoded RL model variables across brain regions to a greater extent, with greater encoding in the striatum, nucleus accumbens, and frontal cortices, compared to more impulsive choosers. More impulsive choosers’ decision-making was dictated by negative prediction errors, leading to risk-aversive tendencies and, ultimately, suboptimal decision-making."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Perceptual Choices and Confidence Judgements can be Modelled as a Single Accumulation Process",
    "presenter": "Kobe Desender",
    "poster_id": "A72",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Evidence accumulation can continue after a choice is made to incorporate new evidence and inform subjective confidence ratings, but we do not know how this post-choice evidence accumulation process differs from the one that informed the initial choice. Existing models disagree regarding whether the post-decision process is a continuation of the initial choice process or reflects a distinct one. In addition, current models disagree on the question of whether post-choice accumulation processes are subject to time-based or boundary-based stopping rules. We implemented these alternative mechanisms across four classes of models and fit them to human data from a task with a speed/accuracy trade-off applied only to the post-decision confidence rating stage, via a deadline. Speed-pressure decreased confidence-RT, certainty, metacognitive accuracy, and changes-of-mind (CoM). The four classes of models were able to fit the data well, but Boundary-Based Stopping Rules fit the data better than the Time-Based Stopping Rules, as the latter were unable to replicate the pattern of decreasing certainty for slower confidence-RTs. However, the behavioural modelling did not conclusively favour one Boundary model over the other. We therefore compared the evolving Decision Variable with a neural marker of evidence accumulation, the Centro-Parietal Positivity (CPP), to further distinguish these two similar models. The Shared-process Boundary-based model was able to replicate qualitative effects of Certainty and CoM on the CPP, while the Distinct-Boundary model could not. We suggest that post-decision evidence accumulation is boundary-based rather than time-based, and shares information with the initial-decision process rather than being a distinct accumulation mechanism."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Computational Modeling of Heuristic and Base-rate Integration in Reasoning",
    "presenter": "Jérémie Beucler",
    "poster_id": "A73",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Base-rate neglect, a key example of biased reasoning, is often attributed to the interplay between intuitive and deliberative processes in dual-process theories. Yet these explanations remain largely verbal and theoretically underspecified. Participants (N = 151) performed a novel, continuous base-rate neglect task where base-rate information and stereotype-driven heuristic strength (quantified using language models) were parametrically manipulated. Clustering analyses revealed three distinct reasoning profiles: stereotype-driven, base-rate-driven, and balanced. A biased drift diffusion model (DDM), in which weighted stereotype and base-rate information jointly determine the drift rate, captured individual differences and reproduced key empirical patterns in accuracy, confidence, and response time. Results show that confidence and reaction time reflect the same underlying evidence signal as choice, revealing how information is integrated during reasoning. Importantly, the model predicts that biased individuals do not benefit from increased deliberation, as they fail to integrate the logical information in the first place. This work advances the computational modeling of reasoning and offers a theoretical framework for understanding how individuals integrate conflicting information."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Decision Commitment with Incomplete Evidence",
    "presenter": "Juan R. Castiñeiras de Saa",
    "poster_id": "A74",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "How is commitment achieved in perceptual decisions under incomplete evidence? Standard models relying on bounded accumulation of evidence assume a readout based on the sign of the accumulator at stimulus termination, but this has not been properly validated. We aimed to resolve this by adapting an established rat auditory lateralization task, imposing varied maximum sound durations (SDmax).\nWhen SDmax was shorter than typical reaction times, accuracy decreased, \nbut remained surprisingly high even for very brief stimuli (e.g. 15ms). Computational modeling, using an adapted DDM framework incorporating both stimulus-dependent and stimulus-independent decision processes, explored how choices are determined post-offset. Models assuming the aforementioned standard readout failed to capture the data. Instead, behavior was best explained by assuming evidence integration continues after offset (approximated by exponential decay of the neural activity providing the sensory input), but crucially, if the decision bound is not reached during this period, the subsequent choice is random. Overall, this work quantifies decision-making under temporal constraints, indicating that bound-crossing is a fundamental requirement to reliably convert integrated sensory evidence into a specific behavioral choice."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "One-shot reinforcement learning decisions engage the hippocampus",
    "presenter": "Christopher S. Iyer",
    "poster_id": "A75",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Decisions about value can be based on multiple sources of information from memory. Classic reinforcement learning models describe how value estimates are incrementally learned over many trials, while decisions can also be guided by “one-shot” episodic memories for single experiences. The goal of this study is to better understand the contribution of these two processes—incremental learning and episodic memory—to value-based decisions. Human participants were scanned with fMRI while performing a decision task in which choices could be guided by either incremental and episodic information. Choices based on episodic information were associated with increased BOLD activity in the hippocampus. Intriguingly, hippocampal activity was also associated with incrementally-learned value information, derived from reinforcement learning models. Finally, we observed reward-related reinstatement of patterns during episodic decisions in the ventromedial prefrontal cortex. These findings reveal both shared and distinct markers of incremental and episodic memory during value-based decisions."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Uncovering brain-wide planning strategies with deep RL: Lessons from the Tower of Hanoi",
    "presenter": "Austin Tudor David Andrews",
    "poster_id": "A76",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Human planning involves generating and executing ac-\ntion sequences under environmental constraints (Mattar\n\u0026 Lengyel, 2022). Experimental studies have identified\nthat areas such as the prefrontal cortex (PFC) , hippocam-\npus and cerebellum play important roles during planning\n(Grafman et al., 1992; Goel \u0026 Grafman, 1995). We pro-\npose that the architectures of deep reinforcement learn-\ning agents capable of solving human-level planning tasks\ncan offer a normative framework for understanding the\ninvolvement of different brain regions in planning. To\ndemonstrate this, we use MuZero (Schrittwieser et al.,\n2020) and a widely used task to study goal-directed plan-\nning and behavior, Tower-of-Hanoi (ToH). We evaluate the\nperformance of MuZero on the ToH under targeted net-\nwork ablations to simulate brain region-specific lesion\nstudies. Ablating the value network reproduces the be-\nhavior observed in patients with PFC damage, while ab-\nlating the policy network mimics cerebellar damage. Our\npreliminary results suggest that deep RL architectures\nmay provide a brain-wide account of human planning."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Fast and robust Bayesian inference for modular combinations of dynamic learning and decision models",
    "presenter": "Alexander Fengler",
    "poster_id": "A77",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "In cognitive neuroscience, there has been growing interest in adopting sequential sampling models (SSM) as the choice function for reinforcement learning (RLSSM), opening up new avenues for exploring generative processes that can jointly account for decision dynamics within and across trials. To date, such approaches have been limited by computational tractability, e.g., due to lack of closed-form likelihoods for the decision process and expensive trial-by-trial evaluation of complex reinforcement learning (RL) processes. We enable hierarchical Bayesian parameter estimation for a broad class of RLSSM models, using Likelihood Approximation Networks (LANs) in conjunction with differentiable RL likelihoods to leverage fast gradient-based inference methods including Hamiltonian Monte Carlo or Variational Inference (VI). By exploiting the differentiability of RL likelihoods, this method improves scalability and enables faster convergence for complex combinations of RL and decision processes. To showcase these methodological advantages, we consider multiple interacting generative learning processes with the Reinforcement Learning - Working Memory (RLWM) task and model. This RLWM model is then combined with SSMs via LANs. When combined with hierarchical variational inference, this approach can accurately recover the posterior parameter distributions in  complex RLSSM paradigms, and moreover, that in comparison, fitting data with the equivalent choice only RLWM model yields a biased estimator of the true generative process."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Meta-Reinforcement Learning in Homeostatic Regulation",
    "presenter": "Naoto Yoshida",
    "poster_id": "A78",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "The homeostasis of internal bodily states is essential for animal survival. In computational neuroscience, Homeostatically Regulated Reinforcement Learning (HRRL) has been proposed as a theoretical framework for modeling the learning of behavior in agents that maintain homeostasis through trial and error. HRRL assumes the existence of the dynamics within the agent and defines rewards based on its internal state. However, it remains unclear what kinds of behavioral learning are enabled by such internally defined rewards. In this study, we hypothesized that when dealing with such internally defined rewards, agents can acquire meta-reinforcement learning (meta-RL) capabilities by incorporating multimodal inputs and recurrent connections into the policy network architecture. Numerical experiments suggested that the proposed architecture enable the HRRL agent to acquire exploratory behaviors in the environment, indicating that meta-learning abilities comparable to those found in previously known meta-RL approaches can be achieved using different architectures."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Experiential value neglect is robust to changes in learning architecture and nature of outcomes",
    "presenter": "Caroline Pioger",
    "poster_id": "A79",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "In decision-making, values attached to options can stem from two sources: past experiences with rewards and punishments (experiential) or explicit descriptions of outcomes and probabilities (descriptive). According to the common currency hypothesis, we encode these subjective values on the same scale. Most studies examine decision-making within either experiential or descriptive options separately, but what about when individuals choose between the two? These hybrid choices reflect real-world decisions, such as choosing between a restaurant we’ve visited before (experiential) and one we know through online ratings (descriptive). Garcia et al. (2023) examined such hybrid choices by asking participants to choose between learned experiential options and symbolically described ones. Their findings revealed a systematic neglect of experiential options which persisted after controlling for alternative explanations such as insufficient learning and ambiguity aversion. This study explores whether experiential neglect arises from differences in how values are represented, or from memory retrieval cost. To investigate this, we ran three experiments, where we systematically manipulated the learning architectures and outcome information to reduce the representational difference between options. Our findings show that experiential neglect persisted across all conditions, further challenging the dominant theory and suggesting that the neglect is primarily driven by memory retrieval cost."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "When jackpot misleads:  The disrupting role of rare rewards in value learning",
    "presenter": "Luning He",
    "poster_id": "A71",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Prior research suggests that, when making decisions from experience, people tend to undervalue high-risk, high-reward options. This tendency has been attributed to underweighting the impact of rare rewards. However, this underweighting view may be confounded with risk avoidance. Here, we challenged a pure “underweighting” account using a two-armed bandit task. We found that choices for options with large but rare rewards were insensitive to their expected values. Instead, they were guided by a value-independent sampling bias for the rare-reward option. These findings suggest that the presence of large but rare rewards disrupts value-based decision-making, shifting the decision policy toward risk sensitivity rather than expected value maximization."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Computational Modeling of Choice Frequency in Habitual Behavior:  A Pre-Registered fMRI Study",
    "presenter": "Hugo Fluhr",
    "poster_id": "A81",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Habits are integral to decision-making but can contribute to maladaptive behavior when they override goal-directed control. Previous research showed that past choice frequency influences current choice independently of reward history. However, many prior attempts to experimentally induce habitual behavior have failed to replicate (e.g., Gera et al., 2023), highlighting the need for robust behavioral paradigms. In this pre-registered fMRI study (N = 71), we aimed to replicate and extend previous behavioral findings to different behavioral situations. Moreover we aimed to investigate the neural mechanisms underlying frequency-based habitual behavior. Participants completed a modified version of the Reward Pairs task, where reward level and choice frequency were manipulated orthogonally. Behavioral data were best explained by a model combining reinforcement learning (RL) and a choice kernel (CK), confirming that both reward and choice history shape current choices. Our pre-registered univariate fMRI analyses are ongoing. So far, they revealed no significant neural correlates of RL or CK values in hypothesized regions of interest. While the behavioral findings reinforce the relevance of frequency-based habits, the neural data point to challenges in detecting their neural substrates using univariate BOLD analyses. Future work will examine whether these effects manifest through distributed neural patterns or in the functional connectivity between candidate brain regions."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Neurocomputational modeling of leadership in delegation decisions",
    "presenter": "Hun S. Choi",
    "poster_id": "A80",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Despite growing interest in understanding human decision-making dynamics in social contexts, its relationship with leadership traits and their underlying neural computations remains poorly characterized. Systematic parameterization of social actions is essential for categorizing leadership styles and explaining individual variability. Here we investigated how individuals respond to decision dilemmas in authoritarian leadership contexts where they could prioritize personal gain (own-reward), team productivity (productivity), or team member wellbeing (welfare). Using a computational model, our results revealed that participants in the productivity group mainly processed each team member’s competence, whereas those in the welfare group concentrated on team members’ unhappiness, with these distinct processes reflected in the frontoparietal value-processing network. These findings demonstrate that individual leadership styles correspond to distinct neurocomputational processes during social decision-making."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Adaptive attentional prioritisation of advice for decision making",
    "presenter": "Rumandeep K. Hayre",
    "poster_id": "A82",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "When we have limited access to reliable information, we are encouraged to seek advice from others to help us make an informed decision. Yet, not all advice is good. Here, we investigated whether people can learn about adviser characteristics and prioritise advice accordingly using a collaborative decision making task combined with eye-tracking. Across two experiments, we showed that participants prioritised advice from those who were more accurate on average and those more confident in the moment. While generally gathering more information when they were most in need of advice, participants strategically sought out information on adviser confidence when advisers disagreed with one another. The current findings show that people learn about adviser characteristics and use them to prioritise the relevant information to support goal-directed decision making."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Exertion of Effort Reduces the Willingness to Exert Effort for Rewards in the Future",
    "presenter": "Selma Lugtmeijer",
    "poster_id": "A83",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "To obtain rewards, we often have to decide whether exerting the effort that’s required is ‘worth it’. Sometimes we make these decisions at one moment in time, about effort we will exert much later. E.g., deciding now whether to review a paper in two weeks time. Yet, the motivation to exert effort is susceptible to fatigue, with recent evidence showing that exerting effort increases fatigue and reduces the motivation to exert effort just seconds later. However, little is known about how much these momentary fluctuations affect decisions to exert effort for reward in the future. In this study, participants exerted different levels of physical force (10% or 48% of their max grip strength for 5s), to induce fatigue. After each initial squeeze they made two decisions, choosing between a fixed low-effort/low-reward option and a variable higher-effort/higher-reward offer. Critically, these decisions were about effort they would have to exert for reward after the task, not immediately following their choices. We found that after a high-effort squeeze people were more likely to choose the rest option than after a low-effort squeeze. These findings suggest that people are unable to ignore momentary fluctuations in fatigue when making decisions about future effort, highlighting the implications of fatigue for motivation."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Task-space dimensions guide human exploration in complex environments",
    "presenter": "Jiahui An",
    "poster_id": "A85",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Humans frequently make decisions in complex, high-dimensional environments, where identifying task-relevant information is critical for rapid behavior optimization. Humans outperform standard reinforcement learning agents in navigating such complexity, yet the cognitive strategies of humans remain unclear. To address this, we developed a novel multi-dimensional learning task in which only a subset of dimensions is reward-related. Crucially, unlike prior studies, subjects are uninformed of the true task dimensionality and have to identify them through exploration. This design closely mimics the ambiguity in real-world tasks. Our results identified two stereotyped choice patterns that reveal “dimension-guided” strategies in exploration and exploitation. Cross-subject analyses suggest that dimension-guided exploration may promote the efficiency of reward-based learning. These findings indicate that humans leverage task dimensionality to guide exploration, and provide inspiration for improving exploration efficiency in AI agents."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Distinct Value and Choice Information Across Putative Subtypes of Primate Orbitofrontal Neurons",
    "presenter": "Emirhan Bugra Albayrak",
    "poster_id": "A84",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "The primate orbitofrontal cortex (OFC) plays a key role in economic decision-making by encoding economic value and other decision-related variables. It has been proposed that different subtypes of OFC neurons may have different encoding properties corresponding to different functions in decision-making. However, there has been no attempt to classify OFC neurons according to their physiological subtype in primates. Here, we identified putative neuron types using an unsupervised clustering algorithm based on the extracellular waveform. We found that specific clusters of neurons had distinct encoding profiles defined by the timing and magnitude of task variable representations. We also identified a cluster of putative pyramidal neurons with choice-predictive activity far above that of other subtypes.  These results represent a first step in understanding the cell subtype-specific roles of OFC neurons in choice."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Latent dimensions in neural representations predict choice context effects",
    "presenter": "Asaf Madar",
    "poster_id": "A87",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Human choice is often affected by the context of available alternatives, a phenomenon known as choice context effects. To explain context effects, current models require the choice options to be described by two numerical attributes. However, decision-makers are not restricted by these attributes and might represent the options by additional latent attributes. Here, we propose using participants’ neural representations to gain access to the full attribute set they consider, while relaxing the assumptions regarding their attribute space. We aimed to use these representations to predict the context effects in participants’ choices. We estimated the context effects elicited by lottery stimuli using one behavioral sample (n=122) and then recruited two independent fMRI samples in a preregistered design (n_first=28,n_replication=34) to estimate the neural representations of each lottery without the context of choice. We predicted the context effects based only on the neural similarity between the individual lotteries, improving out-of-sample predictions by 14% and explained variance by 20% compared to traditional methods. This framework can be generalized to any stimulus type and help extend the study of context effects to more naturalistic stimuli."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Disentangling consistency and reward in repeated moral decisions with mouse tracking and fMRI",
    "presenter": "Xinyi Julia Xu",
    "poster_id": "A86",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Tracking response history and current rewards is critical for making moral decisions. By integrating fMRI and mouse tracking (MT) with a value-based moral decision task, we quantify the level of choice conflict with the MT metric, and examine how individuals incorporate information from the response history to make repeated moral decisions. Our study uses response entropy and cumulative responses (CR) to define choice consistency on both the subject-level and trial-level. We find that a stronger correlation between choice conflict and response entropy is mediated by the weight of reward in decisions. On the neural level, the brain adapts to conflict over the experiment sessions, and the adaption in reward-related brain regions is linked to response entropy. Meanwhile, multivariate representations in cognitive control and self-referential brain regions encode the weight of relative reward and CR. Through understanding choice conflict and response history, our research sheds light on its significance in multi-trial moral decision-making from the consistency perspective. These findings lay the groundwork for studying the underlying mechanisms in repeated decision processes."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "A joint model of risk-taking and learning related to risks based on behavioral and prefrontal oxygenation measures",
    "presenter": "Murat Perit Cakir",
    "poster_id": "A88",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "This study examines the neural and cognitive mechanisms of risk-taking by integrating behavioral \nand fNIRS data through a joint modeling approach. Participants completed a modified Balloon Analogue \nRisk Task (BART) under conditions varying in uncertainty, while a functional near-infrared spectroscopy (fNIRS) device was used to monitor their prefrontal cortex (PFC) activity. A censored Bayesian model estimated individual risk-taking propensity, which was linked to HbO levels across PFC subregions with a joint model. Traditional analyses revealed condition-specific effects, whereas the joint model identified individual-level correlations between risk propensity and PFC activity. Notably, stronger correlations emerged in the left dorsolateral PFC under structured uncertainty and in the right dorsolateral PFC under random uncertainty— patterns not accounted by classical methods. These findings highlight the value of joint modeling in revealing latent brain–behavior relationships. \n\nKeywords: decision making under risk and uncertainty; BART; joint modeling; fNIRS"
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Mid-level arousal facilitates optimal behavioral state in humans and mice",
    "presenter": "Jan Willem de Gee",
    "poster_id": "A89",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Perceptual choice behavior alternates between discrete strategies, which can be identified through hidden Markov modeling. Peak performance occurs during mid-level pupil-linked arousal. Here, we (i) replicated the previously observed “inverted-U” relationship between baseline pupil-linked arousal in mice and engaged state occurrence (Hulsey et al., 2024), (ii) confirmed the model-based prediction that this relationship is mediated by GABAergic interneurons and (iii) established that this relationship generalizes to humans. We conclude that arousal dynamically modulates the cortical state of a sensory region to optimize perceptual decision-making."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Reward morphs non-spatial cognitive maps in humans",
    "presenter": "Nir Moneta",
    "poster_id": "A90",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "This study examines the effect of reward on non-spatial cognitive maps in humans, building on previous research showing reward influences spatial maps in animals. Seventy-two participants (38 undergoing MRI), completed a perceptual discrimination task pre- and post-reward learning. Post-reward exposure, performance improved in previously rewarded areas, with effects generalizing to non-rewarded map areas. Behavioral findings suggest reward learning alters psychological distances between stimuli, corresponding to simulations of place field shifting towards rewarded locations, akin to gravitational pulling. Preliminary fMRI data supports this interpretation, with similar representational shifts in hippocampal representations, but mixed results in the medial-orbitofrontal and visual cortex. This suggests reward affects non-spatial cognitive maps and neural representations."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Distinct Microstructural Brain Correlates of Inter-individual Differences in Reward Learning and Decision-Making",
    "presenter": "Melina Vejlø",
    "poster_id": "A91",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Humans learn which actions yield the highest rewards through trial and error, forming expectations over time. Yet, people often make suboptimal decisions, partly due to noise in how they learn action values. Such learning processes are implicated in several psychiatric conditions. \nIn a large-scale study, participants completed a gamified reward learning task and surveys measuring impulsivity (using BIS) and compulsivity (using OCI-R). Additionally, they underwent quantitative MRI scanning, in which we measured whole-brain microstructural indices of myelination, myeloarchitecture, and cortical iron. Voxel-based quantification analyses revealed that greater myeloarchitectural integrity, particularly in the left angular gyrus and right postcentral gyrus, correlated with higher learning rates for both chosen and unchosen objects. Learning noise was associated with R1 and R2* values in the precentral gyrus and cerebellum. Impulsivity and compulsivity showed distinct relationships with brain regions, such as the cerebellum, frontal gyrus, and insula. These findings highlight the role of brain microstructure in shaping reward-guided learning and individual differences in impulsivity and compulsivity, with implications for psychiatric disorders."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Decision-making reference point biases in the dorsal anterior cingulate cortex",
    "presenter": "Demetrio Ferro",
    "poster_id": "A92",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Probabilistic decision-making is influenced by many subjective factors, including reward seeking, risk acceptance, and satisfaction. A significant aspect, often overlooked in trial-based reward paradigms, is reference-point bias, consisting in the assessment of potential gains and losses based on a relative reference point, based on current wealth status (Kahneman \u0026 Tversky,1979). To address this gap, we set incremental reference points through the accumulation of virtual tokens leading to a fluid jackpot reward, and established their impact on behavioral performance and neural encoding in the dorsal anterior cingulate cortex (dACC) of macaque monkeys. As subjects accumulated more and more tokens, the trial execution approached the jackpot achievement. For higher accumulated tokens subjects exhibited faster and more accurate choices, indicating reference point-dependent behavior. Neuronal activity in the dACC corresponded with reward value during the visual presentation of offer cues, with enhanced encoding for higher ranges of accumulated tokens. Additionally, in easier trials where more valuable options were more salient, both decision-making speed and the neural representation of reward value were enhanced. These findings underscore the critical role of the dACC in integrating reward accumulation and decision-making processes, reflecting biases associated with reference point dependence."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Eye-tracking based Bayesian inference for adaptive decision making and planning processes in a dynamic environment",
    "presenter": "Sanghyun Park",
    "poster_id": "A94",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "In dynamically evolving environments, effective planning is crucial for guiding decisions toward optimal goals and adjusting them as conditions change. However, the underlying neurocognitive mechanisms of planning remain elusive, as these processes are not directly observable through behavior. Previous studies have largely focused on simplified decision-making tasks, often limited to static environments or single-goal scenarios. Here, we introduce a novel arithmetic paradigm that requires multi-step planning and flexible goal switching in a dynamic, multi-goal context. Using eye tracking data, we estimated the utility of each goal and modeled goal switching in real time using a Bayesian framework, capturing individual differences in how participants integrate new information into decisions. High-performing participants were more likely to adjust their choices based on updated utilities and engaged in forward planning when initial plans became infeasible. Moreover, model-derived goal switching probabilities reliably predicted activity in brain regions associated with reward processing and value-based decision-making. These findings suggest that adaptive goal switching is supported by neurocognitive processes that continuously track the evolving utility of multiple goals."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Mouse lockbox: a sequential mechanical decision-making task for freely moving mice",
    "presenter": "Marcus Nicolaas Boon",
    "poster_id": "A93",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Advances in automated tracking tools have sparked a growing interest in studying naturalistic behavior in animals. Yet, traditional decision-making tasks remain the norm for assessing learning behavior in neuroscience. Here, we present an alternative sequential decision-making task to study complex mouse behavior. We developed a 3D-printed mechanical puzzle, a so-called lockbox, that requires a sequence of four steps to be solved in a specific order. During the task, the mice move around freely, enabling the emergence of complex behavioral patterns. We observed that mice exhibit a high level of motivation, willingly engage in the task, and learn to solve it in only a few trials. To analyze the strategy the mice use to solve the task, we used three cameras to capture different perspectives and developed a custom data analysis pipeline. Our analyses suggest that the rapidly increasing performance is primarily due to the acquisition of manipulation skills, although first signs of a cognitive strategy for the task appear during later trials."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "A new multi-level Theory of Mind model for strategic decision-making scenarios",
    "presenter": "Giorgio L Manenti",
    "poster_id": "A95",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Theory of Mind (ToM) enables individuals to infer others' intentions and beliefs. While existing models successfully simulate recursive ToM reasoning, they typically rely on a fixed strategy at the lowest level (e.g., Win-Stay-Lose-Shift). We introduce a novel ToM model that combines a dynamic belief over multiple strategies with recursive best responses at higher levels. Using simulated data from a Matching Pennies game, we show that the model accurately recovers underlying strategies, adapts to changes in the environment, and correctly infers the ToM level of the agent. This approach offers a more flexible and robust framework for modeling strategic social reasoning and opens new directions for understanding decision-making in interactive settings."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Confidence in approach-avoidance conflict",
    "presenter": "Oleg Solopchuk",
    "poster_id": "A96",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Choices that balance several attributes involve mixed feelings. However, how this ambivalence is reflected in confidence judgments is unclear. We tested how people judge separate confidence dimensions corresponding to different choice attributes in the face of a conflict between approach and avoidance. We found confidence judgments to be partially dissociated, with the information about aversive consequences leaking into the confidence of the appetitive dimension of a choice. We also found that confidence decreases the perception of pain and increases momentary happiness, suggesting the need to refine accounts of affective judgments."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Bidirectional Interactions between Local and Global Confidence Explain the Pervasive Effect of Confidence Biases",
    "presenter": "Hélène Van Marcke",
    "poster_id": "A97",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Research into decision confidence, i.e. the subjective feeling about the correctness of a decision, indicates important links between pervasive confidence biases and a variety of real-life outcomes and psychiatric symptoms. While decision confidence is traditionally studied in isolation, recent theoretical accounts posit that such “local” confidence about decisions interacts with global confidence, i.e. the general feeling about the ability to perform a task. Here, we provide empirical and modelling evidence for such bidirectional influences. Using a manipulation of global confidence, we measured both constructs in a perceptual decision paradigm. We found that local confidence is indeed informed by global confidence in addition to accuracy and RTs, while global confidence is informed by local confidence rather than accuracy and RTs. By explicitly modelling global confidence in a signal-detection theory framework, we provide computational evidence for bidirectional interactions between local and global confidence that explain the pervasive nature of confidence biases."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Discounting and Drug Seeking in Biological Hierarchical Reinforcement Learning",
    "presenter": "Boris S. Gutkin",
    "poster_id": "A98",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Despite a strong desire to quit, individuals with long-term substance use disorder (SUD) often struggle to resist drug use, even when aware of its harmful consequences. This disconnect between explicit knowledge and compulsive behavior reflects a fundamental cognitive-behavioral conflict in addiction. Neurobiologically, differential cue-induced activity within striatal subregions, along with dopamine-mediated connectivity from the ventral to the dorsal striatum, is a key factor in driving compulsive drug-seeking. However, the functional mechanism linking these neuropharmacological findings to the cognitive-behavioral conflict remains unclear.\n\nAnother key aspect of addiction is temporal discounting, with studies showing that individuals with drug dependence exhibit steeper discount rates than non-users. Assuming the ventral-dorsal striatal organization reflects a gradient from cognitive to motor-action representations, addiction can be modeled within a hierarchical reinforcement learning (HRL) framework. However, incorporating discounting into the biological HRL framework is challenging, and remains an open problem. \n\nIn this work, we build upon an algorithmic model that captures how the action choices that the agent makes when reinforced with drug rewards become impervious to the presence of negative consequences that often follow those choices. We address the challenge of incorporating discounting into the HRL framework by ensuring that the values of natural rewards converge across all hierarchical levels in the HRL framework. In contrast to natural reward values, we show that the pharmacological effects of drugs on the dopamine system cause divergence in drug reward values.\n\nOur results demonstrate that high discounting amplifies drug-seeking behavior across all levels of the hierarchy, suggesting that faster discounting is associated with increased addiction severity and impulsivity. We show how these results align with the evidence supporting temporal discounting as a behavioral marker. Additionally, our model offers testable predictions and establishes a framework that conceptualizes addiction as a disorder of hierarchical decision-making processes."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Introducing the CORTEX Database: COntext-dependent Reinforcement learning and Transfer EXperiments",
    "presenter": "Isabelle Hoxha",
    "poster_id": "A105",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Learning - Transfer paradigms are particularly relevant to study value learning, as they can uncover contextual value learning. However, there is no consensus within and across laboratories on the implementation of the experimental variable nor the exact computational processing underlying context-dependence, thus making it unclear whether models such as reference-point and range adaptation genuinely generalize across contexts or are merely tailored to specific task structures. We therefore created an extremely large (n\u003e2500 human participants) yet extremely curated dataset to establish a standardized framework and systematically evaluate the applicability of competing value learning models. We showed that the range adaptation model is the best fitting model for half of the participants, with task specifications such as feedback type modulating the relevance of in-context learning models."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Not Just Habits: How Feedback Sensitivity and Belief Updating Shape OCD",
    "presenter": "Lev Kiar Avberšek",
    "poster_id": "A104",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "OCD research has undergone several paradigmatic shifts – from cognitivism that emphasizes cognitive biases and threat-reducing function of compulsions, to the dual-process paradigm that views compulsions as habits. In our study, we build on recent ideas that focus on the relationship between aberrant feedback processing, model-based learning and OCD. Using a probabilistic contingency reversal task on two large online samples and computational modelling, we show that OCD is associated with diminished processing of certain types of feedback (i.e., rewards and valid feedback) and ability to distinguish valid from noise feedback, inflated mental model volatility and excessive epistemic uncertainty. Our results implicate the involvement of biased goal-directed mechanisms in OCD, challenging the habitual view. In future, we want to expand our ideas by investigating the neural correlates of these mechanisms."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Environmental Conflict Modulates Pavlovian Bias: A Computational Account",
    "presenter": "Priyanshu",
    "poster_id": "A106",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "While actions are often modeled as instrumental responses shaped by reinforcement learning, behavior is also controlled by stimulus-outcome associations known as Pavlovian responses. When these Pavlovian biases align with instrumental goals, behavior is facilitated but when they conflict, decision-making can become suboptimal. Such conflicts are particularly relevant in psychopathologies like anxiety, addiction, and depression, where exaggerated Pavlovian biases contribute to maladaptive behaviors. These biases are notoriously difficult to suppress, and current training-based interventions have shown limited success, even with training. To address this, we developed a novel approach-avoidance task with probabilistic outcomes, systematically varying the level of conflict both across trials and in the environment. Participants exhibited Pavlovian avoidance bias in the absence of any conflict in low environmental conflict situations. However, interestingly, as environmental conflict increased this bias was suppressed. Notably, individuals with high trait anxiety showed the strongest initial biases but also demonstrated the greatest improvement in high-conflict environment. These findings highlight environmental conflict as a powerful modulator of maladaptive Pavlovian biases, offering new insights into the development of more effective behavioral interventions for psychopathology."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "How neurocomputational mechanisms of number perception  adapt to prior expectations",
    "presenter": "Gilles de Hollander",
    "poster_id": "A107",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Efficient coding offers a normative theory for how the brain should allocate resources to represent the world, and growing evidence demonstrates that perceptual systems of humans and animals adhere to its principles. However, most existing studies have focused on simple stimuli like Gabor patches and have assumed relatively fixed encoding functions. Here we demonstrate that cognitive and neural representations of numerosity - a higher-level cognitive function - can rapidly adapt to context in ways consistent with efficient coding. Using fMRI (n=39), we show that neural populations tuned to specific numerosities shift their tuning with context, aligning with with a Thurstonian perceptual model in which part of an unconstrained objective stimulus space is linearly mapped to a constrained representational space. Our findings demonstrate how the brain adapts to changing conditions and how neurocomputational modeling of fMRI data can deepen our understanding of the neural representations driving behavior."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Enhancing OCD classification with Transformer-based deep learning on resting-state fMRI: insights from the ENIGMA-OCD cohort and UK Biobank pretraining",
    "presenter": "Maria Pak",
    "poster_id": "A108",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Obsessive-compulsive disorder (OCD) remains challenging to classify due to its heterogeneous clinical presentation and the limitations of static brain connectivity metrics. To address these hurdles, we applied a Transformer-based deep learning model to a record-sized dataset of resting-state fMRI from 2,094 individuals in the ENIGMA-OCD consortium. By pretraining on an extensive UK Biobank dataset and using dynamic\nconnectivity measures across multiple frequency bands, our approach achieved higher predictive performance for OCD than conventional methods. We further conducted uncertainty quantification, revealing a marked reduction in calibration error for the pretrained model. Finally, self-attention-based interpretation pinpointed reduced connectivity within sensorimotor networks in patients with OCD, consistent with prior literature. These findings underscore the value of large-scale pretraining and dynamic rs-fMRI data in enhancing model generalizability, highlighting a promising avenue for more robust OCD classification and, by extension, clinical decision-making."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Cognitive Underpinnings of Dyscalculia: Insights from Behavioral Modeling",
    "presenter": "Maike Renkert",
    "poster_id": "A109",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Developmental dyscalculia (DD) is a learning disorder characterized by impairments in numerical and mathematical cognition. Despite its high prevalence and mental health impact, its cognitive origins and everyday life implications remain poorly understood. Here we test competing theories about cognitive impairments giving rise to DD using behavioral and neural data, positing either specific deficits in number perception or impaired working memory of magnitudes. We recruited 33 adolescents with DD and 33 neurotypical controls to complete a numerical magnitude comparison task during fMRI. Bayesian perceptual choice modeling revealed that individuals with DD showed unchanged perception simple numerosity stimuli, but poorer working memory when having to compare sequentially presented numerical magnitudes. Our data characterize numerical cognition deficits in DD and support (visio-spatial) working-memory accounts of the underlying cognitive impairments."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "A Spatial Tagging Mechanism for Biasing Gaze and Reflexive Selection in Memorized Visual Space",
    "presenter": "Priyanka Gupta",
    "poster_id": "A110",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Although eye movements and covert attention are known to be linked in the brain, the link between eye movements and working memory (WM) remains relatively unexplored. In particular, how gaze mechanisms contribute to selection in visual WM is an actively researched question. Here, we present evidence, for the first time, for a cognitive mechanism that biases gaze and produces reflexive (automatic) spatial selection in memorized visual space. In one dual-task paradigm, participants (n=24) prepared and executed cued saccades to one of four locations, while also maintaining orientation information from each of those locations in WM. Spatial selection in WM was quantified by comparing the mean absolute error for subsequent orientation recall across the four locations. Saccade execution – or even preparation – toward the cued location produced robust spatial selection in WM for information at this location. Indexing this selection, microsaccades were strongly biased toward the cued location during maintenance, although, by design, this location was no more relevant than any other for subsequent recall. Next, the same participants also performed another dual-task, but in this case simply reported the cued location with a manual (button-press) response. Remarkably, this paradigm also revealed robust selection in WM favoring the cued location, accompanied by a strong microsaccade bias towards it. Lastly, spatial biases in microsaccades clearly predicted the extent of WM selection. Our results show how spatially tagging a location for subsequent recall, biases gaze and produces reflexive selection in visual WM."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Perspective: Home-based Sleep Intervention for Dementia Prevention",
    "presenter": "Tomasz M Rutkowski",
    "poster_id": "A112",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Dementia poses a significant and escalating public health challenge, necessitating effective preventative strategies. Age-related sleep disturbances, particularly loss of slow-wave sleep (SWS), are linked to an increased risk of dementia. This study employs a two-stage approach to develop a biomarker of cognitive decline from sleep parameters using machine learning and investigate a non-pharmacological intervention in adults aged 50 and older. We evaluate the efficacy of home-based closed-loop auditory stimulation (CLAS), which enhances slow oscillations using synchronized auditory cues delivered using wearable EEG technology. We aim to assess the potential of CLAS, through improved sleep quality, to delay the onset of cognitive impairment, potentially offering a scalable intervention for dementia prevention."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Association Learning with Myelin Plasticity",
    "presenter": "Charles Shvartsman",
    "poster_id": "A111",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Myelin wraps axons, resulting in increased conduction speed. For a long time, myelin patterns were thought to be established in development, after which they remained static. However, over the past decade, evidence has accumulated demonstrating that myelination changes throughout life, is activity dependent, and that myelin plasticity plays a role in various forms of learning, ultimately shaping behavior. Crucially, blocking the formation of new myelin leads to impaired learning in various domains. Despite the experimental evidence, there has been relatively little computational work exploring myelin plasticity, with no plausible account to how low-level changes in myelin can lead to high-level changes in behavior. Here, we take a step towards such an account. Specifically, we demonstrate that a model of spiking neurons, employing established cortical motifs, and using a simple and biologically grounded myelin learning rule, can learn associations – a cognitive building block that underlies more complex capacities such as motor sequencing and predictive processing. Crucially, synaptic weights are all equal and remain static throughout our simulations; the functional changes we observe arise solely from changes in conduction delays. Our work provides a proof of principle as to how myelin plasticity may shape neural circuits to qualitatively change behavior."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "An Entorhinal-hippocampal Systems Model for Spatial Navigation and Memory",
    "presenter": "Zilong Ji",
    "poster_id": "A114",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Simultaneously localizing one's position and constructing a map of the surrounding environment are fundamental processes underpinning model-based navigation, reasoning, and decision-making. The brain achieves this ability through two complementary strategies: inferring one’s states in the environment from sensory inputs and updating previous states through path integration. However, how these two sources of information interact remains largely unknown.\nHere, we introduce EHSLAM, a mechanistic computational framework of the entorhinal-hippocampal system. By integrating sensory inputs and path-integrative signals, EHSLAM learns localized representations of space as place cells in the hippocampus, by updating synaptic connections in the network after encountering a novel environment. \nThese phenomena are interrelated and mutually reinforce during spatial updates in this framework.\nFurthermore, EHSLAM captures key findings from empirical data, including place cell remapping and grid cell realignment across distinct environments, as well as making testable predictions.\nThis computational framework provides a mechanistic understanding of the neural dynamics involved in spatial navigation and memory."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "What are the best features to decode the levels of working memory load from ECoG data?",
    "presenter": "Funda Yilmaz",
    "poster_id": "A115",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "This study aims to decode the three levels of working memory load in n-back task (0-back, 1-back and 2-back) from ECoG data, utilising different feature selection strategies using regularised logistic regression. The results demonstrated that feature strategies based on common electrodes across subjects yielded the highest classification accuracies within individualized models, followed by  selection of specific brain regions, combined with data-driven methods. We also employed time-frequency analysis to differentiate potential neural markers. The results showed that low-frequency oscillations carried the most discriminative information. Furthermore, our findings indicate that the neural signature of working memory load varies between participants, yet certain cross-participant features appear to be conserved. Overall, effective feature selection may enhance both the interpretation of workload-related neural activity and the performance of simple algorithms."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Evaluating Models of Naturalistic Episodic Memory",
    "presenter": "Mathis Pink",
    "poster_id": "A113",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Recent advances in large language models (LLMs) have enabled them to process extended naturalistic inputs, making them promising candidates for modeling human episodic memory. However, standard transformer-based LLMs rely on full self-attention and positional encoding, which diverge from human episodic memory by supporting soft, parallel attention over complete input sequences. EM-LLM, a recent modification, inspired by episodic memory, replaces full attention with episodic retrieval from a non-parametric memory filled with discrete past episodes that were segmented via surprisal. Here, we evaluate whether EM-LLM captures a core property of episodic memory: the ability to recall the temporal order of events. Using a recency judgment task on segments from a full-length novel and comparing to human behavioral data, we find that a standard full-attention LLM aligns with human performance, while EM-LLM fails to recover temporal order across long sequences. These findings reveal a key limitation in EM-LLM's current design and suggest that temporal organization may require either additional architectural biases or learned representations—highlighting new directions for modeling episodic memory in naturalistic contexts."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Putting Working Memory to Work",
    "presenter": "Ziyi Duan",
    "poster_id": "A116",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Goal-directed behavior depends on the ability to flexibly change mental representations in working memory. Here, we combined neuroimaging and computational models to test the hypothesis that working memory contents can be transformed in multiple brain regions in service of task demands, including spatial remapping and mental rotation. We found: 1) response amplitudes tracked task-relevant spatial locations both before and after spatial remapping; 2) multivoxel response patterns contained more rotated than original orientation information after mental rotations. Besides, this orientation transformation existed across visual field maps, especially in the fovea. These results suggested that working memory representations can be flexibly transformed to achieve task goals."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Emergence of the Primacy Effect in Structured State-Space Models",
    "presenter": "Takashi Morita",
    "poster_id": "A118",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Human and animal memory for sequentially presented items is well-documented to be more accurate for those at the beginning and end of a sequence, phenomena known as the *primacy* and *recency* effects, respectively. By contrast, artificial neural network (ANN) models are typically designed with a memory that decays monotonically over time. Accordingly, ANNs are expected to show the *recency* effect but not the *primacy* effect. Contrary to this theoretical expectation, however, the present study reveals a counterintuitive finding: a recently developed ANN architecture, called *structured state-space models*, exhibits the primacy effect when trained and evaluated on a synthetic task that mirrors psychological memory experiments. Given that this model was originally designed for recovering neuronal activity patterns observed in biological brains, this result provides a novel perspective on the psychological primacy effect while also posing a non-trivial puzzle for the current theories in machine learning."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Decoupling levels of learning: behavioral evidence for dissociable low- and high-level structure learning",
    "presenter": "Beáta Tünde Szabó",
    "poster_id": "A119",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Hierarchical Bayesian models offer a unified framework for understanding both learning and meta-learning—the transfer of abstract knowledge across tasks. We investigate whether these two processes are dissociable through a novel statistical learning paradigm that combines low-level shape pair structures with a higher-order color-based rule. Participants viewed shape scenes organized into covert pairs with consistent color contrast patterns (the pepita rule), followed by tests assessing recognition of both pair-level and meta-structural regularities. Subject-level analyses revealed three learner profiles: (1) those who acquired both low- and high-level structures, (2) those who learned only low-level pairs, and (3) non-learners. Notably, strong low-level learning was a prerequisite for successful meta-learning, aligning with predictions of hierarchical models. These findings support a behavioral dissociation between learning and meta-learning and highlight individual differences in abstract knowledge acquisition and transfer."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Neural Predictors of Subsequent Long-Term Memory After (De)prioritization in Working Memory",
    "presenter": "Frieda Born",
    "poster_id": "A117",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Which factors determine how well information held in working memory (WM) can later be remembered from long-term memory (LTM)? Prior behavioral work (presented at CCN 2024)  from our lab suggests that active WM retrieval (WM-”testing”), particularly when retrieving information that has been deprioritized in WM, can enhance subsequent LTM performance; similar to the well-known “testing-effect” in LTM research. However, the neural mechanisms underlying these effects remain unknown. Here, we used fMRI to study which neural signatures of WM maintenance and/or retrieval predict later LTM performance,  so-called \"subsequent memory effects\" (SMEs). Using a dual-retro-cue paradigm to manipulate attentional priority in WM, we replicate key behavioral effects from our prior work and address the underlying neural mechanisms in ongoing fMRI analysis."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Learning task rule updating strategies requires extensive practice",
    "presenter": "Shengjie Xu",
    "poster_id": "A120",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "People can adjust how fast they update task rules, depending on the volatility of their environment. We investigated whether this adaptivity is primarily driven by recently experienced volatility in task demands, or can also be shaped by learned, environment-specific associations with expected levels of volatility. We trained participants on a Wisconsin Card Sorting Task where different environments required different speeds of task rule updating. Initially, participants updated strategies depending on the most recently experienced levels of volatility (Experiment 1). However, after extensive (four days) training (Experiment 2), participants also developed environment-specific associations. Our findings provide important insights in how people learn to regulate cognitive flexibility."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Unveiling Neural Mechanisms of Memorability: Generative AI Reveals Brain-Behavior Links",
    "presenter": "Hyewon Willow Han",
    "poster_id": "A122",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Some images are more memorable than others, yet the underlying neural mechanisms of memorability are not fully understood. In this study, we introduce a novel framework, “MemBrainGen”, that connects memorability, a behaviorally defined feature of images, with the human brain responses. The framework utilizes generative deep neural network models to investigate how the human brain processes images based on their memorability across visual and memory regions of the human brain. Using MemBrainGen, we successfully manipulated the memorability of natural images in both increasing and decreasing directions, and observed that predicted activations in early-mid visual regions except for V1 showed no difference in response to memorability changes. However, brain regions associated with face and body categories, and the amygdala exhibited increased predicted activation when image memorability was increased. Most notably, V1 and place-associated regions showed lower predicted activation when images with increased memorability were presented to the model. We confirm our findings by demonstrating that brain activation-maximized images have higher memorability scores compared to their original counterparts in high-level visual and memory regions. Reversely, the memorability scores of these images were decreased in the place-selective regions. We further solidify our tested hypotheses by analyzing an independent fMRI dataset. From the univariate analysis with the independent dataset, we found that the direction of changes in brain activation is consistent with the predictions of our framework. This investigation contributes to our understanding of the cognitive processes involved in visual memory. It demonstrates the potential of integrating generative models with neuroimaging to explore the causal links between brain functions and behaviour, paving the way for the formulation of experimentally testable hypotheses."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Soundscape Neuroarts and Cognitive Well-being: EEG and fNIRS  Approach",
    "presenter": "Tomasz Komendzinski",
    "poster_id": "A121",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Early detection of cognitive decline is crucial for maintaining well-being in aging populations. As a pilot study, this research explores the potential of auditory-evoked neurophysiological responses to predict cognitive function in young and middle-aged adults. We developed a deep learning model to estimate reaction time, a proxy for cognitive performance, from electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS) data. Participants engaged in personalized soundscape creation and listening tasks, designed to elicit auditory processing relevant to communication. Our model demonstrated significant predictive accuracy, achieving an R$^2$ value exceeding 0.8 for attention-related cognitive assessments. Our findings suggest that the synergy of passive BCI cognitive assessment, conducted within active auditory paradigms, and deep learning analysis of neurophysiological data, represents a highly promising non-invasive strategy for objective cognitive monitoring. This methodological advancement offers considerable potential for the scalable early detection of cognitive decline, which could facilitate more timely therapeutic interventions and ultimately promote overall well-being."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Functional Role Division in Working Memory Emerges from Intrinsic Neural Timescale Diversity",
    "presenter": "Tomoki Kurikawa",
    "poster_id": "A123",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Neural activity spans a wide range of timescales, both across and within cortical areas. However, the functional significance of this temporal diversity remains poorly understood. In this study, we investigate how heterogeneity in neural timescales within the frontal cortex—a hub for higher cognitive functions—supports performance in a context-dependent working memory task. Specifically, we model a delayed match-to-sample task with context cues, which requires the maintenance of information over time and flexible adaptation of behavior based on contextual input.\n\nWe construct a recurrent neural network (RNN) composed of units with varying intrinsic time constants to perform this task. Our analysis reveals that task performance is optimized when neural timescales are appropriately balanced, consistent with experimental observations. Notably, neurons with slower dynamics play a causal role in improving task performance despite not showing stronger selectivity to task-relevant signals compared to faster neurons. In contrast, fast neurons encode task-relevant information more precisely but only transiently.\n\nThese findings suggest that diversity in neural timescales supports a functional division of labor, enabling stable memory maintenance and flexible signal encoding. This work provides a mechanistic account of how temporal heterogeneity in neural populations can support complex cognitive computations."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Continuous-time Bayesian causal structure learning explains dopamine and behavioral anomalies in associative learning experiments",
    "presenter": "John Vastola",
    "poster_id": "A124",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Cause-effect learning is a core competency of sufficiently intelligent animals, and has been invoked to try to explain anomalous results (in both behavior and phasic dopamine activity) in certain associative learning experiments. But it is unclear how to mathematically formalize the problem of cause-effect learning, especially in a way that accommodates conditional independence structure, priors, and temporal structure (i.e., event order and proximity in time). We propose a novel Bayesian framework for modeling cause-effect learning which incorporates each of those aspects, yet remains relatively simple and has few free parameters. We study salient mathematical properties of our framework, including how inference is affected by topological structure in the assumed causal graph. Finally, we apply our framework to explain associative learning experiments, and find that it parsimoniously accounts for many otherwise puzzling observations. For example, our model explains the observation that cue-reward associations can be weakened by providing free reward at other times (contingency degradation), but only if the free reward is uncued. It also explains the observation that associations can be learned in fewer trials if each trial is longer. Our results suggest a new way to think about cause-effect learning, and support the idea that animals exploit nontrivial (causal) state representations even in simple associative learning settings."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "An Algorithmic Model of Working Memory Based on Sparse Variational Gaussian Processes",
    "presenter": "Dongyu Gong",
    "poster_id": "A125",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Working memory (WM) involves dynamically manipulating information to support perception, decision-making, and other higher-order cognitive processes. Despite extensive interests in modeling WM, the algorithmic basis of how WM encodes and manipulates information and does so in a goal-driven manner remains unclear. Here, we propose a novel algorithmic model of WM that combines sparse variational Gaussian processes with an adaptive computation algorithm. The model recapitulates a wide range of WM phenomena, including capacity limitations, attraction-repulsion dynamics, and retrocue benefits."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "‘NOT GATE’, Not ‘GATE’:  Long-term Memory Interplay with Working Memory via Reversal Coding",
    "presenter": "Shengyuan Wang",
    "poster_id": "A126",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Working memory (WM) is capacity-limited but can recruit long-term memory (LTM) to overcome such limitations. A critical question regarding this interplay is: how does LTM content flow into WM? Specifically, while recruiting LTM content that aligns with the current WM task is beneficial, recruiting inconsistent LTM content might impair performance. In this study, we investigated this question using EEG and the inverted encoding model to decode both WM and LTM content in real time. Contrary to the mainstream ‘flexible gate’ hypothesis, which suggests that a gate selectively allows beneficial LTM content to enter WM, we found that LTM content was consistently decoded, regardless of whether it was beneficial. Importantly, LTM content was represented in a reversed manner (inhibition) compared to WM content (activation). We supported this reversal coding of LTM functions to minimize interference with WM, benefiting WM performance. Our findings challenge the ‘flexible gate’ theory and suggest a ‘NOT gate’ mechanism regarding how LTM interplays with WM, where the coding of LTM content is systematically reversed after entering WM."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Simultaneity or Rhythmic sampling in working memory? Examining the temporal dynamics of maintaining multiple items in human EEG and gaze patterns",
    "presenter": "Felix Bröhl",
    "poster_id": "A127",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "The ability to briefly maintain information for upcoming\ntasks in working memory (WM) is a central aspect of human cognition. While signatures of WM information have\nbeen disclosed with a variety of neural recording techniques, similar correlates have recently been found in\nminiature gaze patterns. However, the precise temporal\ndynamics of WM information in eye movements remains\nunclear. To address this, we investigated whether human\ngaze patterns exhibit encoding of multiple items statically\nor by rhythmic alternation, and how the ocular activity\nrelates to concurrently recorded EEG signals. Our findings indicate that eye movements capture WM information of up to three items at a time. Contrary to evidence\nfor rhythmic WM replay in neural recordings, we found\nthat on a single-trial level, eye movements appeared to\nreflect the orientations of multiple items rather statically\nat the same time. These preliminary results corroborate\nthat eye tracking provides a complementary window into\nWM processes not directly captured with EEG."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "The influence of blocked versus interleaved training regimes and sleep on multi-task learning",
    "presenter": "Mina Habibi",
    "poster_id": "A128",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Recent studies suggest that humans benefit from blocked training in continual learning by promoting more factorized task representations. We investigated whether interleaved training may support human continual learning more when task separation is also aided by consolidation during sleep. Participants learned three tasks under blocked versus interleaved regimes across two experiments: Experiment 1 (contextual cues presented before stimuli; semantically non-informative labels) and Experiment 2 (stimuli presented before contextual cues; semantically informative labels). Testing occurred both immediately and after 24 hours. People trained under the blocked\ntraining regime showed higher accuracy during learning, but this advantage did not persist in the test phase of Experiment 1. In Experiment 2, blocked training resulted in higher accuracy across learning and testing. Critically, no sleep related benefits were found in either training regime for both experiments. RNNs fit to human data, however, revealed increased task separation from Day 1 to Day 2 in Experiment 2 across both training regimes. Our findings suggest that humans can benefit from both training regimes, and that the order and way in which context and stimulus are presented—rather than the regime alone—may play an important role in continual learning."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Enabling Lifelong Learning in AI with Biological Neural Networks Based on Short-Term, Working, and Long-Term Memory",
    "presenter": "Hanav Modasiya",
    "poster_id": "A129",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Achieving Lifelong Learning, the ability of a learning system to continuously acquire and adapt to changing data over time, in Artificial Intelligence (AI) is an integral step towards achieving Artificial General Intelligence (AGI), a hypothetical version of AI that can change our world forever. Currently, the vast majority of models are incapable of exhibiting Lifelong Learning. This research theorizes the first Neural Network architecture inspired by the Three-stage Memory Model--a theory on our brain's memory. By developing a complementary Neural Network learning system comprising the Cerebral Cortex, Prefrontal Cortex, and Hippocampus, mimicking Long-term, Working, and Short-term memory, respectively, this research achieves Lifelong Learning on a simulated computer vision task and develops the first Working memory-inspired model. It also demonstrates the feasibility of using the Three-stage Memory Model for achieving human-like cognition in AI. Therefore, this research reveals a pathway for future research in achieving AGI: the Three-stage Memory Model."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Evidence for Shepard’s Law in the Representational Spaces of Deep Vision Models",
    "presenter": "Daniel L. Carstensen",
    "poster_id": "A131",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Shepard’s (1987) universal law of generalization states that generalization strength decays as a concave function of stimulus distance in psychological space. While widely supported in biological systems, its relevance to artificial neural networks remains unclear. We tested this law across 26 diverse deep vision models using human similarity judgments of naturalistic images. Across models, embedding distances produced concave generalization gradients and aligned closely with human psychological spaces. To examine the role of semantic content, we analyzed model gradients across network depth and compared gradient shapes to human-derived benchmarks. Language-aligned models most closely resembled human data, suggesting semantic representations contribute to model-human alignment. Our findings extend Shepard’s law to modern artificial systems, providing further evidence for its universality. They also highlight deep vision models as compelling proxies for psychological space, providing a novel framework for assessing representational alignment between artificial and human cognition."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Flexible Prefrontal Control over Hippocampal Episodic Memory for Goal-Directed Generalization",
    "presenter": "Nora Wolf",
    "poster_id": "A130",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Many tasks require flexibly modifying perception and behavior based on current goals. Humans can retrieve episodic memories from days to years ago, using them to contextualize and generalize behaviors across novel but structurally related situations. The brain's ability to control episodic memories based on task demands is often attributed to interactions between the prefrontal cortex (PFC) and hippocampus (HPC). We propose a reinforcement learning model that incorporates a PFC-HPC interaction mechanism for goal-directed generalization. In our model, the PFC learns to generate query-key representations to encode and retrieve goal-relevant episodic memories, modulating HPC memories top-down based on current task demands. Moreover, the PFC adapts its encoding and retrieval strategies dynamically when faced with multiple goals presented in a blocked, rather than interleaved, manner. Our results show that: (1) combining working memory with selectively retrieved episodic memory allows transfer of decisions among similar environments or situations, (2) top-down control from PFC over HPC improves learning of arbitrary structural associations between events for generalization to novel environments compared to a bottom-up sensory-driven approach, and (3) the PFC encodes generalizable representations during both encoding and retrieval of goal-relevant memories, whereas the HPC exhibits event-specific representations. Together, these findings highlight the importance of goal-directed prefrontal control over hippocampal episodic memory for decision-making in novel situations and suggest a computational mechanism by which PFC-HPC interactions enable flexible behavior."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Anticipated Relevance Modulates Early Visual Processing",
    "presenter": "Lasse Dietz",
    "poster_id": "A132",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Finding objects in our environment involves comparing visual input with memory representations to prioritize relevant over irrelevant visual input. In dynamic environments, relevance can change. Anticipating relevant visual events allows the visual system to allocate resources efficiently. We examined how predictable changes in stimulus relevance affect memory-guided visual processing. Participants memorized an oriented grating and a cue indicated which of two sequentially presented probes was relevant for a memory match/mismatch judgment. First, using rapid invisible frequency tagging (RIFT), we imperceptibly modulated the luminance at the stimulus location with 60Hz, inducing a corresponding oscillatory response in the occipital electrodes of the EEG signal. We found an increased RIFT response -reflecting that early visual processing intensified- before the presentation of relevant compared to irrelevant probes. Second, using multivariate pattern analyses, we found that memory-matching and memory-mismatching probes evoke a more distinct neural response when they are task-relevant compared to irrelevant. Together, these findings demonstrate that anticipating the relevance of upcoming events enables the visual system to prepare visual processing for efficient memory-guided visual selection."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Top-Down Propagation of Neural State Boundaries During Movie Viewing",
    "presenter": "Linda Geerligs",
    "poster_id": "A133",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Our senses receive a continuous stream of complex information, which we segment into discrete events. Previous research has related such events to neural states: temporally and regionally specific stable patterns of brain activity. The aim of this study was to investigate whether there was evidence for top-down or bottom-up propagation of neural state boundaries. To do so, we used intracranial measurements with high temporal resolution while subjects were watching a movie. As this is the first study of neural states in intracranial data in the context of event segmentation, we also investigated whether known properties of neural states could be replicated. The neural state boundaries indeed aligned with stimulus features and between brain areas. Importantly, we found support for top-down propagation of neural state boundaries at the onsets and offsets of clauses. Interestingly, we did not observe a consistent top-down or bottom-up propagation in general across all timepoints, suggesting that neural state boundaries could propagate in both a top-down and bottom-up manner, with the direction depending on the stimulus input at that moment.  Taken together, our findings provide new insights on how neural state boundaries are shared across brain regions and strengthen the foundation of studying neural states in electrophysiology."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Rapid unsupervised alignment with the natural image manifold",
    "presenter": "Ananya Passi",
    "poster_id": "A134",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "There is a stark contrast between the nature of feature learning in biological and artificial vision. While brains learn without explicit supervision and with little data, deep neural networks require supervised feedback and massive training sets. Here we show that a surprisingly simple unsupervised learning algorithm can yield large improvements in the brain alignment of a deep vision model. Specifically, we trained a network in which each layer learns to compress its representations onto the principal modes of variance for natural images—a form of local learning that does not require backpropagation or supervision. Using a relatively small sample of training images, this unsupervised learning algorithm strongly improves the network’s ability to predict the image-evoked fMRI responses of visual cortex, and it makes downstream learning on an image-classification task more efficient. Remarkably, after an initial unsupervised-learning phase, the first half of the network’s layers can be frozen with little impact on the ability to learn image classification. Together, these findings suggest that a parsimonious learning algorithm—operating locally and without supervision—may be sufficient to induce the features of early-to-mid-level vision and may accelerate the learning of downstream task-specific functions."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Sparks of cognitive flexibility: self-guided context inference for flexible stimulus-response mapping by attentional routing",
    "presenter": "Rowan P. Sommers",
    "poster_id": "A136",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Flexible cognition entails a rapid adaptation of stimulus-response mappings. Standard neural networks struggle in tasks requiring rapid remapping. Here, we propose the Wisconsin Neural Network (WiNN), which generalizes fast-and-slow learning to real-world tasks demanding flexible behavior, using adjustable context states that guide attention in a pretrained convolutional neural network. We evaluate WiNN on a variant of the Wisconsin Card Sorting Task, revealing several markers of cognitive flexibility: (i) WiNN autonomously infers underlying rules, (ii) requires fewer examples than control models reliant on large-scale parameter updates, and (iii) can perform rule inference solely via context-state adjustments. This approach offers a path toward context-sensitive models that retain knowledge while rapidly adapting to complex, rule-based tasks."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Using Artificial Neural Networks to Understand Fluency in the Perception of Paintings",
    "presenter": "Yi Lin",
    "poster_id": "A135",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Fluency—the ease with which an image is processed—is closely linked to aesthetic appraisal. However, existing objective measures of fluency fail to accurately capture subjective ratings. Recently, deep convolutional neural networks (DCNNs) have been proposed as a tool for measuring integration, a concept linked with fluency, using natural scenes as stimuli. Yet, the direct link between fluency and integration remains untested, and it is unclear whether these findings generalize to art perception. In this work, we investigate (1) whether the integration measure via DCNNs effectively captures fluency, potentially outperforming existing methods in the context of art perception, and (2) if DCNNs provide a superior measure of fluency, what specific mechanisms they reveal. Our findings indicate that the DCNN-based integration measure captures subjective fluency well and significantly outperforms other objective fluency measurements. Additionally, we observed that the peak correlation between DCNN-derived integration and various visual characteristics—intended to quantify different aspects of fluency—occurs at different DCNN layers. This suggests that fluency may be a multi-level process, integrating distinct visual characteristics at various processing stages. In summary, a DCNN-based measure of integration provides valuable insights into the concept of fluency."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Rotating Snakes Illusion Reveals Limitations of Visual-Motion Models in Explaining Human Vision",
    "presenter": "Isabella Elaine Rosario",
    "poster_id": "A137",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Deep Neural Network (DNN) models provide a computational framework that enables rigorous understanding of vision. Recent DNN-based motion models have successfully replicated illusions like reverse-phi and barber pole, suggesting possible shared computational principles with human motion processing. However, findings have been mixed on whether DNN models can replicate the \"Rotating Snakes\" illusion—static patterns that induce motion perception in humans. We tested representative optical flow estimation models on both grayscale and color versions of Rotating Snakes, including those featuring recurrent architectures and different training approaches. None of the models predicted optical flows matching the continuous rotational motion humans perceive, either when presented with consecutive static images or under simulation conditions believed to trigger the illusion, such as saccadic eye movements and stimulus onset. Only the motion energy sensor and self-attention based Dual model estimated partial rotation in expected regions, matching or opposing predicted directions—an effect absent in controls. Our results highlight the gap between current DNN-based motion models and human vision. Future models tested in experimental loops should incorporate mechanisms accounting for possible explanations of the Rotating Snakes illusion, such as pupil dilation, eye movements, and contrast-dependent processing latency, as well as color- and contrast-sensitive adaptation functionality."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Modeling human visual neural representations by combining vision deep neural networks and large language models",
    "presenter": "Boyan Rong",
    "poster_id": "A139",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Human visual perception involves transforming low-level visual features into high-level semantic representations. While deep neural networks (DNNs) trained on object recognition tasks have been promising models for predicting hierarchical visual processing, they often fail to capture higher-level semantic representations. In contrast, large language models (LLMs) encode rich semantic information that aligns with later stages of visual processing. Here we investigated whether combining vision DNN features and semantic embeddings from LLMs can better account for the neural dynamics of visual perception in electroencephalography (EEG) data. We demonstrated that their combination significantly improved the prediction of neural responses compared to either model alone. This approach outperformed multimodal modelling, and model comparison showed that the observed improvement was due to capturing complex information rather than a single factor."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Behavioural markers of recurrent processing in the brain",
    "presenter": "Timothee Maniquet",
    "poster_id": "A138",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "The human brain processes visual input quickly and reliably, even in the face of large perturbations in visual input. How cortical computations solve object identity under complex visual changes is not well explained by current models. Recent work using deep neural networks (DNNs) points to recurrent processing, with models shown to be more resilient to visual complexity when equipped with recurrent connections. Here, we explore recurrent processing, and ask whether we can pinpoint the substrates of these computations in the brain. We design a stimulus set including visual manipulations known to trigger recurrent processing, from which we establish a behavioural benchmark of performance markers of recurrent processing. We then record fMRI on participants tasked with classifying the images from this stimulus set, and compare our benchmark markers with cortical representations. Preliminary results using this approach indicate that regions in the prefrontal cortex as potential candidates for hubs of recurrent connectivity in the context of challenging object recognition."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "On the Origin of 3D Perception in Visual World Models",
    "presenter": "Wanhee Lee",
    "poster_id": "A141",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "3D perception is fundamental to both biological and artificial vision, enabling navigation, interaction, and scene understanding. However, learning 3D structure in a self-supervised manner remains challenging: fully structured geometric methods impose rigid constraints that limit adaptability to natural videos, while unstructured, data-driven approaches lack geometric consistency and controllability. We propose a hybrid approach that starts with minimal priors and progressively builds structured representations from intermediate cues. Specifically, we extract optical flow from an autoregressive video model, use it to infer depth and subsequently 3D shape, and feed these representations back into the model. Our framework enables 3D understanding from a single image, achieving human-level depth estimation, supporting shape inference beyond visible surfaces, and completing 3D scene representations without explicit supervision. Moreover, the model’s learning trajectory aligns with developmental patterns of depth perception in humans, providing insights into both cognitive and artificial vision. These findings demonstrate that 3D perception can emerge through minimally structured learning in a developmentally plausible way."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Straightening of Natural Visual Sequences in Video DNNs: the Role of Locality and Temporal Coherence",
    "presenter": "Anne W. Zonneveld",
    "poster_id": "A142",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Predictions about future states of the world play an important role in guiding human behavior. In vision, internal representations are straightened relative to input space, supporting linear extrapolation and predictability. While DNNs are promising models of human visual processing, standard image-trained architectures lack this property. We assessed straightening, quantified as a reduction in curvature in feature vs. pixel space, across 19 different video DNNs, including both CNNs and Transformers. Straightening occurred in late CNN layers, but was absent in Transformers. Critically, models with global attention lacked temporal coherence in feature space, a prerequisite for straightening. These findings suggest that temporally localized processing, like 3D convolutions, enables brain-like invariant representations, whereas Transformers, despite their strong performance, may rely on mechanisms that diverge from biological vision."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Cross-Subject Brain Decoding for  Naturalistic Movie Reconstruction",
    "presenter": "Myeonggyo Jeong",
    "poster_id": "A140",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Understanding how the brain processes dynamic visual stimuli remains a central challenge in neuroscience. Although recent AI-based methods have succeeded in reconstructing static images from fMRI data, decoding continuous movie scenes entails another complexity layer to navigate spatiotemporal brain activities that are distinctly represented across different individuals. Here, we propose a multi-subject fMRI decoding framework to combine inter-subject functional alignment—which uncovers shared neural representations among participants—with subject-specific tokens to tag idiosyncrasy of an individual’s functional dynamics in learning fMRI representation. By integrating these two complementary techniques, our method simultaneously achieves robust cross-subject generalization and person-optimized modeling, requiring only minimal fine-tuning. Moreover, we also employed a whole-brain Transformer to link fMRI signals to the CLIP image-text embeddings, preparing enriched brain-video mapping input representation for a subsequent video generation. Finally, we employed AnimateDiff and FreeInit, the two up-to-date algorithms to maximize temporal coherency across reconstructed frames. Advancing fMRI movie decoding techniques holds a promise to develop a quantitative mean to scrutinize brain dynamics underlying naturalistic visual experiences."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Modeling the Hierarchy of the Human Olfactory Perceptual Space via Hyperbolic Embeddings",
    "presenter": "Farzaneh Taleb",
    "poster_id": "A143",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Olfactory perception is a complex, high-dimensional process, still largely understudied compared to vision or audition. In this work, we investigate the hierarchical organization of human olfactory perception by embedding perceptual data in hyperbolic space. Hyperbolic geometry, characterized by its exponential volume growth, is particularly well-suited for capturing hierarchical structures. We apply a contrastive learning approach to embed olfactory perceptual data in the Poincaré ball model of hyperbolic space and analyze its structural properties. Our results reveal that odorants with higher perceptual entropy, indicative of greater uncertainty or ambiguity in their perceptual descriptors, tend to be positioned closer to the center of the Poincaré disk, while odorants with lower entropy, reflecting more consistent and distinct perceptual judgments, are mapped toward the boundary. Additionally, individual differences in olfactory perception are reflected in the spatial distribution of embeddings, suggesting that confidence, personality traits, and perceptual biases may influence the way odors are structured in the human olfactory perceptual space. These findings provide a computational framework for modeling olfactory perception. Our approach contributes to the broader goal of understanding the computations underlying sensory perception, bridging cognitive science, neuroscience, and machine learning."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Characterizing the temporal profile of meaning-making with visual art",
    "presenter": "Dominik Welke",
    "poster_id": "A144",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Insight can play an important role in information processing and meaning making. A series of processes, from curiosity (drive state) to insight (uncertainty reduction) to pleasure (reward and reinforcement), might represent a fundamental epistemic arc for motivated learning. Here, we present a paradigm that combines measurements of curiosity, insight (aha), understanding and liking to outline the contours of this epistemic arc, employing the so-called ”title effect” – the fact that semantic information accompanying an artwork (such as titles) can spark insight and change how an observer understands and enjoys a piece of art. In an EEG and eye tracking study, we investigated how the different ratings were represented in the observers’ (neuro)physiological recordings. Using a time-resolved decoding approach, we were able to recover the graded ratings and track the temporal sequence of processing. Some correlates were found seconds after image onset, long after the time frames typically investigated in classical EEG studies."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Unravelling the relationship between location and categorisation improves convolutional neural networks",
    "presenter": "Jean-Nicolas Jérémie",
    "poster_id": "A145",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Many studies have attempted to enhance the performance of convolutional neural networks (CNNs) by increasing model complexity, adding parameters, or adopting alternative architectures. Our approach differs in that we prioritise ecological plausibility in order to achieve high accuracy with minimal computational cost. We focus on visual search, which requires the localisation and categorisation of a target object in natural scenes. Due to the inhomogeneity of foveal retinotopy in human visual representations, localisation plays a key role in correctly categorising labels of interest when performing this task. We propose a framework referred to as a 'likelihood map', based on the probability of correctly identifying the target label, which explores prediction by a dedicated network according to the position of the fixation point. Depending on the scenario, it can be guided (or not guided) by the target label in a manner similar to Grad-CAM or DFF. In both scenarios, we demonstrate improved classification performance when the sensor shifts towards the region of interest. Beyond its computational benefits, this framework can be used as an experimental tool to further investigate the neural mechanisms underlying visual processing."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Neural encoding with affine feature response transforms",
    "presenter": "Steven Scholte",
    "poster_id": "A146",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Current linearizing encoding models that predict neural responses to sensory input typically neglect neuroscience-inspired constraints that could enhance model efficiency and interpretability. To address this, we propose a new method called affine feature response transform (AFRT), which exploits the brain's retinotopic organization. Applying AFRT to encode multi-unit activity in areas V1, V4, and IT of the macaque brain, we demonstrate that AFRT reduces redundant computations and enhances the performance of current linearizing encoding models by segmenting each neuron's receptive field into an affine retinal transform, followed by a localized feature response. Remarkably, by factorizing receptive fields into a sequential affine component with three interpretable parameters (for shifting and scaling) and response components with a small number of feature weights per response, AFRT achieves encoding with orders of magnitude fewer parameters compared to unstructured models. We show that the retinal transform of each neuron's encoding agrees well with the brain's receptive field. Together, these findings suggest that this new subset within spatial transformer network can be instrumental in neural encoding models of naturalistic stimuli."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Gestalt processing in late-sighted children and deep neural networks",
    "presenter": "Lukas Vogelsang",
    "poster_id": "A147",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Holistic perceptual organization guided by Gestalt principles like similarity in luminance, color, or form is critical for organizing complex visual scenes. Past studies have demonstrated the development of these abilities in infancy. How important is visual experience during these early years for the emergence of Gestalt processing? Does visual deprivation during this period permanently compromise perceptual grouping? Using tasks validated previously with normally-sighted infants, we assessed perceptual grouping from immediately pre-surgery to several months afterward. Late-sighted patients showed persistently poor performance across all Gestalt cues tested, despite longitudinal gains in other visual functions. Computational simulations using deep neural networks trained with typical or atypical developmental acuity trajectories tested the role of early visual degradations characteristic of normal development. Although early degradations enhanced holistic processing in more naturalistic assessments, the benefits for explicit Gestalt processing were very modest. These findings underscore the critical role of early visual experience in developing perceptual organization, reveal important deficits from early deprivation, and suggest new avenues for future computational work."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Brittle Brain Encoding: Poor Out-of-Distribution Generalization Shows the Human Brain is neither a Nintendo Entertainment System nor a Four-Layer Convolutional Neural Network",
    "presenter": "Yann Harel",
    "poster_id": "A148",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "To explore the correspondence between artificial neural networks and brain function, we tested three models—trained agent, untrained agent, and game RAM—on their capacity to use game data to predict brain activity (fMRI) in humans playing Super Mario Bros. All brain encoding models performed similarly within the training distribution (training levels), but none generalized to out-of-distribution (OOD) levels. The OOD performance drop was generally greater than the difference between models. Our results underscore how current brain encoding approaches may overstate brain-model similarity, and highlight the critical importance of evaluating generalization when using brain scores to compare models."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "It’s a feature, not a bug: Multi-granular world models explain inattentional blindness",
    "presenter": "Mario Belledonne",
    "poster_id": "A150",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Why do we sometimes miss what’s right in front of us? Does our sometimes striking inability to notice a “gorilla” strolling in our midst repudiate the computational sophistication of human vision? Instead of regarding this and the related phenomena of inattentional blindess (IB) as a human quirk, here, we posit the opposite: that it is a signature of several advantageous computational adaptations. We realize this hypothesis by developing goal-conditioned world models, the first-ever model that reverse-engineers human IB by precisely capturing the elements relevant to our goals, while coarsely summarizing the rest scene."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Human-like compositional visual inference through neural diffusion on syntax trees",
    "presenter": "Sylvia Blackmore",
    "poster_id": "A149",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Compositionality—the ability to decompose experiences into constituent parts and flexibly recombine them—is fundamental to human intelligence. Despite the vast combinatorial space created by even basic elements, humans efficiently navigate potential configurations during visual inference. We present a neuro-symbolic approach framing visual compositional inference as inverse graphics through guided program synthesis, implemented as neural diffusion on syntax trees. Our model represents images as programs, using a conditional neural network and value model to enable efficient beam-search through program space. Validated against human behavioral data, the model achieved human-like performance across trial types. This framework provides a computational account of visual inference as search through compositional state space."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Topographic Vision Transformers",
    "presenter": "Yash Shah",
    "poster_id": "A151",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Functional organization in the form of topographic maps is a hallmark of many cortical systems and is believed to arise from biophysical efficiency, such as the minimization of neuronal wiring length. Recently, Margalit et al. (2024) developed the TDANN as a topographic convolutional neural network (CNN) that recapitulated gross ventral stream topography while minimizing feedforward wiring length. However, standard CNNs lack mechanisms for within-layer long-range interactions that are well identified in the primate visual cortex. Here we leverage a vision transformer (ViT), which learns to behave locally like CNNs through training and possesses long-range interactions via self-attention, to learn topographic properties. We find that a topographic ViT reproduces key topographic motifs, maintains high object categorization performance, and shows reduced inter- and intra-layer wiring length. We thus introduce a new class of topographic models that can express hypotheses about the roles of local vs. long-range cortical interactions in the brain."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Dynamical changes in functional organization resolve ambiguous motion perception",
    "presenter": "Alessandra Pizzuti",
    "poster_id": "A153",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Visual perception typically aligns with sensory input, but under bistable conditions, identical stimuli can lead to alternating percepts. While prior work has emphasized perceptual switches, less is known about how the brain stabilizes perception post-switch. Using 7T fMRI, we studied nine participants viewing a bistable motion quartet that induced spontaneous alternations between vertical and horizontal motion. Perceptual transitions involved dynamic interactions between hMT+ and frontoparietal regions (area 46, PF, PFm), while sustained perception engaged hMT+ and the intraparietal areas. Computational modeling revealed increased hierarchical complexity during ambiguity resolution, with frontal and parietal regions ascending in the functional hierarchy. These findings offer important constraints for modeling how the brain organizes itself to resolve perceptual ambiguity, and more broadly, how it supports conscious experience."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Head-Direction Cells in Postsubiculum Show Systematic Parallax Errors During Visual Anchoring",
    "presenter": "Sven Krausse",
    "poster_id": "A152",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Spatial navigation relies on the head-direction (HD) system, which integrates angular head velocity (AHV) to track orientation. Since integration accumulates drift over time, visual landmarks provide corrective cues. However, whether the HD system explicitly accounts for the apparent shift in proximal objects' position when viewed from different angles, remains unclear. These shifts are caused by the parallax effect, where closer objects move more strongly on the retina than distant ones.\n\nHere, we analyzed postsubicular HD cell activity in mice navigating with a single visual cue. We discovered a systematic parallax bias in the decoded HD, indicating that the HD system misinterprets the cue's position dependent on the viewing angle. The observed error was smaller than predicted by a pure vision model, which we show can be explained by the combination of AHV integration with simple visual anchoring. \n\nNotably, each animal exhibited a unique anchoring angle — the direction at which the cue was associated with head direction — suggesting that the HD system maintains a relatively stable and possibly learned mapping between the cue angle from visual input (bearing) and head direction.\n\nThese results provide evidence that the HD system, at least in simplified environments, does not perform explicit parallax correction but may instead attenuate errors passively through AHV integration and simple anchoring to multiple cues. This highlights a fundamental trade-off in neural coding between computational efficiency and positional accuracy, with implications for biological and artificial navigation systems."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "High-Level Perceptual Learning for Initially Ambiguous Stimuli",
    "presenter": "Logan T. Dowdle",
    "poster_id": "A155",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Perceptual skills are frequently studied using simple stimuli like Gabor patches to isolate basic perceptual mechanisms. However, in everyday expert decision-making, perceptual skills are deeply intertwined with higher-order cognition and semantic knowledge. For example, pathologists must accurately distinguish between various tissue types by interpreting complex visual patterns in microscopy images. To investigate how perceptual and semantic representations emerge for initially ambiguous stimuli and interact in the brain, we conducted a high-resolution 7T fMRI pilot study in which a lay participant learned to distinguish tissue with high and low tumor-infiltrating lymphocyte (TIL) count from histopathology images. The participant underwent daily perceptual training with feedback, paired with at-home semantic study of cancer types. fMRI data were collected in five sessions distributed over the training period wherein the participant performed a TIL classification task. This allowed us to track behavioral learning and corresponding changes in cortex-wide neural representations."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Ventral Stream Responses to Inanimate Objects are Equally Aligned with AlexNet (2012) and Modern Deep Neural Networks",
    "presenter": "Leilany Torres Diaz",
    "poster_id": "A156",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "The field of deep learning is continuously developing novel neural network architectures, from residual connections in CNNs (He et al., 2016) to vision transformers with self-attention mechanisms (Dosovitskiy et al., 2020). While these advances appear to increase models' visual capabilities, it remains unclear whether modern architectures are becoming more brain-aligned. To address this question, we examined an Inanimate Objects neuro-imaging dataset with reliable neural responses to 72 inanimate objects in early visual cortex (EarlyV), posterior occipito-temporal cortex (pOTC), and anterior occipito-temporal cortex (aOTC). We compared alignment between model feature spaces and voxel-space using classic, unweighted representational similarity analysis. We included top-performing models on the Brain-Score platform (Schrimpf et al., 2018) and a leading visual foundation model (DinoV2, Oquab, 2023). We found that an AlexNet baseline model (Krizhevsky et al., 2012) matches or exceeds these models in alignment with each brain region, with substantial reliable variance in aOTC remaining unexplained by any model. These results suggest that progress in deep neural network development is missing key aspects of high-level visual representation in the human brain."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Universally Controversial Stimuli Reveal that Adversarial Robustness Improves DNN Prediction Accuracy across the Entire Human Auditory Cortex",
    "presenter": "David Skrill",
    "poster_id": "A157",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Model comparison is central to all scientific progress. In sensory neuroscience, a key challenge is that distinct models often make similar neural predictions due to correlations between distinct features in the tested stimulus set. Here, we show how to distinguish models for a full neural population by designing a targeted “universally controversial” stimulus set that makes distinct, high variance predictions across an entire sensory cortical system (human auditory cortex) in every subject tested. We applied to compare the neural prediction accuracy of standard artificial neural networks (ANNs) from ANNs trained to be robust to “adversarial attacks”. Standard ANNs are notoriously vulnerable to small stimulus perturbations that can substantially alter the network’s decisions without meaningfully altering human perception. Yet, we find that the prediction accuracy of standard and robust ANNs in the human auditory cortex is virtually indistinguishable when measured using fMRI responses to natural sounds. In contrast, when tested with controversial stimuli, the cortical prediction accuracy of the robust model remains high throughout the auditory cortex, while the predictive power of the non-robust model drops to near zero. Universal controversiality thus opens the door to much more powerful model comparisons in sensory neuroscience and demonstrates a strikingly uncontroversial model improvement from adversarial training."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Testing a Learning Theory of Aesthetic Appeal Using Category Learning and Deep Neural Networks",
    "presenter": "Edward A Vessel",
    "poster_id": "A158",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "How do people mentally represent visual art, and how do those representations relate to aesthetic value? The learning theory of aesthetic valuation suggests that the aesthetic appeal we feel from engaging with visual objects is an affective signal for learning, and thus depends on how those objects relate to what we know about the visual world. Yet this theory is hard to test, given the difficulty of directly measuring the relevant aspects of an observer's internal perceptual models. We outline a behavioral and modeling paradigm for training observers in a visual artwork training task and, in parallel, tuning deep neural networks (DNNs) to serve as proxies for internal representations. Here we show that the task successfully modulated observer's knowledge and internal representations about a set of artworks, and we explore how architecture and training target affect the ability of DNNs to capture salient aspects of human observers' behavior."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "A more selective integration function to improve deep neural network models of visual perception",
    "presenter": "Michael W. Spratling",
    "poster_id": "A159",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Human visual perception remains substantially more robust than computer vision. We hypothesised that this might be due to the higher selectivity of biological neurons compared to artificial neurons which typically employ a linear integration function that is poor at feature detection. To test this hypothesis we replaced the convolutional layers in deep neural networks (DNNs) with a new integration function, the Consistent Intensity Metric (CIM). We trained networks based on CIM on six benchmark image classification tasks (MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100, and TinyImageNet) and compared the performance of these networks with equivalent convolutional neural networks matched to have an equal number of parameters. Consistent with our hypothesis, the CIM-based networks were better able to generalise from the training data. This was demonstrated by higher accuracy on both the standard test data and distorted input images (the common corruptions data-sets). Furthermore, test images that did not belong to any of the categories in the training data-set were less likely to be misclassified as belonging to one of the known categories. Our results suggest that using a more selective integration function can help address some of the reliability and robustness issues of DNNs. As these issues do not affect humans, this modification also makes DNNs functionally more similar to the biological visual system."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Deep neural networks can learn generalizable same-different visual relations",
    "presenter": "Alexa R. Tartaglini",
    "poster_id": "A160",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Although deep neural networks can achieve human-level performance on many object recognition benchmarks, prior work suggests that these same models fail to learn simple abstract relations, such as determining whether two objects are the same or different. Much of this prior work focuses on training convolutional neural networks to classify images of two same or two different abstract shapes, testing generalization on within-distribution stimuli. In this article, we comprehensively study whether deep neural networks can acquire and generalize same-different relations both within and out-of-distribution using a variety of architectures, forms of pretraining, and fine-tuning datasets. We find that certain pretrained transformers can learn a same-different relation that generalizes with near perfect accuracy to out-of-distribution stimuli. Furthermore, we find that fine-tuning on abstract shapes that lack texture or color provides the strongest out-of-distribution generalization. Our results suggest that, with the right approach, deep neural networks can learn generalizable same-different visual relations."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Modeling the Human Visual System: Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms",
    "presenter": "Shreya Saha",
    "poster_id": "A161",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Over the past decade, predictive modeling of neural responses in the primate visual system has advanced significantly, driven by diverse deep neural network approaches. These include models optimized for visual recognition, cross-modal alignment through contrastive objectives, neural response prediction from scratch, and embeddings from large language models (LLMs). Additionally, various readout mechanisms—from fully linear to spatial-feature factorized methods—have been developed to map network activations to neural responses. Despite this progress, it remains unclear which approach performs best across different regions of the visual hierarchy.\nIn this study, we systematically compare these methods for modeling the human visual system and propose novel strategies to enhance response predictions. We demonstrate that the choice of readout mechanism significantly impacts prediction accuracy and introduce a biologically grounded readout that dynamically adjusts receptive fields based on image content and learns geometric invariances of voxel responses directly from data. This novel readout outperforms factorized methods by 3-23\\% and standard ridge regression by 7-53\\%, setting a new benchmark for neural response prediction.\nOur findings reveal distinct modeling advantages across the visual hierarchy: response-optimized models with visual inputs excel in early to mid-level visual areas, while embeddings from LLMs—leveraging detailed contextual descriptions of images—and task-optimized models pretrained on large vision datasets provide the best fit for higher visual regions. Through comparative analysis, we identify three functionally distinct regions in the visual cortex: one sensitive to perceptual features not captured by linguistic descriptions, another attuned to fine-grained visual details encoding semantic information, and a third responsive to abstract, global meanings aligned with linguistic content. Together, these findings offer key insights into building more precise models of the visual system."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Sampling Rate and Task Selection in EEG-Authentication",
    "presenter": "Polina Tapal",
    "poster_id": "A162",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "EEG-based authentication is a promising security alternative, measuring unique neural patterns, but variability in sampling rate ($f_s$) across datasets can distort performance metrics. This study evaluates how fs normalisation impacts authentication accuracy by analysing multiple EEG datasets with differing $f_s$. Using machine learning classification pipeline, we show that optimal $f_s$ selection depends critically on task-specific neural dynamics: high-frequency gamma tasks require ≥500 Hz, while alpha-dominated paradigms perform well at 128 Hz, and auditory potentials remain stable even at 98 Hz. Notably, prefrontal tasks show inherent limitations unaffected by $f_s$. Findings emphasise the need for standardised paradigm-specific $f_s$ in EEG authentication to improve reproducibility and robustness. This work provides practical insights for optimising biometric systems and advancing EEG-based authentication."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Up-down states and criticality in a chain of adaptive excitable integrators",
    "presenter": "Mario Martinez-Saito",
    "poster_id": "A163",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "We describe a chain of unidirectionally coupled adaptive excitable elements slowly driven by a stochastic process from one end and open at the other end, as a minimal toy model of unresolved irreducible uncertainty in a system performing inference through a hierarchical model. Threshold potentials adapt slowly to ensure sensitivity without being wasteful. Activity and energy are released as intermittent avalanches of pulses with a discrete scaling distribution largely independent of the exogenous input form. Subthreshold activities and threshold potentials exhibit Lorentzian temporal spectra, with a power-law range determined by position in the chain. Subthreshold bistability closely resembles empirical measurements of intracellular membrane potential. We suggest that critical cortical cascades emerge from a trade-off between metabolic power consumption and performance requirements in a critical world, and that the temporal scaling patterns of brain electrophysiological recordings ensue from weighted linear combinations of subthreshold activities and pulses from different hierarchy levels."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Mapping Modular Processing of Compressed Videos Across Human Visual Cortex",
    "presenter": "Christina Sartzetaki",
    "poster_id": "A154",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "How the brain makes sense of the constant stream of visual information it receives largely remains a mystery. One prominent idea is that it evolved specialized pathways for sparse cortical engagement, and those can be accurately captured with handcrafted features; however, deep neural network (DNN) features overall align better with brain representations. In this work we study the brain alignment of a multi-pathway DNN that leverages compressed video formats, and partition the variance captured between its three modular components across visual brain regions recorded with fMRI during video stimuli. We find that its components map well to known brain pathways, and that it captures overall more variance than a 3D convolutional network. Achieved using only existing features in the compressed format, this points to the ineffectiveness of conventional full-frame processing for explaining brain responses to dynamic stimuli and to compression as a potential solution."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Harmonizing resting state functional MRI data to increase sample size and to classify tinnitus brain connectivity",
    "presenter": "Shagun Ajmera",
    "poster_id": "A164",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "The neuroscientific understanding of tinnitus, or ringing in the ears, is limited at present, partly due to a lack of sufficient neuroimaging data. However, data scarcity can be overcome with ‘harmonization’ methods if scans from independent sites and studies are pooled successfully. We achieved data harmonization by merging a modest tinnitus fMRI dataset acquired in our lab (n=35) with another large dataset, the Lifespan Human Connectome Project Aging (n=377). We measured resting-state functional connectivity (FC) between brain regions, and used deep learning architecture to purge dataset-identifying information from FC maps while retaining characteristic patterns of tinnitus-related FC. Hallmark artifactual information of the datasets was significantly reduced while reconstructing individual FC data. Default mode network connectivity was identified to be crucial for distinguishing tinnitus, as masking the network’s connections severely impacted downstream classification on reconstructed FC. The data harmonization model was successful in merging FC matrices from independently acquired fMRI datasets, without losing out on meaningful functional connectivity patterns of interest for the neuropsychological disorder."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Neural oscillations encode context-based informativeness during naturalistic free viewing",
    "presenter": "Songyun Bai",
    "poster_id": "A165",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "In everyday vision, humans actively sample the environment by moving their eyes several times per second. While predictive processing accounts have been successful in explaining neural oscillatory activity during fixated viewing in non-human primates, whether these accounts extend to free viewing and humans is still unknown. To address this, we developed a novel analysis pipeline combining large-sample, head-fixed MEG, eye-tracking, and a generative deep neural network model to investigate how the brain encodes visual input with varying levels of contextual predictability. Our results show that the informativeness of the current fixation is positively associated with the occurrence of alpha-beta oscillations across large parts of posterior cortex. It is furthermore positively associated with gamma-band activity, though more specifically localized to central posterior regions corresponding to the foveal representation. In conclusion, contextual predictability is rapidly and transiently encoded in neural oscillations in different frequency bands during free viewing."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Multi-task batteries for individual brain mapping: Experimental design and implementation",
    "presenter": "Jörn Diedrichsen",
    "poster_id": "A167",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "While the characterization of individual human brain organization with functional magnetic resonance imaging (fMRI) has in the past relied heavily on resting-state data, it has been shown that a more powerful identification of functional brain organization can be achieved with batteries including a broad set of tasks. Following practical considerations, these multi-task datasets are often designed such that each imaging run includes only a small number of similar tasks or conditions, such that most task-task contrasts have to be made across fMRI runs. Here we show that a design in which all tasks are measured repeatedly within the same imaging run is statistically superior both for estimating tasks-rest contrasts, as well as any task-to-task contrast. An interspersed multi-task design leads to more predictive brain parcellations and connectivity models, even though the design requires participants to constantly switch between tasks. We present a flexible Python toolbox that implements 20+ common tasks with this design, and that automatizes the generation of multi-task batteries for fMRI experimentation. Furthermore, we provide a framework for sharing and integrating pre-processed data across a growing number of multi-task datasets."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Automated characterization of naturalistic behaviors in a chronic model of epilepsy",
    "presenter": "Aurélie De Groote",
    "poster_id": "A168",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Epilepsy encompasses a set of complex, multifaceted disorders presenting a large panel of disease symptoms. A deeper understanding of their underlying disease mechanisms is likely to be required for the development of disease-modifying therapies (Gschwind et al., 2023; Lignani, Baldelli \u0026 Marra, 2020). Several forms of epilepsy are characterized by changes in gene expression profiles of neuronal networks that lead to significant alterations at the neuronal network and behavioral levels. In this study, we investigate the behavioral phenotype of a chronic epilepsy model by leveraging machine learning algorithms to analyze long-term video data of mice in naturalistic settings. We aim to identify behavioral markers beyond seizures and assess the impact of potential therapeutic treatments. Describing their behaviors in terms of behavioral modules will also allow us to better understand behavioral transitions and to capture correlations between neural pathways and behavior in healthy and pathological conditions."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "nCREANN:Nonlinear Brain Connectivity and Diverse Applications",
    "presenter": "Nasibeh Talebi",
    "poster_id": "A169",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Understanding directed brain connectivity is crucial in neuroscience, yet traditional linear connectivity methods may oversimplify neural interactions. We introduce nCREANN, nonlinear causal relationship estimation by artificial neural networks, which models the neural dynamics by a nonlinear multivariate autoregressive (nMVAR) process and estimates directed connectivity. This method leverages the Taylor expansion of nonlinear input-output mapping of the neural network to dissociate linear and nonlinear connectivity patterns. We summarize considerations and diverse applications of nCREANN in neuroscience studies. Results highlight distinct linear and nonlinear connectivity patterns in Autism Spectrum Disorder (ASD) and Attention-Deficit/Hyperactivity Disorder (ADHD) subjects, a superior classification accuracy of ADHDs (up to 99%), and deeper insights into neural mechanisms underlying adaptive behavior, event segmentation, metacontrol processes, and dynamic working memory gating. nCREANN provides a powerful tool for uncovering nuanced brain dynamics and enhancing understanding of neural disorders."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Graph-Attention-Based Integration of Brain Structure and Function for Trait Anxiety Prediction: Preliminary Results",
    "presenter": "Jungyoun Janice Min",
    "poster_id": "A166",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Trait anxiety is a stable personality trait linked to increased vulnerability for internalizing disorders. Although altered intrinsic activity in individuals with trait anxiety has been frequently reported in resting-state fMRI studies, its relationship to structural connectivity, which is axonal pathways of large-scale brain dynamics, remains underexplored. Leveraging the LEMON dataset (N = 132), we trained a graph-attention network integrating temporally structured functional signals at rest with subject-specific structural constraints. Our model outperformed a traditional structure–function coupling baseline, achieving statistically significant prediction (r = 0.194, p = 0.026). Attention based interpretation highlighted importance of frontal–parietal and occipital pathways, suggesting that the attentional and sensory networks may contribute to trait anxiety."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Working memory synchronizes oscillations in visual cortex",
    "presenter": "Mrugank Dake",
    "poster_id": "A170",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Working memory (WM) extends the duration over which representations are available to guide behaviors, and supports a wide range of cognitive functions. Here, we test the hypothesis that WM induces oscillations in the beta band that support stimulus representations that are behaviorally meaningful. Using MEG, we find that oscillations in the beta range over visual cortex a) change in topographic patterns of power during the delay, b) contain item-level information about memoranda, c) synchronize with PFC, and d) predict trial-wise memory behavior. Thus, we conclude that visual WM depends on synchronous oscillations in and between visual and pre-frontal cortex."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Multi-task batteries for individual brain mapping: Optimal battery selection for brain parcellation and connectivity modeling",
    "presenter": "Bassel Arafat",
    "poster_id": "A171",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Group-level atlases are commonly used in neuroimaging\nto define regions-of-interest (ROIs) - however, they ignore\nthe substantial inter-individual variability in brain organization.\nWhile resting state data can be used to derive\nindividual functional maps, recent work has shown that\nmaps obtained with a broad task battery generalize better\nto new mental states. With limited scanning time an important\nquestion becomes which tasks to choose for an\noptimal task battery. Here we propose to base this selection\non the empirical activity maps themselves, and evaluate\ntwo selection strategies: One that seeks to maximize\nthe imaging contrast in the region of interest (activation\nstrength) and one that seeks to maximize the independence\nof different subregions (representational spread).\nUsing simulations and real fMRI data, we show that representational\nspread consistently yielded better performance\nfor brain parcellations and connectivity models.\nIn simulations, representational spread outperforms activation\nstrength and random selection for batteries from\n3-16 tasks. We confirm these findings for real fMRI data,\nfor the cases of cerebellar and cortical parcellations, and\na cortico-cerebellar connectivity model. Our study therefore\noffers an automated method for optimizing task battery\nselection for different brain areas and demonstrates\nthe value of principled task selection for individual brain\nmapping."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Evidence accumulation across the senses in the face of causal uncertainty",
    "presenter": "Jochem Beurskens",
    "poster_id": "A172",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Humans need to make accurate and timely decisions based on constant influx of noisy sensory signals. They should integrate signals from common causes, but segregate those from separate causes. Previous research has shown that the brain arbitrates between integration and segregation consistent with Bayesian Causal Inference models (BCI). However, these static models ignored the dynamics of perceptual decision making and therefore could not account for response times. Using psychophysics, we show that the influence of spatially disparate visual signals on observers' perceived sound location declines with longer response times. This pattern is best captured by a dynamic BCI model that accumulates evidence jointly about the signals' locations and their causal structure (i.e. common vs. independent causes) over time in a forgetful fashion, until a decisional threshold is reached. By accounting for both response choices and times these dynamic BCI models advance our understanding of how observers dynamically combine signals from multiple sensory modalities in the face of causal uncertainty. They provide a novel perspective on previous neuroimaging results showing a progression  from fusion to BCI multisensory interactions along cortical pathways."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Representations of past and future event boundaries from naturalistic experience are reactivated in the Default Mode Network during recall",
    "presenter": "Avital Hahamy",
    "poster_id": "A174",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Replay, the reactivation of neural sequences from past experiences, has been postulated to support memory consolidation and planning during real-time animal behavior. We test this hypothesis in a human fMRI movie-recall task. We find that during recall, Default Mode Network regions reactivate both past and future representations of movie event boundaries (moments of scene transitions), potentially supporting the tracking and planning of narrative recall. We postulate that at event boundaries,  replay-like mechanisms in humans construct a cognitive model of unfolding experiences which facilitates narrative regeneration from memory."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Partially recurrent neural networks maximize performance and minimize wiring",
    "presenter": "Marcus Ghosh",
    "poster_id": "A173",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Many circuits in the brain are bidirectional and sparse. Meaning that signals flow from sensory inputs to later areas and back; yet, between any two connected areas there exist some but not all pathways. What advantages or disadvantages do these architectures confer, compared to feedforward or fully connected networks? To address this question, we introduce a new class of partially recurrent neural network architectures, between these two extremes. An exhaustive search of these architectures reveals significant differences in their performance, learning speed and robustness to noise. Though, surprisingly, many perform as well as, or even better than, fully connected networks, despite having fewer parameters (a proxy for wiring cost). To explain these functional differences, we show that different architectures learn distinct input-output mappings and memory dynamics, both of which are predictive of function. Ultimately, our results demonstrate that partial recurrence allows networks to maximize performance with minimal wiring. More broadly, our work provides a general framework for linking network structure to function."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Rhythmic interactions between early visual areas and prefrontal cortex  predict bistable perception",
    "presenter": "Yanni Zhang",
    "poster_id": "A176",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Visual perception integrates external stimuli with the brain’s intrinsic dynamics, particularly under ambiguity. Predictive processing posits that hierarchical oscillations act as dynamic priors to resolve uncertainty, yet how cross-regional signals are temporally coordinated remains unknown. To address it, we combined the bistable Ternus paradigm—eliciting element motion (EM) or group motion (GM), depending on the observer's internal state—with intracranial EEG (iEEG) in six patients to explore prefrontal-visual oscillatory dynamics. We found that prestimulus alpha phase in early visual areas (V1-V3) and theta phase in prefrontal cortex (PFC) predicted perceptual outcomes. Directed connectivity analyses revealed stronger low-frequency (2-8 Hz) coupling from PFC to visual regions during EM percept, indicating top-down predictive signaling. These findings suggest that coordinated pre-activated dynamics between early visual areas and the PFC play a critical role in shaping perceptual outcomes, offering insights into how intrinsic neural processes influence conscious visual perception."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "SwiFT V2: Towards Large-scale Foundation Model for Functional MRI",
    "presenter": "Jiook Cha",
    "poster_id": "A175",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Foundation models, leveraging large-scale datasets and extensive parameter counts, show unprecedented capabilities across various domains. Recent studies have explored foundation models for neuroimaging to effectively capture the complex dynamics of the human brain. However, training such models end-to-end on four-dimensional functional MRI data remains unexplored. Here, we introduce SwiFT V2, a fMRI foundation model based on the 4D Swin fMRI Transformer. We pre-trained SwiFT V2 using masked image modeling on large-scale aggregated resting-state fMRI datasets of 49,321 subjects. Especially, we trained models up to 8.8 billion parameters with maximal update parameterization technique, leading to stable and efficient scaling. We observed that these models follow neural scaling laws, where performance predictably improves with scale. Also, we showed that masked modeling pre-training enhances performance across various downstream tasks. These results validate the application of scaling principles to fMRI modeling and motivate the further development of large foundation models for neuroscience."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Communication versus computation: The hidden costs, shaping the brain's architecture",
    "presenter": "Kayson Fakhar",
    "poster_id": "A177",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Large-scale human brain networks exhibit complex topological characteristics, likely reflecting a balance among competing objectives such as minimizing wiring cost and maximizing communication efficiency. Interestingly, computational modelling has suggested that the connectivity of the brain is biased towards enhanced communication rather than a minimized wiring cost. Yet, the relationship between such communication efficiency and the functional capacity of the brain, e.g., to solve computational problems, remains unclear. To address this question, we used a game-theoretical framework in which individual brain regions establish connections only if it improves their signalling efficiency, given the wiring cost. We show that, firstly, complex network architectures naturally emerge from these local interactions, capturing some hallmarks of the brain. Secondly, resulting networks have both superior communication and reduced wiring cost compared to empirical brain networks. However, these optimal networks exhibited diminished memory capacity relative to empirical networks. Our findings suggest that efficient communication does not necessarily translate to improved computation. Instead, functional capacity may have played an essential role in shaping brain network architecture, potentially even at the expense of communication efficiency."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Learning richness modulates equality reasoning in neural networks",
    "presenter": "William Lingxiao Tong",
    "poster_id": "A179",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Equality reasoning is ubiquitous and purely abstract: sameness or difference may be evaluated no matter the nature of the underlying objects. As a result, same-different (SD) tasks have been extensively studied as a starting point for understanding abstract reasoning in humans and across animal species. With the rise of neural networks that exhibit striking apparent proficiency for abstractions, equality reasoning in these models has also gained interest. Yet despite extensive study, conclusions about equality reasoning vary widely and with little consensus. To clarify the underlying principles in learning SD tasks, we develop a theory of equality reasoning in multi-layer perceptrons (MLP). Following observations in comparative psychology, we propose a spectrum of behavior that ranges from *conceptual* to *perceptual* outcomes. Conceptual behavior is characterized by task-specific representations, efficient learning, and insensitivity to spurious perceptual details. Perceptual behavior is characterized by strong sensitivity to spurious perceptual details, accompanied by the need for exhaustive training to learn the task. We develop a mathematical theory to show that an MLP's behavior is driven by *learning richness*. Rich-regime MLPs exhibit conceptual behavior, whereas lazy-regime MLPs exhibit perceptual behavior. We validate our theoretical findings in vision SD experiments, showing that rich feature learning promotes success by encouraging hallmarks of conceptual behavior. Overall, our work identifies feature learning richness as a key parameter modulating equality reasoning, and suggests that equality reasoning in humans and animals may similarly depend on learning richness in neural circuits."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Emergent oscillations in a cortical column model of predictive coding with multiple interneuron types",
    "presenter": "Kwangjun Lee",
    "poster_id": "A178",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "We propose a biologically grounded computational model of predictive coding (PC) that integrates a neuroanatomically informed hierarchy of cortical areas with laminar organization and cell-type-specific connectivity. The model performs PC on naturalistic images through Hebbian learning and prediction error minimization. The model assumes that stereotypical pyramidal-PV-SST-VIP circuits with the same structure but different bottom-up and top-down inputs compute positive and negative prediction errors. Sensory inference in the model generates neural oscillations, with simulations of optogenetic inactivation revealing distinct roles for PV, SST, and VIP cells in these dynamics. Furthermore, the model exhibits mismatch negativity-like responses to deviant stimuli. This work offers a biologically plausible framework for understanding the neural circuits underlying PC in the cortex."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Understanding Diverse Reasoning Procedures in Foundation Models via Mechanistic Interpretability",
    "presenter": "Mohanna Hoveyda",
    "poster_id": "A180",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Foundation models exhibit impressive performance on tasks that appear to require a wide range of reasoning abilities. \nHowever, they struggle to generalize under distribution shifts and struggle with reasoning problems that are trivial for humans.\nThese inconsistencies raise a critical question: which internal mechanisms, if any, underlie the successes and failures of these models in reasoning tasks?\nWhile numerous benchmarks have been proposed to probe reasoning capabilities, our understanding of the underlying mechanisms responsible for such reasoning-like behavior remains limited. \nWe hypothesize that \\textit{distinct reasoning procedures are supported by specialized, possibly modular, computational pathways} in large-scale models. \nMechanistic interpretability (MI) offers a promising set of tools to identify and analyze such pathways. However, most existing work operates in an isolated manner: evaluating a particular model for a particular reasoning task, often in a single modality. \nTo address this gap, we first lay out a high-level taxonomy of reasoning processes and then conduct a systematic analysis of how mechanistic interpretability has been used to investigate diverse reasoning processes in various foundation models, across three main axes: (i) reasoning type, (ii) MI technique, and (iii) modality. \nWe aim to develop a broader understanding of whether (A) different reasoning processes share computational mechanisms or are supported by distinct subsystems, and whether (B) such mechanisms are consistent across modalities other than text, such as vision."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Framed RSA: honoring representational geometry and regional-mean response preferences",
    "presenter": "JohnMark Taylor",
    "poster_id": "A182",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Representational similarity analysis (RSA) characterizes the geometry of neural activity patterns elicited by different stimuli while discarding their regional-mean activity and the location or orientation of the patterns in multivariate response space. Regional-mean activation analysis serves the complementary purpose of comparing the average population response to different stimuli. Here we introduce a novel method, framed RSA, which honors both the geometry and the regional-mean preferences in evaluating model-predicted representations. To achieve this, we augment the stimulus patterns with two reference patterns: the zero-point (origin) and a uniform constant pattern, enabling RSA to incorporate information about the global location, orientation, and mean activation of neural population codes. Framed RSA improves accuracy for both brain region identification (using fMRI data from the Natural Scenes Dataset) and deep neural network layer identification relative to existing RSA approaches. Framed RSA thus combines the strengths of two complementary and traditionally separate analysis approaches, and improves power for model-comparative inference."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Adaptive decoding of temporally variable neural activity in single-trial time series",
    "presenter": "Pablo Oyarzo",
    "poster_id": "A181",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Cognitive processes, specially those involving higher-order functions, often unfold with temporal variability. This complicates the use of time-locked analysis techniques, including standard machine learning-based decoding methods. Although existing methods perform well in tasks with externally timed events, decoding covert processes -such as imagery or recall- remains difficult due to uncertainty in the timing of the underlying neural dynamics. In these cases, task-relevant neural signals may occur at variable latencies across trials, violating the temporal alignment assumptions of standard decoding models.\nWe introduce the Adaptive Decoding Algorithm (ADA), a nonparametric framework for decoding under temporal uncertainty. ADA performs two coupled tasks: (i) it estimates, for each trial, the temporal window most likely to reflect task-relevant neural activity, and (ii) it uses this information to decode the trial label.\nUsing controlled simulations, we show that ADA outperforms conventional methods that assume fixed temporal structure. These results demonstrate that explicitly modeling trial-specific timing can substantially improve decoding performance in scenarios where the timing of relevant neural activity is unknown."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Neuroinflammatory Cascades in Urethritis-Mediated Cognitive Dysfunction: Computational Frameworks and Multiscale Modeling Perspectives",
    "presenter": "Saket Ram Ganti",
    "poster_id": "A183",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Emerging research reveals a compelling and underrecognized connection between urological inflammation and cognitive dysfunction, particularly in the context of urethritis. Traditionally viewed as a localized condition affecting the lower urinary tract, urethritis is now gaining attention for its potential to trigger systemic effects, including neuroinflammatory responses that disrupt brain function. This paper investigates the hypothesis that urethritis-induced inflammation may contribute to cognitive impairment through a cascade of immunological, biochemical, and neural interactions. We explore how peripheral immune activation can influence central nervous system (CNS) homeostasis, leading to alterations in neural circuitry and behavior. By integrating insights from neuroimmunology, computational neuroscience, and systems-level modeling, we propose a multidimensional framework for understanding how peripheral pathology can manifest as central dysfunction. This interdisciplinary approach not only bridges the gap between urology and neuroscience but also shows new avenues for early diagnosis, predictive modeling, and targeted interventions for inflammation-associated cognitive decline."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Universal Differential Equations as a Common Modeling Language for Neuroscience",
    "presenter": "Ahmed ElGazzar",
    "poster_id": "A184",
    "topic_area": "Methods and Computational Tools",
    "abstract": "The rise of large-scale neuroscience datasets has driven widespread adoption of deep neural networks (DNNs) as models of biological neural systems. While DNNs can approximate functions directly from data circumventing the need for mechanistic modeling, they risk producing implausible and difficult-to-interpret models.\nIn this paper, we argue for universal differential equations (UDEs) as a unifying approach for model development and validation in neuroscience. UDEs view differential equations as parameterizable, differentiable mathematical objects that can be augmented and trained with scalable deep learning techniques. This synergy facilitates the integration of classical mathematical modeling with emerging advancements in AI into a potent framework. We provide a primer on this burgeoning topic in scientific machine learning and \ndescribe a generative modeling recipe for fitting UDEs on neural and behavioral data. Our goal is to show how UDEs can fill in a critical gap between mechanistic, phenomenological, and data-driven models in neuroscience\nand highlight their potential to address inherent challenges across diverse applications such as understanding neural computation, controlling neural systems, neural decoding, and normative modeling."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Comparing different criteria for neural dimensionality estimation",
    "presenter": "Francesco Edoardo Vaccari",
    "poster_id": "A185",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Despite dimensionality reduction is essential in modern Neuroscience and Principal Component Analysis (PCA) continues to serve as the standard approach, in the field it is still missing a widely accepted criterion for choosing the number of components to retain. To fill this gap, we aimed to compare the performance of different retention criteria. We designed a data simulation procedure to generate data matrices with a ground-truth latent structure. Simulation parameters were varied to compare the different retention criteria in several scenarios. Among the tested criteria, Parallel Analysis and a cross-validation scheme, specifically conceived for dimensionality reduction, resulted to be the most effective methods. Finally, by applying these criteria to real spiking activity, we show that different criteria can lead to significantly different results in the estimation of dimensionality and noise. Our study highlights the need for an explicit definition of “dimensionality” in the analysis of population spike activity and a consequent careful choice of the retention criterion to be used, as this can lead to important biases and non-comparable results between studies."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Sequential Memory Generation in the Neuroidal Model",
    "presenter": "Patrick Riley Perrine",
    "poster_id": "A186",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Understanding the process of memory formation in neural systems is of great interest in the field of neuroscience. The Neuroidal Model poses a plausible theory for how memories are created within a computational context. Previously, the algorithm JOIN has been used to show how the brain could perform conjunctive and disjunctive coding to store memories. A limitation of JOIN is that it does not consider the coding of temporal information in a meaningful manner. We propose $\\texttt{SeqMem}$, a similar algorithmic primitive that is designed to encode a series of items within a random graph model. We investigate the feasibility of this procedure empirically by observing its stability in our model. Our goal here is to inspire further work in scaling our methods to function at a human-level magnitude of computation."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Representational tuning models: Uniting representational similarity analysis and neural encoding models.",
    "presenter": "Stan Bergey",
    "poster_id": "A187",
    "topic_area": "Methods and Computational Tools",
    "abstract": "In fMRI analysis, neural encoding models reveal how individual voxels respond as a function of continuous stimulus parameters, and how response function parameters change within and between brain areas. Conversely, representational similarity analysis reveals structure in the responses of a region of interest (ROI) to any stimuli. Here we develop representational tuning models to unite these approaches. These response models first rescale the representational dissimilarity matrix (for an ROI) to a 2-dimensional representational space then find (for each voxel) the Gaussian function within this 2D space that best predicts the voxel’s response to all stimuli. By deriving continuous response function parameters from the ROI’s responses, this approach requires no a priori hypothesis of stimulus parameters underlying the response function. It thereby allows application of neural encoding models to arbitrary stimulus sets. We test this approach for responses from the Natural Scenes Dataset within 12 visual field maps. We show that representational tuning models significantly predict voxels’ responses to natural images in higher-level (but not early) visual field maps, especially when sampling from other visual field maps, and we demonstrate that the principal components of representational spaces reflect the spatial structure of responses across the cortical surface within an ROI."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Interpretation of Individual Differences in Cognitive Computational Neuroscience",
    "presenter": "Jessica Schaaf",
    "poster_id": "A188",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Computational neuroscience offers a valuable opportunity to understand the neural mechanisms underlying behavior. Suppose that you fit a computational model to behavioral data to generate an individual-specific prediction error regressor. You in turn use this regressor to model activity in a brain region of interest. What do individual differences in the resulting regression weights mean? Typically researchers interpret these individual differences as differences in neural coding. Yet, in five scenarios, we illustrate through simulations that such individual differences may stem from other factors. By acknowledging these alternative interpretations of individual differences, and by openly sharing reproducible code, we aim to advance the understanding and interpretation of individual differences in computational neuroscience."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Using Llama-3 to Refine Psychotherapy in Silico",
    "presenter": "Kristin Witte",
    "poster_id": "A190",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Psychotherapy is deeply personal and often time-intensive. Although therapeutic interactions crucially impact treatment outcomes, strategies to improve them often rely on trial and error, often resulting in a long and costly process with minimal improvement for the client.\nThis project explores the potential of Large Language Models (LLMs) as low-risk, cost-effective tools for enhancing therapy. Using Llama-3, we simulated therapist-client dialogues, supervised by an expert LLM. The Expert LLM iteratively refined the Therapist LLM's responses, which were subsequently rated by the  Client LLM.\nTo extend this framework to real-world data, we applied LLM therapy revision to real therapy transcripts. For each segment of a recorded session, we compared the Client LLMs rating of the real therapist's last response to ratings of an LLM generated response and an LLM-based revision of the actual therapist's response, assessing satisfaction based on prior conversation context. These comparisons revealed that LLM-generated responses were often rated more favorably than human responses, with responses  revised based on LLM feedback rated most favorably. This suggests that LLMs could meaningfully support and enhance therapeutic interactions, and improve quality of treatment."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Functional Inter-Subject Alignment Outperforms Anatomical Alignment on fMRI Data in Inter-Subject Information Transfer",
    "presenter": "Leo Michalke",
    "poster_id": "A189",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Group-level analysis in neuroscience requires precise alignment of neural data across participants. A common approach is to spatially align brains and use spatial smoothing to enhance inter-individual overlap. However, this purely anatomical approach makes strong assumptions about functional-anatomical coupling that are likely violated due to substantial inter-individual variability in functional neuroanatomical organization.\nIn this work, we compare multiple methods which aim to find a common representation of functional magnetic resonance imaging (fMRI) data across participants. The data was recorded from 30 participants listening to naturalistic auditory stimuli (Forrest Gump audio movie). We compare the standard anatomical approach (MNI space combined with spatial smoothing) and three inter-subject alignment methods (multiset canonical correlation analysis (MCCA), Kettenring 1971; group ICA, Calhoun 2009; Hyperalignment, Xu 2012) which seek to find a functional alignment by maximizing similarity of activation time-series between subjects in a latent space under different constraints.\nIn order to evaluate the inter-subject information transfer of the different alignment methods, we designed a classification task based on decoding the occurrence of function versus content words in the audio movie. Inter-subject classifiers were trained on the aligned data from one set of subjects and tested on a held-out subject in a leave-one-subject-out fashion.\nThe results show that functional inter- subject alignment methods (accuracy: MCCA, 0.638; ICA, 0.637; Hyperalignment, 0.611; chance level 0.5) greatly outperform the standard anatomical alignment method (MNI space, 0.508). This indicates that the important features for across subject generalization lie within the latent functional spaces, while anatomical-functional representations can be idiosyncratic. \nOur work demonstrates that functional inter-subject alignment has the potential to improve the generalizability of data representations when combining data of different subjects."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "High Entropy Deep Brain Stimulation for Treatment Resistant Depression",
    "presenter": "Lily Chamakura",
    "poster_id": "A191",
    "topic_area": "Methods and Computational Tools",
    "abstract": "A key challenge for reliably treating psychiatric disorders such as depression through deep brain stimulation (DBS) is characterizing the individual brain responses to electrical stimulation over the target stimulation space. Here, we propose the high-entropy stimulation paradigm that can sample from a wide array of spatiotemporal patterns, and explore a much larger and more natural portion of the stimulation space than conventional piecewise constant pulse trains. We used the high entropy stimulation paradigm to stimulate a patient with treatment-resistant depression who had implanted DBS electrodes, using a custom-built GUI, and the patient's ongoing brain activity was recorded using intracranial stereo encephalogram (sEEG). We show that the stimulation modulates the responses along a low-dimensional manifold spanned by the evoked responses in pre-frontal brain regions. Overall, by generating richer and more natural patterns of electrical stimulation, the proposed high entropy stimuli are useful to efficiently probe the influence of external stimulation on brain states."
  },
  {
    "start": "2025-08-12T01:30:00Z",
    "end": "2025-08-12T16:30:00Z",
    "title": "Bridging Critical Gaps in Convergent Learning: How Representational Alignment Evolves Across Layers, Training, and Distribution Shifts",
    "presenter": "Chaitanya Kapoor",
    "poster_id": "A192",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Understanding convergent learning---the degree to which independently trained neural systems---whether multiple artificial networks or brains and models---arrive at similar internal representations---is crucial for both neuroscience and AI. Yet, the literature remains narrow in scope---typically examining just a handful of models with one data distribution, relying on one alignment metric, and evaluating networks at a single post-training checkpoint. We present a large-scale audit of convergent learning, spanning dozens of vision models and thousands of layer-pair comparisons, to close these long-standing gaps.  First, we pit three alignment families against one another---linear regression (affine-invariant), orthogonal Procrustes (rotation-/reflection-invariant), and permutation/soft-matching (unit-order-invariant). We find that orthogonal transformations align representations nearly as effectively as more flexible linear ones, and although permutation scores are lower, they significantly exceed chance, indicating a privileged representational basis. Tracking convergence throughout training further shows that nearly all eventual alignment crystallizes within the first epoch---well before accuracy plateaus---suggesting it is largely driven by shared input statistics and architectural biases, not by convergence towards the final task solution. Finally, when models are challenged with a battery of out-of-distribution images, early layers remain tightly aligned, whereas deeper layers diverge in proportion to the distribution shift. These findings fill critical gaps in our understanding of representational convergence, with implications for neuroscience and AI."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Dissociable Effects of Uncertainty in Perceptual and Cognitive Control",
    "presenter": "Julian Q. Kosciessa",
    "poster_id": "B2",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "The ability to manage uncertainty is a hallmark of flexible control. But uncertainty is not a unitary construct: it arises from various sources that challenge diverse processing domains – from ambiguous perceptual inputs to conflicting cues about task-relevant features. To disentangle uncertainty sources in perceptual and cognitive control, we independently manipulated perceptual uncertainty (relative choice evidence) and task uncertainty (uncertainty about relevant feature sets) in a dynamic perceptual decision task. Across three experiments, we observed a double dissociation in behavioral effects: perceptual uncertainty reduced accuracy, while task uncertainty primarily slowed response times. Conceptually replicating and extending prior work, functional MRI revealed robust engagement of a fronto-thalamic network in response to task, but not perceptual, uncertainty. We propose that thalamocortical circuits track uncertainty in a differentiated fashion to exert domain-specific control. By establishing robust and dissociable effects, this work provides a foundation for understanding how the human brain manages diverse uncertainties."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Subjective, more than objective, expectation and surprise explain perceptual decisions during learning",
    "presenter": "Jessye Clarke",
    "poster_id": "B5",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Under popular ‘predictive coding’ accounts in cognitive neuroscience, the brain continuously generates predictions about sensory input and integrates them with incoming signals in order to form a percept. There is evidence that these perceptual predictions and corresponding prediction errors can be entirely implicit. However, we are selectively aware of some violations of objective statistical structure - triggering conscious experiences of surprise. In order to investigate the influence of this subjective awareness on learning and perception, we conducted two behavioural studies pairing a probabilistic perceptual discrimination task with trial-by-trial ratings of subjective expectation (Experiment 1) and surprise (Experiment 2). We found that the subjective experience associated with predictions and prediction errors can explain independent variance in behaviour to that explained by the ‘objective’ expectedness or prediction error parameter of a learning model. This suggests that beyond just the presence of a statistically probable or improbable event, subjective awareness of statistical regularities or prediction errors influences downstream stimulus processing and behavioural responses during perception and learning."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural Mechanism of Compositional Generalization",
    "presenter": "Zilu Liang",
    "poster_id": "B3",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Compositionality is a fundamental feature of cognition. Humans can break down learned knowledge into constituents and reassemble them flexibly to solve new problems. Using fMRI, we investigated the neural underpinnings of compositionality in a task where discrete features indicated locations on spatial axes. Participants were trained on a subset of stimuli with feedback and tested on the held-out without feedback. Successful generalization required decomposing the trained stimulus-location associations into rules and recombining the rules to solve the test. Different brain regions adopted distinct representation strategies: rules were represented in high-dimensional parallel manifolds in the hippocampus and composed by vmPFC to solve the test; the superior parietal gyrus put the test stimuli into a low-dimensional spatial reference frame; V1 represented both training and test stimuli as their locations on the ground-truth map."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Integrating explicit reliability for optimal choices: effect of trustworthiness on decisions and meta-decisions",
    "presenter": "Lucie Charles",
    "poster_id": "B4",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "A key challenge in today’s fast-paced digital world is to integrate information from diverse sources with different reliability. Beyond estimating the reliability of information based on prior knowledge, it is also fundamental to understand whether people can use explicit information about the reliability of the source. In particular, a question that remains underexplored is how people use probabilistic information about the likelihood of a source to give correct information. Here, we investigated how such explicit probabilistic estimates of reliability are encoded and integrated into decision processes. To do so, we developed a novel paradigm that required participants to combine evidence from sources with different explicit levels of reliability to estimate among two responses which one was more likely to be correct. Additionally, participants had to rate after each choice the extent to which they felt they were influenced by a given source of information. Through computational modelling, we found that participants misrepresented the reliability of sources, distorting the probability a source to give correct information. As a results, they gave too much weight to unreliable source and too little weight to sources that were reliably wrong sources. However, we found that subjective report of influence correctly predicted the effective influence a source had on the decision. These findings suggest that participants were at least partially aware of what bias their choices."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Dissociable Dynamic Effects of Expectation During Statistical Learning Across Cortical Layers",
    "presenter": "Hannah H McDermott",
    "poster_id": "B6",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "The brain seemingly generates internal predictions, to optimise behaviour. Predictive processing has been repeatedly demonstrated in non-invasive studies on human volunteers and in animal models. One commonly reported phenomenon is expectation suppression (ES) or the suppression of neural activity in response to expected stimuli. Our recent EEG study yielded direct evidence that ES is expressed as two opposing mechanisms, amounting to sharpening vs. dampening of neural activity by stimulus expectation, albeit at different time points both within and across trials. In this high-field neuroimaging study, we test if these dissociable dynamics of expectation effects can be explained in the context of hierarchical learning mechanisms. Healthy volunteers (N=15) completed an associative learning task with paired visual and auditory stimuli. Univariate analyses examined region- and layer-specific activity for expected vs. unexpected sounds. General Linear Model (GLM) analyses of activation across conditions revealed differences between valid and invalid trials across the auditory cortex and occipital cortex."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "A mathematical theory of relational generalization in the face of exceptions",
    "presenter": "Luke Cheng",
    "poster_id": "B1",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Relational reasoning is a cornerstone of higher-order cognition in humans and animals, enabling zero-shot generalization to novel situations using rules like transitivity. In the real world, agents need to flexibly decide when to apply these rules and when to learn exceptions from them. It has remained unclear how standard learning systems can accomplish this. To investigate this topic, we introduce a new task paradigm: transitive inference with exceptions.  This requires subjects to infer an ordered relation and generalize using the transitive rule but also requires them to memorize a certain violation to this rule. We use a standard statistical learning system to understand the minimal inductive biases necessary to perform this task. Intriguingly, these models can generalize where possible and memorize exceptions where necessary. However, successful generalization depends on their representational geometry: an overly conjunctive representation yields a systematic pattern of errors in generalization. Ultimately, we introduce a novel task paradigm for understanding relational reasoning in the real world, explain how a standard learning system can generalize on this task, and make systematic predictions for human behavior."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "A perceptual repulsion from gravitational expectations of acceleration",
    "presenter": "Nick Simpson",
    "poster_id": "B7",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "It is often observed that we are biased to report and perceive what is more likely to be there. However, an opposite, repulsive effect has recently been reported in the visual domain, such that objects are more likely to be reported as decelerating when moving downwards. We wished to examine the nature of this effect to better understand the nuanced influences of expectation on perceptual decisions. Across three experiments, we replicate this effect, demonstrate that the bias has contributions from retinal space as well as contextual cues concerning object position, and present evidence that it has a perceptual locus. That perception can be repelled from our expectations opens a host of questions concerning the complex interactions between learning and perception."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Apathy as a Loss of Prior Precision in the Bayesian Brain",
    "presenter": "Rebecca S Williams",
    "poster_id": "B8",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Apathy is defined as a reduction in goal-directed behaviour. It is pervasive in dementia and is associated with poor prognosis. Treatments remain elusive. We propose and test a new model of apathy based on Bayesian brain principles. We propose that apathy arises from a reduction in precision of beliefs regarding action outcomes. Here, we test a potential mechanism for prior precision, in the GABAergic gain of superficial pyramidal neurons in the prefrontal cortex. \n\nFifty healthy adults (aged 50-85) undertook a goal-directed task during magnetoencephalography (MEG, Hezemans et al, 2020). Apathy was assessed using the Apathy-Motivation Index (AMI). Generative modelling was implemented to assess the involvement of three nodes in the prefrontal (PFC), premotor (PMC) and primary motor cortex (M1). See preregistration: https://tinyurl.com/wbt6rpx9.\n\nThere was strong evidence confirming the negative correlation between prior precision and apathy (B=12.2, p\u003c0.01). There was very strong evidence that lower prior precision was associated with reduced gain on the superficial pyramidal neurons in the PFC and PMC, but not M1.\n\nThese results support the proposed mechanism of apathy, in terms of cognitive process (prior precision) and neural underpinning (frontal cortical gain). This opens novel avenues for the treatment of apathy."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "A hierarchy of metacognitive capacities",
    "presenter": "Sophie Bavard",
    "poster_id": "B9",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "How do we evaluate our overall performance on a task? While most research on metacognition focuses on local confidence - our ability to assess accuracy on a trial-by-trial basis - real-world decisions often rely on global confidence, a broader judgment of overall success, measured through self-performance estimates. This study introduces a novel method to investigate self-performance estimates in memory and perceptual tasks. Participants made decisions in blocks containing two item categories and then selected the category they believed they performed better on, reflecting self-performance estimates. By analyzing the relationship between self-performance estimates and factors such as local difficulty, accuracy, response times (RT), and local confidence, our study shows that self-performance estimates rely on different cues depending on the domain: both accuracy and local confidence shaped self-performance estimates in memory, while only local confidence did in perception, without a contribution of  RT or difficulty. These findings advance our characterization of metacognitive processes and pave the way for the development of Interventions to modify metacognition."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Learning Multisensory Representations Using Predictive Coding",
    "presenter": "Parva Alavian",
    "poster_id": "B10",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Integration of information arriving in the brain from different sensory modalities is essential for robust perception. In this work, we use the predictive coding framework - a prominent theory of cortical processing- to perform multisensory representation learning. Our model can learn meaningful joint representations from two separate streams of data. These representations function as a form of hetero-associative memory, allowing the network to recall or reconstruct one modality from the other. The reconstructed outputs preserve class-relevant features, even in the absence of one sensory modality. These results suggest that predictive coding networks can serve as a biologically plausible framework for modeling multisensory representation learning."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "The relationship between pupil dilation and neural surprise in natural language comprehension",
    "presenter": "Quirin Gehmacher",
    "poster_id": "B12",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Predictive processing theories propose that language comprehension involves generating and updating context-based expectations. We tested whether such predictions are reflected not only in neural activity but also in pupil-linked responses. Using GPT-2, we derived contextual predictions and analysed MEG and pupil data recorded during audiobook listening. Replicating prior work (Heilbron et al., 2022), we find that MEG responses are modulated by both lexical surprise and semantic prediction error. Extending this, we show that pupil dilation selectively tracks semantic prediction error, suggesting sensitivity to meaning-level violations. We assess the mapping function from surprise to these pupil and MEG measures, focusing on linear vs non-linear response profiles and discuss their relation with respect to current predictive processing theories."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Paradigms for probing socio-affective Bayesian inference in individuals with blunted affect – preliminary insights",
    "presenter": "Katharina V. Wellstein",
    "poster_id": "B11",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Despite their key impact on clinical outcomes, negative symptoms in schizophrenia remain poorly understood. We designed a functional Magnetic Resonance Imaging (fMRI) study to test a mechanistic hypothesis of negative symptoms, based on the Bayesian inference and predictive coding framework. We thus designed tasks and chose models that capture this inference process, i.e. a social-affective prediction task and a control task that can be fit with Bayesian generative models. Here we present preliminary data of a first pilot study testing whether we can extract prediction error (PE) learning quantities that are uniquely social-affective. This is a crucial component of the upcoming fMRI study. Our preliminary results indicate that the two tasks in conjunction with the models may capture learning quantities that are unique to the social-affective task as well as quantities that capture general specific PE-learning. Given the preliminary nature of the study, results may change."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Are Prediction Error Attenuations Domain-specific in Autism but Domain-general in ADHD?",
    "presenter": "Irene Sophia Plank",
    "poster_id": "B14",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Research on repetition suppression suggests that neural correlates of prediction errors (PE) in autism spectrum disorder (ASD) might be domain-specific with increased attenuations for faces compared to objects. Contrastingly, research assessing mismatch negativity in attention-deficit/hyperactivity disorder (ADHD) indicates a domain-general attenuation of PE. Therefore, we captured neural correlates of PE to colours and emotions in adults with ADHD or ASD to assess domain-specificity of PE attenuations. We extracted participant-unspecific precision-weighted PE and prediction strength (PS) for emotions and colours of presented faces separately using a generative model, specifically a Hierarchical Gaussian Filter (HGF). While we found neural correlates of colour precision-weighted PE and PS as well as emotion PS in the pooled sample regardless of diagnostic group, we did not find any differences in neural correlates of emotion or colour precision-weighted PE or PS between our groups. This null finding indicates intact neural correlates of precision-weighted PE in ASD and ADHD for both colours and emotions."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Five Illusions Challenge Our Understanding of Visual Experience",
    "presenter": "Paul Linton",
    "poster_id": "B13",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Do inferential models of vision provide us with models of visual experience? I argue they do not. Five new visual illusions suggest that real-world visual experience is much simpler than we previously thought. These new illusions suggest that real-world visual experience is not an inferential process. Instead, the visual inferences modelled by inferential models of vision reflect purely cognitive inferences about our visual experience and/or the world. These five illusions provide us with new ways to tease visual experience and purely cognitive inferences apart."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Single-Neuron Evidence for Attention and Prediction Error to  Nested Auditory Regularities in the Human Auditory Cortex",
    "presenter": "Vinicius Rezende Carvalho",
    "poster_id": "B15",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "The primary auditory cortex has traditionally been viewed as a feature detector rather than a processor of complex statistical regularities. We present rare single-neuron recordings from the human transverse temporal gyrus (TTG) during a Global-Local paradigm with nested temporal regularities. Time-resolved modeling of neural responses revealed that TTG neurons are sensitive to local deviance and its interaction with global task context, with this activity strongly modulated by attention. Notably, we observed super-additive responses to combined local-global deviants and relatively late response latencies (~200ms), suggesting complex integration of predictive signals rather than simple feature detection. These findings provide direct neuronal evidence for hierarchical processing at the earliest cortical stage of auditory processing, consistent with theoretical models that emphasize context integration and prediction in perception."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Audiovisual Integration Follows Different Rules for Perceptual and Metacognitive Decisions",
    "presenter": "Porte Perrine",
    "poster_id": "B17",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Being confident in our inferences about the presence or absence of multisensory stimuli is crucial in many contexts. We investigated how humans form both amodal and modality-specific confidence judgments following the detection of audiovisual stimuli. To model this, we extended a Bayesian evidence accumulation framework. The model accurately reproduced amodal detection and modality-specific confidence judgments, despite being fitted only to amodal decisions and decision times. However, it failed to capture amodal confidence. Overall, this suggests that different integration rules apply to perceptual and metacognitive decisions."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural representations that resist interference",
    "presenter": "Xian Li",
    "poster_id": "B18",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "How can we successfully return to a complex scenario after our thinking is interrupted? We hypothesize that an “episodic background” representation is constructed over time, and that this representation can passively persist through interruptions. In fMRI, participants listened to narrative stimuli that were interrupted (or not) by silent pauses or theory-of-mind tasks. When the story was interrupted by pauses, we observed reliable and story-selective dorsal posterior medial cortex (dPMC) patterns that persisted during the pauses. These intra-interruption dPMC patterns gradually shifted over the course of the narrative and predicted subsequent story understanding and memory.  Moreover, similar dPMC patterns were detectable in participants who performed theory-of-mind tasks during interruption. These data are consistent with an episodic background process that can passively maintain evolving narrative context in the face of interference."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Predictions emerge in neural networks trained to perceive Bach's music",
    "presenter": "Alejandro Tabas",
    "poster_id": "B16",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Predictive processing proposes that the prior knowledge relevant for inference is compressed into a prediction on the immediately future states. Here we inquire whether neural networks trained to infer the current latent state in musical sequences develop a set of internal predictions on what comes next. \n\nWe used noisy tokenized Bach compositions as sensory inputs and trained RNN as models of neural circuits. We first trained the networks to infer the current latent state (token of the composition without noise) given a stream of observations (tokens of the composition with noise). After the training, we inspected whether the internal states of the network stored predictive information on the next token. To do this, we fitted a linear readout from the hidden states of the network optimized to predict the next latent state. To ensured that the predictions were stored in the network and not computed by the linear readout, we compared the predictive performance of the network with that of a linear network trained to predict the next latent state based on the current latent state.\n\nThe results confirm that neural circuits optimised to perceive the current state learn to predict future sensory input, suggesting that predictive capabilities emerge as a natural consequence of such optimization. These findings offer computational evidence for predictive processing and provide insights into how biological systems might compress their prior knowledge and use it to navigate in noisy environments."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Confidence in Sound Localization Reflects Calibrated Uncertainty Estimation",
    "presenter": "Sagarika Alavilli",
    "poster_id": "B19",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Humans localize sounds using a combination of binaural and monaural cues. However, the location of a sound remains ambiguous under many conditions. Because sound localization is often used to guide behavior, representing the uncertainty of a sound’s location is likely to be critical to decisions about where and when to act. However, little is known about whether humans represent the uncertainty associated with a sound’s location and whether any such representations are calibrated to the accuracy of localization. To study these issues, we developed a new class of stimulus-computable models to enable the representation of uncertainty. We optimized the model for sound localization in natural conditions and then compared its uncertainty estimates to those of humans."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Valuation Precision: from Behavioral Measures to Computational Modeling and Neural Underpinnings",
    "presenter": "Hui-Kuan Chung",
    "poster_id": "B21",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Noise in value representations presents decision scientists with an identification problem. We measured trial-wise valuation precision using willingness-to-pay ranges for lotteries, where the range width indicates perceived valuation imprecision (uncertainty). Using a Bayesian inference model, we isolated distinct prior and likelihood components of valuation imprecision affecting the generation of value representations. Furthermore, upregulating norepinephrine and dopamine, but not acetylcholine increased valuation precision through these distinct components."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Adaptive planning and policy mixture explain the naturalistic foraging in 3D virtual reality.",
    "presenter": "Jae Young Jeon",
    "poster_id": "B22",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Animals and humans often reject immediate rewards by using mental models to plan for future outcomes. However, proactive rejection—intentionally skipping favorable immediate options—has been understudied due to difficulties distinguishing it from forced or reactive rejections. Using a custom-designed Minecraft-based 3D foraging task paired with a sequential Bayesian inference model, we systematically identified and characterized proactive rejection behaviors. Participants strategically increased rejection of immediate rewards as spatial regularity became more apparent, resulting in enhanced overall foraging outcomes. Our computational modeling revealed that planning depth and preference for information gathering significantly predicted rejection frequency. Crucially, proactive rejection behaviors—unlike reactive rejections—were best explained by adaptive modulation of planning depth and information prioritization based on participants' confidence in spatial regularity. These findings provide mechanistic insights into proactive rejection, highlighting its potential as a behavioral marker for goal-directed planning processes."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Early Sensory Responses Track High-Level Visual Surprise",
    "presenter": "David Richter",
    "poster_id": "B23",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Predictive processing theories propose that the brain continuously generates expectations about incoming sensory information and compares these predictions to actual inputs, resulting in sensory prediction errors. However, it remains unclear which stimulus features are predicted by the brain and hence which errors drive neural responses. Here, we addressed this question by recording EEG while participants viewed object images that were expected or unexpected based on probabilistic cues. We used a deep neural network to quantify low- and high-level visual representational distances between expected and unexpected stimuli. Neural activity was then regressed onto these surprise metrics. Results showed a modulation of evoked activity over occipital electrodes approximately 200ms after stimulus onset by high-level, but not low-level, visual surprise. These findings suggest that high-level visual predictions are rapidly integrated into sensory processing."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Sophistication of Human Adaptive Probability Learning in Dynamic Environments",
    "presenter": "Florent Meyniel",
    "poster_id": "B24",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Maintaining accurate beliefs in a changing and noisy environment is a challenging computational problem. Previous studies have shown that humans adapt their learning dynamically, especially in the face of change. This conclusion is mostly supported in the context of magnitude learning (e.g., tracking a reward amount, an object position), and currently remains more uncertain in the case of probability learning (e.g., tracking the probability of an event occurring). Here, we initiate an open benchmarking approach to uncover the computations humans use for probability learning. We compared a wide range of models—including optimal Bayesian models, suboptimal variants, and simple prediction error-based update rules, using several datasets in which participants provided trial-by-trial probability estimates. Bayesian inference often outperformed simple prediction error-based models, despite being more computationally demanding and often considered less biologically plausible. Furthermore, inference strategies appear to depend on environmental volatility: under moderate volatility, an optimal Bayesian model best explains behavior, whereas in more stable environments, a simpler Bayesian approximation is better. These results so far highlight the sophistication of human adaptive learning for probability and suggest that humans can adapt their inference strategies based on environmental context. We invite others to contribute models and datasets to this benchmark to refine these conclusions."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Mechanism of learning and inference in the prefrontal cortex",
    "presenter": "Shuyi Luo",
    "poster_id": "B20",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Despite causal relationships being inherently unobservable in a direct manner, people can infer these relationships with limited data. Using a one-shot causal inference task with fMRI, we investigated the inference process at both behavioural and neural levels. Our findings reveal that participants integrated observed causal evidence with their prior beliefs about the underlying causal structures that prevailed in the world to infer unobservable causal relationships. This process engaged a midbrain region linked to dopamine and learning but also a specific and circumscribed region of  dorsolateral prefrontal cortex (dlPFC) in which activity was related to several aspects of the inference process."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Experiential Semantic Information and Brain Alignment: Are Multimodal Models Better than Language Models?",
    "presenter": "Anna Bavaresco",
    "poster_id": "B25",
    "topic_area": "Language and Communication",
    "abstract": "A common assumption in AI is that multimodal models learn language in a more human-like way than language-only models, as they can ground text in images or audio. However, empirical studies checking whether this is true are largely lacking. We address this gap by comparing word representations from contrastive multimodal models vs. language-only ones in the extent to which they capture experiential information---as defined by an existing norm-based 'experiential model'---and align with human fMRI responses. Our results indicate that, surprisingly, language-only models are superior to multimodal ones in both respects. Additionally, they learn more unique brain-relevant semantic information beyond that shared with the experiential model. Overall, our study highlights the need to develop computational models that better integrate the complementary semantic information provided by multimodal data sources."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "A ‘sweet spot’ for creative ideation: Non-linear associations between semantic distance and creativity",
    "presenter": "William Orwig",
    "poster_id": "B27",
    "topic_area": "Language and Communication",
    "abstract": "Creativity researchers have sought to standardize idea assessment via computational measures of semantic distance: the degree of conceptual dissimilarity between words. The relationship between semantic distance and creativity has traditionally been described using linear models, with the embedded assumption that as semantic distance increases, so does the creative quality of ideas. However, informal observations would suggest that distant associations may sometimes become too incoherent or nonsensical to be considered creative. Using generalized additive models (GAMs), we explored the non-linear nature of this relationship across three divergent thinking tasks: alternate uses, question asking, and metaphor generation. Our results revealed a consistent pattern: human ratings of creativity increased with semantic distance up to a certain threshold (between 0.9 – 1), after which point, additional semantic distance does not translate into more subjectively creative ideas. These findings provide a more nuanced understanding of the interplay between semantic distance and creativity, suggesting a potential “sweet spot” for semantic distance in creative ideation."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Compositional Meaning in Vision-Language Models and the Brain",
    "presenter": "Maithe van Noort",
    "poster_id": "B26",
    "topic_area": "Language and Communication",
    "abstract": "What is the role of compositional structure in the alignment of visual and linguistic brain areas to computational semantic embeddings? Vision-language models (VLMs) have shown meaningful alignment to the brain in their representations of semantic structure, for both images and text. However, the extent to which these representations capture  compositional structure -- i.e. changes in meaning based on changes to the combinatorial structure of parts -- remains uncertain. Here we leverage Winoground, a dataset designed to test compositionality in multimodal representations, to compare the compositional structure captured by different model embeddings, as well as fMRI responses collected as part of a larger study on multi-modal meaning (with 2760 image and 2760 semantically equivalent language trials). In contrast to VLM embeddings, neural representations in the brain show a striking absence of compositional processing (chance level performance) when evaluated on the Winoground benchmark -- despite robust semantic encoding of individual concepts as measured by voxel activity predictions. This is intriguing as distinctions between stimuli in Winoground are trivial to any English-speaking human, highlighting the challenge of identifying the substrates of compositional processing in the brain. Our targeted dataset and evaluation pipeline lay the foundation for systematic, cross-modal evaluations of compositionality in both artificial and biological neural representations."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Persistent and evolving encoding of phonemes reflects general auditory processing",
    "presenter": "Oli Danyi Liu",
    "poster_id": "B29",
    "topic_area": "Language and Communication",
    "abstract": "Speech recognition involves storing and integrating sequentially presented information. Recent work in cognitive neuroscience has identified temporal characteristics in humans' neural encoding of speech that may facilitate this process. A modeling study found similar properties in a self-supervised learning model trained on raw speech, suggesting these properties can arise without prior linguistic knowledge. In this work, we further explore the domain specificity of the same properties through testing representations of speech extracted from a model only trained on non-speech audio. The model replicated key aspects of the temporal characteristics, implying they might not be specific to speech perception, but rather features of general auditory processing."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Sensorimotor Integration in the Dorsal Speech Processing Stream via Directed Beta-Band Interactions",
    "presenter": "Olivia Bizimungu",
    "poster_id": "B30",
    "topic_area": "Language and Communication",
    "abstract": "Speech processing relies on dorsal stream brain circuitry linking speech sounds with their articulatory representations (Hickok \u0026 Poeppel, 2007). We investigated sensorimotor integration along the dorsal stream by measuring frequency-specific directed connectivity between auditory and frontoparietal brain regions during a categorical speech perception task. Beta-band (13-29Hz) interactions between auditory and sensorimotor cortex related to perceptual behaviour, including when participants' articulatory configuration  was mechanically perturbed. These findings advance the neurophysiology of sensorimotor integration in speech perception."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Intracranial EEG reveals multiplexed encoding of auditory, speech, and language embeddings in the human temporal lobe",
    "presenter": "Atlas Kazemian",
    "poster_id": "B31",
    "topic_area": "Language and Communication",
    "abstract": "Speech understanding in the human brain involves several representational transformations: Air pressure fluctuations become a time-frequency representation in the cochlear; auditory cortex extracts speech-relevant units; distributed networks extract amodal representations of meaning and structure. Recent advances in speech and language models have led to a series of studies using text-based large language model (LLM) representations to model these neural transformations. Here we examine the brain’s end-to-end processing of language by extending the investigation to include biophysical models of the cochlear and auditory cortex, as well as performance-optimized models of speech (Whisper) and text (GPT-2, Llama-3). We use these models to predict the activity of spatially precise neural populations recorded via intracranial EEG (iEEG) as participants listened to audiobooks. Our findings are twofold. First, we observe clear differences in encoding performance within both auditory and language model families. Second, each model type captures distinct aspects of the signal in temporal lobe electrodes, suggesting that these regions encode a mixture of intermediate auditory and higher-level semantic features. Together, these results highlight the importance of examining model–brain alignment with fine-grained temporal precision."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models",
    "presenter": "Raquel Fernández",
    "poster_id": "B32",
    "topic_area": "Language and Communication",
    "abstract": "Text representations from language models have proven remarkably predictive of human neural activity involved in language processing. However, the word representations learnt by language-only models may be limited in that they lack sensory information from other modalities. Here, we leverage recent AI advancements in multimodal modelling to investigate whether current pre-trained vision-language models (VLMs) yield concept representations that are more aligned with human brain activity than those obtained by models trained with language-only input. Our results reveal that VLM representations correlate more strongly than those by language-only models with activations in brain areas functionally related to language processing. Altogether, our study indicates that vision-language integration better captures the nature of human concepts."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Working Memory Integrates Geometrical and Temporal Languages of Thought",
    "presenter": "Elyes Tabbane",
    "poster_id": "B33",
    "topic_area": "Language and Communication",
    "abstract": "Human cognitive uniqueness is often attributed to the language faculty (Hauser, Chomsky, Fitch, 2002) that is, a set of abilities supporting speech internally (as thoughts) and externally (in communication). However, natural speech is not the only uniquely human ability, and the properties that support it might not be all there is to human cognitive singularity. In fact, our capacities for music, mathematics and other structured domains have been proposed to rely on a faculty to produce and make use of mental programs that efficiently capture the regularities in our environment to create compressed internal representations (Dehaene et al., 2022). These mental programs rely on a set of internal languages that can combine symbols based on rules and syntactically organize them to create structured expressions, this has been called the Language of Thoughts (LoTs) hypothesis (Fodor, 1975, Dehaene et al., 2022). However, it remains unclear how different LoTs interact to form shared mental representations. We investigate this gap using two behavioral experiments : (1) complexity ratings and (2) delayed sequence reproduction. We show that sequences combining geometrical and temporal regularities, and thus engaging two LoTs, are both judged simpler and remembered more efficiently than those using a single LoT."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Understanding the Neuro-Cognitive Mechanisms of Orthographic Learning in Humans and Baboons: A Comparative Study Using Domain-Specific Mechanistic and Domain-General Connectionist Models",
    "presenter": "Janos Pauli",
    "poster_id": "B34",
    "topic_area": "Language and Communication",
    "abstract": "Learning to read is essential for social participation. Here, we investigate how humans and baboons learn orthographic information. We use a neuro-cognitive mechanistic model—the Speechless Reader (SLR) and two connectionist models (CORnet-Z and ResNet-18) to investigate a human and a baboon dataset. The connectionist models employ neuronally plausible CNN architectures, while the SLR provides transparent implementations of orthographic decision behavior using pixel, letter, and letter sequence level prediction errors as representations. To align models and data, we train the models using identical trial sequences for each human and baboon. The SLR outperforms the CNNs across both species, especially on trial-wise metrics. While CNN responses diverge from individual behavioral patterns, the SLR's interpretable errors reveal that the complexity of orthographic representations increases with training. This finding suggests that domain-specific mechanistic models offer valuable insight into learned visual behavior across species."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Spatiotemporal tracking of phonetic content and probability in the human brain during continuous speech understanding",
    "presenter": "William Turner",
    "poster_id": "B35",
    "topic_area": "Language and Communication",
    "abstract": "To understand speech, the human brain integrates\nboth the content and probability of what is being\nsaid. Whether content and probability are encoded\nin the same neural populations at the meso-scale,\nhowever, remains unknown. Here, we leverage the\nexceptional spatiotemporal precision of intracranial\nelectroencephalography (iEEG), to track the neural\nencoding of phonetic features and phonetic\nsurprisal (i.e. phoneme-level probabilities), during\ncontinuous speech processing. We identify neural\npopulations that jointly encode phonetic information\nand phonetic surprisal in the superior temporal lobe.\nBy contrast, we find that lexical surprisal (i.e.\nword-level probabilities) is encoded by adjacent but\ndistinct populations. Overall, our findings have\nmechanistic implications for how content and\nprobability are neurally integrated in the temporal\nlobe, to give rise to robust speech understanding."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Dynamic Norm-Based Coding of Vocal Identity by the Primate Voice Patches Neurons",
    "presenter": "Yoan Esposito",
    "poster_id": "B28",
    "topic_area": "Language and Communication",
    "abstract": "The neural representation of conspecific vocal identity remains poorly understood in non-human primates. Drawing on norm-based coding theories of face identity, we investigated whether a similar mechanism supports voice identity encoding in the macaque brain. We recorded spiking activity from the fMRI-localized voice sensitive region of two awake macaques while presenting synthetic “coo” vocalizations morphed along a continuum from anti-voice to caricature. Dimensionality reduction techniques revealed distinct neuronal subpopulations: for example, one exhibited a very early V-shaped tuning profile, with increasing firing rates as voices deviated from the average, consistent with a norm-based coding strategy; another showed the exact inverse pattern, but with delayed responses. These temporally and functionally distinct subpopulations suggest complementary encoding strategies for representing vocal identity, potentially reflecting both deviation-from-mean and identity-certainty mechanisms. Our findings mirror encoding patterns observed in face-selective regions and provide novel evidence that norm-based coding may be a general principle of high-level social perception across sensory modalities."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "A Model of Continuous Speech Recognition Reveals the Role of Context in Human Phoneme Perception",
    "presenter": "Gasser Elbanna",
    "poster_id": "B37",
    "topic_area": "Language and Communication",
    "abstract": "Humans successfully transform acoustic signals into meaning despite great variability in the speech signal. The underlying mechanisms that enable such robust perception remain unclear, in part due to the absence of models that replicate human performance and that could be used to test mechanistic hypotheses. We built an artificial neural network model of continuous speech perception, optimized to recognize sequences of sub-lexical units from cochlear representations of acoustic signals. We then developed non-word recognition benchmarks to evaluate human and model speech perception. The model closely matched human performance and replicated human-like patterns of phoneme recognizability and confusions. However, human-model similarity was dependent on recurrent processing, suggesting that human recognition depends critically on bidirectional integration of information in the speech signal. The model and benchmark set the stage for future investigations into the neural and perceptual mechanisms underlying speech."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural Mechanisms of Linguistic Working Memory: Phrase Composition, Storage and Retrieval",
    "presenter": "Théo Desbordes",
    "poster_id": "B36",
    "topic_area": "Language and Communication",
    "abstract": "Understanding how the brain stores and manipulates linguistic information in working memory is central to understanding human cognition. Can we characterize the format of linguistic information storage in working memory? In this magnetoencephalography (MEG) study, participants read one-word, two-word, and five-word noun phrases followed by a matching task with a visual image. We found that individual word representations were maintained in neural activity for variable durations, depending on upcoming compositional demands. Critically, during a delay period following phrase reading, we observed a transition from word-specific to more abstract neural codes, with activity scaling alongside semantic complexity—suggesting compression of linguistic information. Retrieval dynamics revealed that access to surface-level properties was faster than to deeper semantic features, consistent with a decompression step. Finally, in ongoing work we explore potential contributions of reactivations —including coactivations and sequential replays— and oscillatory mechanisms such as phase-amplitude coupling, to the memory process. Together, these results map out the trajectory of linguistic processes, from online composition, through working memory storage, to retrieval. These findings place strong computational and biological constraints on models of linguistic working memory and could inform the design of new memory architectures in artificial conversational systems."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Toward Affective Empathy in AI: Encoding Internal Representations of  \"Artificial Pain\"",
    "presenter": "Angeline Wang",
    "poster_id": "B38",
    "topic_area": "Language and Communication",
    "abstract": "Current chatbots excel at demonstrating cognitive empathy through language analysis but they lack mechanisms to internalize emotional intensity, a hallmark of human affective empathy mediated by neural substrates like the anterior cingulate cortex (ACC). We propose a framework inspired by ACC-mediated \"Artificial Pain\" encoding, integrating emotion classification with intensity regression. Using the Emotional Support Conversations (ESConv) dataset, we carry out transfer learning using MentalBERT, MentalRoBERTa, and ModernBERT in a multi-task setup that jointly models emotion categories and corresponding intensity levels on a 1–5 scale. We then evaluate these models to assess their capacity for emotion understanding and graded affective representation.MentalRoBERTa achieves state-of-the-art performance in single-task classification (F1=0.59) and multi-task settings (F1=0.63), with intensity regression showing significant correlations to the human-annotated ground-truth, but with relatively high estimation error. While multi-task learning improves emotion classification through shared intensity signals, predicting the intensity of emotions remains challenging, highlighting the need for model training with larger datasets. This work establishes a benchmark for emotion intensity-aware affective AI, bridging natural language processing methods with neuroscientific principles. Future implications include the advancement of affective empathy in human-agent interactions."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural Dynamics and Representational Content across Abstract and Concrete Concepts",
    "presenter": "Mancano Martina",
    "poster_id": "B39",
    "topic_area": "Language and Communication",
    "abstract": "Concrete concepts are represented in the brain according to linguistic, experiential and taxonomic organizational principles, but it is not clear how much these principles contribute to the representation of the abstract domain. Moreover, compared to their spatial location in the brain, little is known about the temporal unfolding of these processes. In this study, we use Magnetoencephalography (MEG) and semantic models to investigate the localization and temporal unfolding of concrete and abstract concepts. Concrete and abstract words were presented visually to the participants during MEG recording. Data were analyzed through Representational Similarity Analysis (RSA), separately for concrete and abstract words. The semantic models were based on linguistic, experiential, and taxonomic information. We collected data from 6 participants, and data collection is ongoing. We expect significant correlations between the MEG signal and the distributional model for abstract and concrete concepts. This correlation is expected to temporally precede correlations with the experiential models. Concrete concepts signal is expected to correlate with the sensorimotor experiential model, while abstract concepts signal with the emotional one. Taxonomic models are expected to correlate with concrete but not with abstract concepts signal."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Detecting Mild Cognitive Impairment Across Languages: An Analysis of Speech Features in Chinese and English",
    "presenter": "Lee Hung Wei",
    "poster_id": "B40",
    "topic_area": "Language and Communication",
    "abstract": "Speech analysis offers significant potential for the early, cross-linguistic detection of Mild Cognitive Impairment (MCI), but the crucial features for this remain unclear. Our study investigated a classification model for MCI detection in both English and Chinese, using three interpretable acoustic feature sets: time-domain (TD), eGeMAPS (EGE), and short-time Fourier transform (STFT). We found that integrating multi-domain features yielded the best performance in combined language conditions. Specifically, robust cross-linguistic acoustic markers were linked to energy variation, voicing regularity, fine-grained temporal and spectral dynamics, and amplitude envelope features, as identified by group-based SHAP analysis."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "From sequences to schemas: How recurrent neural networks learn temporal abstractions",
    "presenter": "Vezha Boboeva",
    "poster_id": "B42",
    "topic_area": "Language and Communication",
    "abstract": "The world, despite its complexity, harbors patterns and regularities crucial for animals, with numerous real-life processes evolving over time into structured sequences of events. Brains have evolved to learn and exploit these sequential regularities, by forming knowledge at different degrees of abstraction: from simple transition and timing, to chunking, ordinal knowledge, algebraic patterns, and finally nested tree structures. How regularities expressed in algebraic patterns or abstract schemas (e.g., AAB or ABA) are encoded in the brain is still an open question. Here, we study whether and how neural circuits acquire, organize and use such an abstract code. We first build a computational framework to generate sequences with abstract temporal patterns. Next, we propose Recurrent Neural Networks (RNN) models performing different tasks requiring learning and predicting such sequences, and study the conditions under which learning is possible. We study the internal representations formed by the network models, and the extent to which these representations might be abstract, allowing to generalize to novel sequences and tasks."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural Substrates of a Symbolic Action Grammar in Primate Frontal Cortex",
    "presenter": "Lucas Y. Tian",
    "poster_id": "B43",
    "topic_area": "Language and Communication",
    "abstract": "At the core of intelligence is the capacity to solve new problems. In turn, problem-solving has been hypothesized to depend on cognitive operations resembling symbolic grammars (Newell \u0026 Simon, 1976), with two core components: discrete units (symbols) and rules for recombining symbols into new composite representations (syntax). Whether and how symbolic grammars are implemented in neuronal substrates remains unknown. Here, we establish a research program to elucidate the neural basis of action grammars. In a drawing task, macaque monkeys learn action grammars, which guides how they compositionally generalize to draw new images. Our behavioral analyses indicate that these grammars consist of symbolic action primitives and syntactic rules. In recordings of neuronal activity across motor, premotor, and prefrontal areas, we identified separate populations encoding action grammar components, including motor primitives, action symbols, and syntactic rules. Here, we report the discovery of an action symbol representation in ventral premotor cortex (PMv). Specifically, we found that PMv encodes planned stroke primitives, and does so in a manner exhibiting three symbolic properties: abstraction, categorical structure, and recombination. Thus, we have established a paradigm to study compositional generalization using action grammars, and identified a representation of action symbols in PMv. In ongoing work, we are studying how neural activity, in PMv and interconnected areas, may implement the systematic composition of symbols using syntactic rules."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural Encoding of Continuous Word Meaning Likelihood During Semantic Disambiguation",
    "presenter": "Andrey Zyryanov",
    "poster_id": "B44",
    "topic_area": "Language and Communication",
    "abstract": "Language comprehension hinges upon our ability to resolve semantic ambiguities, yet the neural representations underlying disambiguation remain unclear. To fill this gap, we recorded MEG while participants listened to German sentences containing an ambiguous target word, Blatt (meaning paper or leaf) or Tor (meaning gate or goal). In a behavioral pre-study, participants read these sentences and rated which target meaning was most likely. While group-averaged ratings showed that meaning likelihood varied continuously across sentences, single-trial ratings were categorically biased towards either meaning. To test whether the neural representation of meaning likelihood is categorical or continuous, we decoded the target word from neural activity and examined the effect of meaning likelihood on cross-meaning generalization. Around 800 ms before target onset, cross-meaning generalization was most accurate for neutral sentence contexts where target meanings were equally likely. Crucially, this improvement in generalization accuracy was parsimoniously modelled by a linear function of meaning likelihood. Thus, although explicit semantic judgments are distributed categorically, neural processing of ambiguous words reflects continuous meaning likelihood."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Speaker-Specific Semantic Priors Enhance Both Expected and Unexpected Speech Across Processing Levels",
    "presenter": "Fabian Schneider",
    "poster_id": "B45",
    "topic_area": "Language and Communication",
    "abstract": "Predictive processing is fundamental to speech perception, yet how priors shape neural representations at different hierarchical levels remains debated. Here, we investigate how humans combine expectations about what another person is going to talk about, i.e., speaker-specific semantic priors, with ambiguous sensory inputs. Using a combination of stimulus reconstruction models, representational similarity analysis, and single-trial encoding models of EEG responses, we show two complementary processes of speaker-specific semantic priors: sharpening of low-level acoustic representations, pulling them towards the expected acoustic signal and that prediction errors only at higher levels of the neural hierarchy, signaling semantic surprisal. Critically, speaker-specific priors were not applied when incoming words clearly deviated, indicating flexibility as a function of their relative likelihood. Together, these findings provide evidence for a unified theory of predictive processing in the brain in which priors enhance both expected and unexpected information at different levels of the processing hierarchy."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Decomposition of uncertainty into dispersion and strength during speech processing",
    "presenter": "Pierre Guilleminot",
    "poster_id": "B46",
    "topic_area": "Language and Communication",
    "abstract": "Speech comprehension relies on contextual predictions to minimize error between expected and incoming auditory input. This mechanism hinges on linguistic uncertainty, typically quantified using Shannon entropy, which captures the overall unpredictability of a given word based on prior context. However, Shannon entropy is only one member of the broader Rényi entropy family, which enables a distinction between uncertainty due to the strength of dominant predictions and that due to the dispersion across alternatives. Here, we investigated which entropy-based measure of uncertainty best reflects neural processing during speech listening. Using intracortical recordings from subjects listening to an audiobook, we computed the mutual information between neural responses and different uncertainty measures derived from a large language model. Our results reveal that, rather than Shannon entropy, the brain separately processes the strength and dispersion of predictions in distinct neural populations. This suggests a multidimensional representation of uncertainty. More generally, these findings highlight the need for a more refined definition of uncertainty in cognitive neuroscience."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Concerns in evaluating hierarchical correlations between human brains and end-to-end automatic speech recognition models",
    "presenter": "Yi Wang",
    "poster_id": "B41",
    "topic_area": "Language and Communication",
    "abstract": "Recent neural encoding studies have attempted to compare the human brain speech perception network with artificial neural network models trained end-to-end (e2e) on automatic speech recognition (ASR), aiming to reveal the temporal dynamics of human speech processing. Multiple studies have reported a prominent correspondence between e2e ASR models and human brains in terms of the hierarchical encoding of linguistic features, from low-level acoustic features to high-level semantic features. While different types of e2e ASR models have been used to investigate this correspondence, there has not been a consensus on the most suitable ASR model type for such investigations.\nThis extended abstract will discuss concerns regarding the use of three mainstream types of e2e ASR models when evaluating their hierarchical correlation with human speech perception network, including the recurrent neural network transducer, the attention-based encoder-decoder model using tokenizer (i.e. Whisper) and the self-supervised transformer model. We suggest that further caution is required when using these models in the hierarchical correlation studies, due to issues such as varying decoding latency, mismatched context window and difficulty in representation disentanglement inherent in each model type, respectively."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Disentangling belief and strategy in natural visual search",
    "presenter": "Hyunwoo Gu",
    "poster_id": "B48",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Our beliefs and strategies are not always aligned. For example, in visual search, an aligned strategy would be to choose to look where one believes the target to be (belief maximization). However, previous studies using simple stimuli have found that human search performance is comparable to an ideal observer which maximizes information (Najemnik \u0026 Geisler, 2009), a strategy which can sometimes select gaze to locations where little is known instead of where the target is believed to be. In naturalistic settings, however, visual search poses additional challenges; for example, targets can take on many possible appearances, and object affordances can suggest actions such as reaching, which may influence gaze strategies. While widely used predictive models of visual saliency (Itti \u0026 Koch, 2001; Droste, Jiao, \u0026 Noble, 2020; Kümmerer, Bethge, \u0026 Wallis, 2022; Ding et al., 2022; Hosseini, Kazerouni, Akhavan, Brudno, \u0026 Taati, 2024; Yang et al., 2024) have achieved impressive accuracy in predicting human fixations from image features, they do not model belief or strategy. Moreover, these models are not normative as they do not specify the ideal criteria that can be compared to human performance, prohibiting a principled way to assess optimal belief propagation and visual search strategy. To address these challenges and characterize the gaze selection strategy, we generalized an ideal observer model (Najemnik \u0026 Geisler, 2005) to natural images with an explicit modular structure of belief and strategy. Across a publicly available dataset (COCO-search18) and a dataset we collected, we found that estimated strategies do not align with the beliefs, deviating from an intuitive, maximum-seeking strategy. Furthermore, we explicitly tested whether people’s choice of eye movements matches their beliefs using a novel gaze-contingent paradigm, and we found that where people shift their gaze to and where they believe the target to be can differ substantially. Taken together, these results suggest that people tend to prioritize information-seeking over belief maximization in naturalistic visual search."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Understanding Human Limits in Pattern Recognition: A Computational Model of Sequential Reasoning in Rock, Paper, Scissors",
    "presenter": "Daniel Yamins",
    "poster_id": "B47",
    "topic_area": "Language and Communication",
    "abstract": "How do we predict others from patterns in their behavior and what are the computational constraints that limit this ability? We investigate these questions by modeling human behavior over repeated games of rock, paper, scissors from Brockbank \u0026 Vul (2024). Against algorithmic opponents that varied in strategic sophistication, people readily exploit simple transition patterns (e.g., consistently playing rock after paper) but struggle to detect more complex sequential dependencies. To understand the cognitive mechanisms underlying these abilities and their limitations, we deploy Hypothetical Minds (HM), a large language model-based agent that generates and tests hypotheses about opponent strategies, as a cognitive model of this behavior (Cross et al., 2024). We show that when applied to the same experimental conditions, HM closely mirrors human performance patterns, succeeding and failing in similar ways. To better understand the source of HM’s failures and whether people might face similar cognitive bottlenecks in this context, we performed a series of ablations and augmentations targeting different components of the system. When provided with natural language descriptions of the opponents’ strategies, HM successfully exploited 6/7 bot opponents with win rates \u003e80% suggesting that accurate hypothesis generation is the primary cognitive bottleneck in this task. Further, by systematically manipulating the model’s hypotheses through pedagogically-inspired interventions, we find that the model substantially updates its causal understanding of opponent behavior, revealing how model-based analyses can produce testable hypotheses about human cognition."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Humans and variational autoencoders agree: attractive faces are average and feminine, not symmetric and young",
    "presenter": "Francisco M. Lopez",
    "poster_id": "B49",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Why do we find some faces more attractive than others? Four properties are frequently identified as sources of beauty: symmetry, youth, femininity, and averageness. Recent experiments show that only the latter two determine facial attractiveness. Here, we test whether a neural network trained with unsupervised learning can reproduce and explain this phenomenon. We train a variational autoencoder (VAE) on face images and estimate its preference judgments as the compression of a face in the latent space. We find that, like humans, the VAE’s preferences are significantly correlated with the averageness and the femininity of the faces but not their symmetry or youth. Furthermore, the VAE correlates with human attractiveness judgments. In sum, this work suggests that human aesthetic face preferences can be explained by the efficiency with which a face can be encoded."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Selective Is Selective, or Not? Investigating Consistency and Task Relevance of Selectivity Metrics in DNNs",
    "presenter": "Anastasia Lado",
    "poster_id": "B50",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Neural response selectivity is a long-standing phenomenon in cognitive neuroscience, with distinct cortical areas selectively responding to visual categories across processing stages. Such selectivity is typically quantified using measures ranging from simple response ratios to detailed statistical comparisons between preferred and non-preferred categories. But how consistent and stable are these measures? And, critically, does selectivity capture behavioral relevance? Recent computational studies have shown that both trained and untrained deep neural networks (DNNs) exhibit category-selective units. Here, using face selectivity as our test case, we leverage DNNs to systematically compare a broad range of selectivity metrics while assessing their relevance to task performance. Our results reveal low agreement between selectivity metrics and lesioning-based rankings, and the consistency among metrics varies with spatial scale, processing stage, and training. However, all metrics yield similar face decoding accuracy. These findings caution against overreliance on any single metric and inform the interpretation of selectivity in computational and neural data."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Attention modulates within-category differentiation of natural auditory objects in human auditory cortex",
    "presenter": "Patrik Wikman",
    "poster_id": "B51",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Categorical representations of auditory objects arise in non-primary auditory cortex. However, it is unclear whether these regions support subcategorical diff-erentiation (e.g., instrument types), and whether attention enhances this differentiation. fMRI data (n = 20) were acquired in two experiments: OA, where participants listened to sound objects (speech, instruments, animals); and 3OA, where they attended to designated objects in scenes containing one object per category. SVM decoders were trained on OA-data to distinguish subcategory objects (e.g., dog vs. bird). Decoders were tested: (1) on OA-data to identify regions with stimulus-dependent within-category differentiation; (2) on 3OA scenes to assess if attention boosts within-category differentiation (comparing attended vs. distractor object differentiation). Stimulus-dependent and attention-related speaker identity differentiation involved spectrally non-sensitive STG regions, whereas animal and instrument differentiation was confined to spectrally sensitive regions. Results suggest speaker identity differentiation involves higher-level object representations, while other naturalistic sounds are differentiated via lower-level acoustic features."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Explaining neural mechanisms of age-related dedifferentation in the ventral stream through deep neural networks",
    "presenter": "Akilles Rechardt",
    "poster_id": "B52",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Healthy older adults show less distinct visual category representations compared to young adults in later parts of the ventral visual stream (VVS), a phenomenon known as dedifferentiation. However, the neural mechanisms causing this are unclear. We used a deep convolutional neural network to model the VVS and applied noise and synaptic damage to different layers of the model while reading out category distinctiveness from a late, category-selective layer that models inferior temporal cortex (IT). We expected greater damage to IT to cause stronger dedifferentiation. As predicted, greater damage led to greater dedifferentiation. However, damage to earlier layers of the model (e.g., V1) caused greater dedifferentiation in IT compared to damaging later layers. This suggests that age-related dedifferentiation in IT could result from damage to upstream areas of the network. Our findings also match structural brain imaging work indicating early to late VVS white matter tract integrity is related to the distinctness of category representations. In sum, our modelling approach for the first time provides a mechanistic explanation for age-related dedifferentiation."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Attention alters numerosity tuning in the human brain",
    "presenter": "Liangyou Zhang",
    "poster_id": "B54",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Many animals, including humans, can rapidly and accurately perceive numerosity, the number of objects. Although typically considered a cognitive function, recent studies proposed that numerosity perception instead relies on sensory processes. The brain contains numerosity-tuned neural populations that respond most strongly when passively viewing a preferred numerosity (e.g., six). Real-world vision, however, is target-oriented, and attention may alter numerosity processing. Just like spatial attention attracts neural responses to attended locations, we asked whether attention could alter numerosity preferences toward task-relevant (i.e., attended) numerosities. Using 7T fMRI and population receptive field (pRF) modeling, participants reported the occurrence of displays containing either numerosity two (low), numerosity six (high), or white stimuli (non-numerical). Neural preferences altered accordingly: lowest when attending to two, highest to six, and intermediate for color. These results show that numerosity tuning of neural populations not only depends on stimulus properties but also on the behavioral goals of the observer."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Attention Modulates the Geometry of Auditory Representations",
    "presenter": "Caterina A. Pedersini",
    "poster_id": "B55",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Attention plays a crucial role in shaping auditory representations, yet its impact on brain space geometry remains unclear. In this study, we investigated how attention modulates auditory object representations using fMRI (n=20). We conducted two experiments: (1) Exp OA, where participants listened to a single auditory object from one of three categories (speech, animal or instrument sounds) (2) Exp 3OA, where an auditory scene comprising three overlapping objects, each from a different category, was presented, with attention directed to one of the objects. We applied principal component analysis (PCA) to reduce data dimensionality and Procrustes analysis to align brain representations across both participants and experiments. Our results demonstrate that in 3OA auditory scenes, attention reorganizes scene geometry by shifting the entire representation toward the representational location of the attended object (as estimated from OA data). These effects were observed across nonprimary auditory regions for all object types and in a broader language network for speech. Our approach shows that key attentional effects emerge when neural data are analyzed in multidimensional brain spaces."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "NeuroAdapter: Visual Reconstruction with Masked Brain Representation",
    "presenter": "Hossein Adeli",
    "poster_id": "B53",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Recent advances in generative decoding models have shown that complex visual scenes can be reconstructed from brain activity. However, current models rely on an intermediate step that maps the brain data to rich image and text feature spaces, resulting in overly large and computationally intensive models. This intermediate process may also cause loss of information deriving from the selectivity and receptive field location of individual brain units. In this work, we explore the capabilities of visual decoding in the absence of intermediate representations. We propose NeuroAdaptor, a simple modular framework that directly encodes the neural data from different brain regions to condition the diffusion process. To avoid overfitting, our model incorporates a random token-masking strategy. We train our model on the 7T-fMRI Natural Scenes Dataset (NSD) and evaluate it on multiple metrics. NeuroAdapter excels at capturing high-level semantic visual content from fMRI signals, outperforming more complex models. Our model demonstrates a promising direction for scaling decoding models up to whole-brain image reconstruction."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "pRF Amplitude Differs within the Ventral Visual Processing Stream in Autism",
    "presenter": "Charlotte A. Leferink",
    "poster_id": "B56",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Retinotopic activation patterns in visual regions have been shown to differ between autistic and neurotypical individuals (Schwarzkopf et al., 2014), and may contribute to differences in perception and memory that characterize the condition (Robertson and Baron-Cohen, 2017). Recent work has shown that the posterior-anterior transition from perceptual to memory-related areas is marked by an increase in negative amplitude retinotopic responses, which are thought to play a role in to mnemonic processing (Steel et al., 2021; Steel*, Silson*, et al., 2024) and have not been investigated in individuals with autism. Here, we explored the distribution of retinotopic voxels and their amplitudes from perceptual to mnemonic areas along the ventral visual stream using fMRI and voxel-level modeling. We found that the control group had more retinotopic voxels within parahippocampal place area (PPA) and the ventral place memory area (vPMA) than the autism group. Further, while both groups showed a greater proportion of negative amplitude voxels within mnemonic areas relative to PPA, autists had a greater average percentage of negative retinotopic voxels in PPA and in vPMA than the controls. These findings provide evidence for neural processing differences in autistic individuals that extend beyond early visual cortex into memory processing regions."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Representational dynamics of visual contents underlying object selectivity",
    "presenter": "Chun-Hui Li",
    "poster_id": "B57",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Attention allows humans to prioritize relevant information in line with behavioral goals, yet the temporal dynamics of its influence on object selectivity remain unclear. In this study, we integrated magnetoencephalography (MEG), functional magnetic resonance imaging (fMRI), and deep neural networks (DNNs) to investigate how goal-directed attention selectively modulates visual object representations. Representational similarity analysis (RSA) revealed enhanced category-specific similarity in brain regions under attention, with top-down modulation from prefrontal cortex to category-specific areas through Granger causality analysis. GLM-based RSA confirmed dynamic object-selective representations in the fusiform face area (FFA) and parahippocampal place area (PPA). Furthermore, MEG-DNN fusion showed attentional effects across hierarchical DNN layers, reflecting a coarse-to-fine processing pattern. These findings offer new insights into the spatiotemporal dynamics of object-based attentional modulation in the human brain."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Conceptual priorities shape individual gaze patterns during naturalistic visual attention",
    "presenter": "Caroline E. Robertson",
    "poster_id": "B59",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "In daily life, we readily recognize people, places, and objects (e.g., “soldier,” “stadium,” “flag”), as well as the conceptual links between them (e.g., “patriotism”). Here, we show that conceptual-level information shapes individuals’ gaze patterns in a naturalistic eye tracking paradigm. Participants (N = 61) explored a large set of real-world photospheres (N = 100) in headmounted VR while their gaze was continuously monitored using in-headset eye tracking. To assess the informational priorities guiding individual differences in gaze, we leveraged the embedding spaces of large vision and language models. We found that individually-specific gaze patterns across diverse real-world photospheres can be captured by a large language model (LLM) that encodes abstract relationships beyond the visual image content. We demonstrate that the embedding spaces of language and vision models explain unique variance in gaze behavior, and that LLM-based models capture individually specific attentional priorities. These results highlight a new dimension of human selective attention: namely, the influence of individuals’ unique conceptual-level information seeking priorities."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Interpretable prediction of human fixations from behavior-derived representational dimensions",
    "presenter": "Luca Kämmer",
    "poster_id": "B58",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Eye movements are core to how humans perceive their environment, which is why understanding which properties guide fixations can help us better understand visual perception. Although there are many computational models trying to predict eye movements, they mostly aim to maximize performance without offering insights into why certain image regions draw our gaze. We addressed this question by leveraging 49 representational object dimensions that capture visual and semantic object information to predict human fixations on images. We weighted these dimensions with their relevance for an image to generate behaviorally-relevant feature maps, without specifically training on fixation data. Our approach outperformed a permutation-controlled baseline and matched the performance of a saliency model. Crucially, our predictions are interpretable, offering insights into which representational dimensions drive them. Lastly, we showed how predictive individual dimensions are of fixation in general which helps us better understand which features drive gaze allocation."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "From Pixels to Human Typicality Judgments: Disentangling Category Structure and Neural Network Representations",
    "presenter": "Itay Inbar",
    "poster_id": "B60",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Humans can consistently rate the typicality of objects with respect to basic-level categories, but what do these ratings reveal about the computational mechanisms underlying categorization? We evaluated human typicality judgments against predictions from image-computable models. Each model paired a vision transformer (ViT), trained on one of five tasks, with one of three category structure models---prototype, exemplar, or a linear decision-bound model. This yielded 15 models systematically varying in representational and category structure assumptions. We found that predictions from a prototype model using the representations of a ViT trained on image classification aligned most closely with human judgments. However, this model’s advantage over the alternatives was not consistently significant, and its performance remained well below the leave-one-subject-out noise ceiling. Simulations showed that although some models were statistically indistinguishable in prediction accuracy, all 15 made distinct predictions. We discuss experimental design considerations that may enable stronger comparisons among these alternative models."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "How attention shapes simplified mental representations for planning",
    "presenter": "Jason da Silva Castanheira",
    "poster_id": "B62",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Naturalistic behaviour involves complex\nproblems with multi-step actions, making\nsearching for a solution challenging. Despite this\nhurdle, human planning is efficient—it frugally\ndeploys limited cognitive resources—and\nflexible—adapting to novel problems. Recent\nwork suggests that humans reason about difficult\ndecisions by constructing simplified\nrepresentations of their environment. However,\nhow these simplified representations are\nconstructed remains unknown. Here, we\ncharacterize how visual attention controls which\naspects of a scene enter a task representation for\nuse in planning. When task-relevant information\nis spatially confined to a visual hemifield, people\ncan more easily construct simplified and useful\ntask representations. Inspired by the ‘spotlight of\nattention’ analogy, we incorporate the effects of\nvisuospatial attention into a novel computational\nmodel of constructing task representations for\nplanning. Together, our work bridges\ncomputational models of decision-making and\nperception to better understand how individuals\nrepresent their environments in aid of planning."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Knowledge differences affect gaze behavior during naturalistic object exploration",
    "presenter": "Amanda J. Haskins",
    "poster_id": "B61",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "How does knowledge affect visual exploration? Here, we investigated whether learning about an object would change participants’ gaze behavior. Adults (N = 22) wore a head-mounted eye tracker while exploring real-world objects that were relatively unfamiliar (e.g., a rolodex). We assessed learning-related differences in both (1) oculomotor behavior (e.g., fixation durations) and (2) participants' fixated visual content by leveraging embeddings from a large vision-language model. We find evidence that before learning, gaze is more exploratory (i.e., shorter, more frequent fixations). Moreover, we find that differences in fixated content across participants are increased after learning about an object, suggesting that knowledge states may contribute to differences in gaze behavior. These results underscore the importance for future work of quantifying individual knowledge states to further leverage naturalistic eye tracking as a window into learning and cognitive development."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Nothing Intrinsically Memorable: The Relational Nature of Animacy Effects in Visual Memory",
    "presenter": "Dyllan Simpson",
    "poster_id": "B63",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Are some images intrinsically more memorable than others? Previous research has demonstrated that animate objects are remembered better, on average, than inanimate objects, leading to claims that animate features are intrinsically more memorable. We challenge this view by showing that the same animate objects are either highly memorable or highly forgettable depending solely on their representational distinctiveness to other objects in the study set. By manipulating the proportion of animate objects (10% vs. 90%, N=1600 objects) in our dataset, we created a reversal in how distinctive animate items were, on average, from all items according to neural data from IT cortex in macaques, and in a recognition memory experiment (N=93), this led to a complete reversal of traditional memory advantages for animate items. Overall, these findings demonstrate that animate stimuli do not have intrinsically higher memorability — instead, their higher memorability emerges from their relation to other items in memory."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Exploring the Neural Representation of Elementary Math Concepts",
    "presenter": "Samuel Debray",
    "poster_id": "B64",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Mathematical cognition engages a distributed network of brain regions, but the fine-grained organization of this network remains unclear. Using high-resolution 7T fMRI, we investigate how elementary math concepts from two domains - arithmetics (integers and fractions) and geometry (shapes) - are represented in the brain. We test whether these concepts are neurally organized not only by category but also according to shared numerical magnitude.\n\t\nBehavioral similarity judgments reveal consistent cross-category associations between concepts with equivalent magnitudes (e.g. \"three\" and \"triangle\"). In the brain, we observe distinct activation patterns for arithmetic versus geometric items, and further differentiation between integers and fractions - refining prior accounts of the math network’s structure. Although direct magnitude-based correspondence was not detected in neural similarity patterns, semantic distances derived from GloVe embeddings significantly predict both behavioral judgments and neural representations in parietal and temporal regions. These findings offer new insights into how mathematical concepts are structured and encoded in the human brain."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Covert attention is flexibly allocated in value-guided choice",
    "presenter": "Amy X. Li",
    "poster_id": "B66",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Computational models of human decision making suggest that visual attention influences evidence accumulation from choice options, with non-fixated peripheral options being downweighted relative to the fixated option via a constant parameter. Such models negate the possibility that levels of covert attention might change across the course of a decision, shaped by information gathering and options encountered. We explicitly test how attention is allocated beyond gaze (i.e., covert attention) during value-guided choice using a probe-detection task. Participants (N = 31) completed a choice task, with simultaneous eyetracking recording, where they chose between differently valued patches to earn rewards. During each trial, probe letters were flashed briefly onscreen at each patch location, and probe report accuracy at each location was used as a measure of attentional deployment. Results showed that covert attention at peripheral locations was modulated by decision-relevant variables – such as their decision-relevance and relative value – and has downstream consequences for fixation-related choice biases. These findings indicate that attention to peripheral options is flexible rather than fixed."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Cross-Context Value Dynamics: The Impact of Contextually Irrelevant Values on Choice Behavior",
    "presenter": "Neele K. Elbersgerd",
    "poster_id": "B67",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "In everyday decisions, the same choice option can yield different rewards depending on the context. Two coats will, for instance, have different utility depending on whether the goal is to be shielded from rain or to dress for a dinner. We reanalyzed behavioral data from a previous study, where participants switched between contexts with different object features predicting rewards: color in one context and motion in another (Moneta et al., 2023). Participants were explicitly cued in each trial which features to focus on, and outcome depended only on the cued context. Our analysis focused on how competing contexts influence choice and learning trial by trial. We identified two potential learning signals from irrelevant features, carrying information about the value expectation for (1) the chosen object’s irrelevant feature (Object Prediction Error) and for (2) the best feature in the irrelevant context (Context Prediction Error). Combining reinforcement learning and general linear models, we found evidence for both learning signals influencing participants’ behavior, i.e. that value updates were also guided by the difference between context-irrelevant values and outcomes. The strongest support was found for Object Prediction Error updates. Through examining how outcome-irrelevant information influences subjective value assignment, we deepen our understanding of how competing goals are processed and shape future choice behavior."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Discovering visual categorical selectivity across the whole brain in silico using transformer-based encoders and large-scale generative models",
    "presenter": "Ethan Hwang",
    "poster_id": "B65",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "The human visual cortex contains several regions that selectively respond to particular categories (e.g. faces, places, bodies). However, it is unclear whether there are regions responsive to additional (possibly more complex) categories, either inside or outside the visual cortex. Jointly discovering the categories and the corresponding selective regions, without relying on the researchers' biased imagination, remains a methodological challenge. Here, we take an in-silico approach to discovering category-selective regions. We trained a state-of-the-art transformer-based encoding model that predicts neural responses from natural scenes. We then used this model to generate hypotheses about category-selectivity of different regions throughout the human brain by performing in-silico mapping, using large amounts of computation. We use diffusion-based generative models and retrieval from large image datasets to find images that maximally activate different parcels. We found many parcels with complex selectivity, transcending simple categorical concepts: scenes with multiple objects (sport events), specific subcategories (places with vanishing points or parallel lines), and specific interactions (tool use). Our study demonstrates a data-driven paradigm for discovery of visual selectivity for each region with sets of optimal images. The category-selectivity hypotheses generated can be tested in future fMRI experiments."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Value GradientRescales Grid-like Representations during Reward Learning",
    "presenter": "Hai-Tao Wu",
    "poster_id": "B68",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Cognitive maps are hypothesized to organize abstract features spatially, akin to physical environments, to guide decision and generalization. We propose that to better facilitate reward learning, these maps may distort by how fast value changes (i.e., value gradients), over-representing dimensions where feature changes yield larger reward differences. Using fMRI, we tested this during a reinforcement learning task with jellyfish images varying in two features (spot number, tentacle number) as options. Participants learned value maps where one feature had twice the reward sensitivity of the other. Under the assumption of internal maps scaled by value gradients, we observed six-fold periodic BOLD signals—a signature of grid-like coding—in entorhinal cortex (EC) and medial prefrontal cortex (mPFC). Compared with a range of alternative scales, signal strength peaked around value gradient scale and correlated with choice accuracy. These results point to a possibility that cognitive maps may be optimally constructed to reflect the reward structure in service of goal-directed behavior."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Towards generative AI-based fMRI paradigms: reinforcement learning via real-time brain feedback",
    "presenter": "Giuseppe Gallitto",
    "poster_id": "B69",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Traditional fMRI’s reliance on fixed task paradigms perpetuates the reverse inference problem, limiting specificity in brain-behavior mapping. We present Reinforcement Learning via Brain Feedback (RLBF), an approach reversing the direction of inference by using real-time fMRI and reinforcement learning to dynamically adjust stimuli – optimizing the ‘stimulus space’. In a visual cortex proof-of-concept study (N=10), the algorithm successfully optimized checkerboard parameters within 35 trials, improving brain prediction from chance-level to a mean absolute percentage error (MAPE) of 12.7% (SD:6.3%; inter-trial improvement of 0.6%) and achieving stable convergence, despite fMRI noise. Stimulation optimization revealed a preference for maximum contrast stimuli at 18Hz (+/-10Hz across participants) – aligning with known visual processing properties – and demonstrated the potential for brain activity to effectively tune AI models, offering new avenues for personalized experimental design and rigorous testing of reverse inference claims."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Social Scene Perception in neurotypicals, bvFTD and AI models",
    "presenter": "Laura Iris Van Hove",
    "poster_id": "B70",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Social-cognitive skills are required to successfully navigate the social world. Behavioural variant frontotemporal dementia is a neurodegenerative disorder characterised by a decline in social cognitive capabilities. One theoretical framework proposes that this decline is due to neurodegeneration affecting the way in which social contextual information is processed. Interestingly, preliminary evidence suggests that both bvFTD patients as well as simpler artificial network models display a contextual bias when interpreting a social scene in comparison to neurotypicals and multimodal AI models. We investigated this hypothesis by designing and running a novel social cognitive paradigm in neurotypicals, bvFTD patients and AI models. Our preliminary results in humans show initial support for the hypothesis that bvFTD patients are more influenced by contextual cues than neurotypicals. Further analyses will compare these behavioural findings with AI models."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "A Reinforcement Learning Model for Task-modulated Perception",
    "presenter": "Shervin Safavi",
    "poster_id": "B71",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Perception is a highly active process; therefore, it can and should be approached from a decision-theoretic perspective.  Elevating perception from a mere sensory inference to a decision process requires us to consider, for instance, how the value of sensory objects influences what we perceive and how the task at hand affects our perception. Here, we suggest that multistable perception could be a suitable candidate to study task-modulated perception in both humans and animals. Multistable perception is the dynamical alternation that arises when a single sensory input has more than one interpretation or explanation. Multistable perception is one of the most venerable perceptual phenomena that has been formalized as a decision process. We extend the previous model of perceptual multistability by incorporating a richer state space and action repertoire for a reinforcement learning agent, and we show that this allows us to explain the established task-modulated perception during perceptual multistability. Our model replicates and explains recent findings on the modulation of perception by task observed in previous studies.\nThis is achieved by incorporating two key elements-- changes in attentional resource allocation and representation of the environment volatility-- into a reinforcement learning paradigm. These changes are implemented in the model by systematically adjusting the observation and transition functions in our partially observable Markov decision process (POMDP) model of perpetual decisions. Taken together, our findings further support the view that perception is an active, goal-directed process, aligned with principles shared by other aspects of cognition."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Testing an Integrative Framework for the Sense of Control",
    "presenter": "Alireza Modirshanechi",
    "poster_id": "B72",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "The extent to which we experience a sense of control over our environment shapes how we perceive the world and plan our actions. But when precisely do we consider ourselves to be in control? Here, we define the sense of control as the 'degree of _a priori_ readiness to collect rewards that have yet to be announced.' We formalize this notion and propose degree-$l$ empowerment ${\\rm Emp}_l$ as a unified measure that integrates various conceptualizations of the sense of control. $l$ is a free parameter that regulates how the sense of control depends on three fundamental determinants: (i) action availability, (ii) _certain_ achievability of potentially desired outcomes, and (iii) _possible_ achievability of potentially desired outcomes. We show that ${\\rm Emp}_l$ accurately predicts more than 80% of participants' decisions in an experimental paradigm in which they choose between possible and certain achievability of future rewards, and that the value of $l$ effectively captures inter-individual differences in participants' preferences that are also associated with scores on the widely recognized Locus-of-Control survey. Our findings thus lay a foundation for identifying the human sense of control and investigating its relationships with personality traits, psychological disorders, and broader sociological conditions."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Memory of Navigation in Individuals with Varying Social Anxiety: A Virtual Reality EEG Study",
    "presenter": "Haiyan Wu",
    "poster_id": "B73",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "While social exploration and spatial navigation share overlapping cognitive processes, how social anxiety influences navigation behavior, memory, and neural activity in immersive VR environments with social feedback remains unclear. This study combines VR and EEG to examine how individuals with varying social anxiety levels navigate virtual spaces under socially rewarding (e.g., positive facial feedback) or punishing conditions. Participants searched for landmarks in VR while receiving token rewards/punishments and feedback from strangers. Analysis of navigation paths, curiosity, and anxiety revealed correlations, and EEG data showed improved memory performance over time. Notably, high socially anxious individuals displayed altered hippocampal-prefrontal connectivity and frontal alpha asymmetry during social threat exposure. These findings advance understanding of socio-spatial neural mechanisms and potential therapies for social anxiety."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Why is GABA related to neural distinctiveness? A computational account of age-related neural dedifferentiation",
    "presenter": "Quan Zhou",
    "poster_id": "B75",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Neural activation patterns in response to different stimuli (e.g., houses vs. faces) are less distinctive in older than younger adults—a phenomenon known as age-related neural dedifferentiation. A growing body of evidence suggests that GABA, the brain’s primary inhibitory neurotransmitter, may play a role: GABA levels decline with age and are associated with individual differences in neural distinctiveness. To explore why, we used an independently developed model of koniocortex to simulate the relationship between GABA (measured using MR spectroscopy) and neural distinctiveness (measured using fMRI) in a large sample of older adults. We manipulated the amount of divisive normalization in the model to simulate individual differences in GABA levels and assessed the impact on the distinctiveness of the model’s output activation patterns (measured by computing the average cosine similarity among the patterns). Cosine similarity significantly predicted empirical neural distinctiveness values measured by fMRI and mediated the relationship between GABA and neural distinctiveness. These findings provide a computational account of how age-related declines in GABA reduce the brain’s ability to maintain distinct neural representations."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Relational Information Predicts Human Behavior and Neural Responses to Complex Social Scenes",
    "presenter": "Wenshuo Qin",
    "poster_id": "B74",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Understanding social scenes depends on tracking relational visual information, which is prioritized behaviorally and represented in the superior temporal sulcus (STS). However, computational models often overlook these cues. Here, we evaluate two social interaction recognition models, SocialGNN and RNN Edge, that explicitly incorporate relational signals—gaze direction or physical contact—and compare their predictions to human behavioral and neural responses. SocialGNN organizes video frames into a graph structure with nodes representing faces and objects, and edges encoding relational signals. RNN Edge is simpler, processing only relational information over time without node features. We found both models strongly predicted human behavioral ratings of social interactions and were comparable to state-of-the-art AI models with far less training data and simpler architectures. Both models also better predict STS responses of people watching social interaction videos than a matched visual model trained without relational cues. These findings underscore the value of integrating relational cues into computational models of social vision."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Other’s Pain Drives Optimal Decisions",
    "presenter": "Judit Campdepadrós Barrios",
    "poster_id": "B77",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Decision-making under uncertainty may be different for monetary self-gains than when someone else’s pain is at stake. We use a three-armed bandit task in self-gain and other-pain conditions, and observe that participants show better performance when trying to avoid pain for another individual. Differences in behaviour stem from overstaying decisions in the condition of self-gain. Computational modeling reveals higher consideration of value in pain decisions than in gain ones."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural Mechanisms Underlying the Impact of Pavlovian Observational Learning on Decision Making",
    "presenter": "Pyungwon Kang",
    "poster_id": "B76",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Increasingly, modern humans encounter threats indirectly, through social networks and media, affecting their behavior significantly which can increase anxiety and affect subsequent decision making. However, the underlying neural mechanisms are unclear. This study investigates how observational threat learning shapes instrumental decision-making (threat avoidance learning) and its neural basis. During neuroimaging 44 participants observed others experiencing threats, which enhanced subsequent instrumental learning, especially when observational Pavlovian cues aligned with instrumental outcomes—an effect absent without observational learning. Computational modeling indicated arbitration between observational Pavlovian and instrumental values. Neuroimaging revealed that periaqueductal gray (PAG) activity correlated with observational aversive prediction errors, which in turn affected subsequent decision-making. These findings suggest that the PAG plays a critical role when socially acquired threat information shapes instrumental threat avoidance."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Social responsiveness buffers internalizing symptoms from social isolation",
    "presenter": "Shuhan Wang",
    "poster_id": "B79",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Social isolation is related to internalizing symptoms, but individual differences exist in the susceptibility to the negative effect of social isolation. Little is known about the factors contributing to such variable outcomes brought on by social isolation. One possibility is that social responsiveness (i.e., adaptability of social behavior) may modulate the relationship between social isolation and internalizing symptoms. In this study, participants played an iterative trust game, where they learned the trustworthiness of social others. Social responsiveness is quantified as responsive investments to others’ trustworthiness by computational modeling. We found that social responsiveness attenuated the effect of social isolation on internalizing symptoms, which suggests that social responsiveness is a protective factor against negative outcomes from social isolation."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "rewardGym - a framework for streamlining experiments in cognitive neuroscience",
    "presenter": "Simon R Steinkamp",
    "poster_id": "B78",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Despite significant efforts, psychological and cognitive sciences currently lack standardized frameworks for setting up tasks and models in ways that can be integrated with data, generalized over tasks, and shared between researchers. Typically, researchers program an experimental task, assess how a theoretical model would solve it, and analyze behavioral data, all using custom code. Due to a lack of standardization, cognitive models are often experiment-specific, making it difficult to test the generalizability of models across different tasks and increasing the burden of testing existing models on new tasks. Here, we present a standardized framework that addresses these challenges. We build on the Gymnasium standard from reinforcement learning (RL), which defines how artificial agents interact with computational environments. This standard helps us establish a common graphical language for different tasks that captures the connections between states, actions, and rewards. This representation is further inspired by neuro-nav, which extends the Gymnasium framework to classical neuroscience experiments and focuses on neurally plausible RL. By expressing tasks in a formal language, it is possible to build libraries of models where agents can perform each task. This allows standardized software to perform parameter inference and model comparison on real and synthetic data. What distinguishes our framework is its focus on running experiments. We provide a high degree of control over the environments (e.g., stimulus order) and a direct way to augment the graphical representations with stimulus information. This allows for a direct transition from simulations with artificial agents to running experiments with human participants using PsychoPy. Additionally, we provide logging utilities that save data in BIDS format, which is standard in neuroimaging. With this framework, we hope to make psychological and cognitive science more reproducible and robust."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Modelling homeostatic influences on human risky choice",
    "presenter": "Steven Geysen",
    "poster_id": "B80",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Decision-making is often thought to be under homeostatic control. However, evidence remains mixed and the underlying mechanisms are unclear. One potential mechanism that might drive changes in decision-making may be the hunger hormone ghrelin, which interacts with the dopaminergic system. In two separate studies, we examined the effects of manipulating ghrelin levels on human risky choice in healthy male participants, either via a brief fasting period (study 1, $N=37$) or one night of total sleep deprivation (study 2, $N=40$). We found no credible effect of the experimental manipulations on the proportion of risky choices. Computational modelling did not reveal consistent effects of  homeostatic manipulations on model parameters. Including manipulation-induced changes in ghrelin levels in the model reveal no robust associations. FMRI analyses did not reveal homeostatic effects on neural signatures of subjective value or choice. Our results suggest that homeostatic influences in risky decision-making may be weaker than previously thought."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Preliminary evidence indicates that selective maintenance of adverse events may explain conditioning phenomena attributed to fear generalization",
    "presenter": "Deepta Chandrasekhar",
    "poster_id": "B82",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Understanding how fear memories are maintained over time is crucial for improving the effectiveness of anxiety treatments. Previous work suggests that fear generalizes over time from the stimuli associated with aversive events to other, similar, stimuli, and that such stimuli are remembered better due to their association with potential aversive events. At the same time, a recent model showed that memory maintenance that is specific to the feared stimuli can explain phenomena such as why a fear response that is extinguished sometimes returns over time. Here, we test the prediction of this model that memory will be better for the specific stimuli associated with aversive events, and not for similar stimuli that were not followed by an aversive event. N=441 participants completed an online-administered fear-conditioning task with trial-unique stimuli from two categories (animals and objects) and a subsequent surprise recognition memory test. Preliminary results indicate that participants had better memory for the category of stimuli associated with aversive events compared to the stimulus category that was not associated with an aversive event. However, this effect was mainly driven by memory of the specific stimuli from trials with the aversive event. These results support the idea that memories in this paradigm are primarily organized according to emotional rather than semantic similarity, as has been shown in other domains as well."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Mechanisms of Coordination for Cooperative and Competitive Interactions",
    "presenter": "Sepideh Khoneiveh",
    "poster_id": "B81",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "How do individuals coordinate in dynamic social contexts that require switching between cooperative and competitive modes? We developed a two-player Social Foraging Task (SFT) where participants, without direct communication, freely choose between cooperative, competitive, or independent islands to coordinate and collect resources. Analysis revealed four key behaviors influencing coordination: value sensitivity, persistence, their interaction, and leader–follower dynamics. Clustering participants’ choices revealed three distinct strategies: economic, sticky, and mixed which each linked to different coordination patterns. To capture these, we built a computational model with two parameters: value sensitivity and persistence. In well-coordinated pairs, these parameters were more similar, suggesting strategic alignment supports stable interaction. Notably, uncoordinated choices were common but often purposeful, serving to re-establish coordination. These findings shed light on how people achieve, lose, and recover coordination in dynamic social settings."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Computational mechanisms underlying how humans adapt their choices to the average effort of the environment",
    "presenter": "Emma V. Scholey",
    "poster_id": "B84",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Would the tallest hill in Amsterdam seem as effortful to climb in the Alps? Effort is relative, and deciding to exert ourselves can depend on how effortful other options in the environment are on average. But how people decide whether to exert effort as the average effort distribution of the environment changes remains unclear. Across three experiments, participants completed a novel task choosing whether to accept an offered level of physical effort for rewards or wait for potentially easier alternatives. Participants completed this task in both `easy' and `hard' environments with different average effort distributions. We found that people dynamically adjusted their effort preferences based on the environment, becoming less willing to exert mid-levels of effort in easier environments. A computational model tracking average effort rates could explain these choice behaviour patterns. These results provide a computational framework for understanding how effort-based choice is influenced by the environment."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Demographic Prompting Fails to Bridge the Individual Variability Gap: GPT-4o Aligns with Average but Not Individual Emotional Ratings of Images",
    "presenter": "Chace Ashcraft",
    "poster_id": "B83",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Large language models (LLMs) and vision language models (VLMs) have been shown to closely align with human behavior in aggregate, but tend to align less well with individuals, and poorly approximate the variability of cohorts of human agents. We explored aligning models to specific individuals based on their demographic data on an emotion rating task by eliciting ratings along two standard psychological emotion dimensions on the previously human-normed OASIS dataset. We created AI \"proxy\" participants for human participants in the original OASIS study by prompting GPT-4o with a human participant's demographic data, then instructed the AI participant to rate a set of images for emotional valence or arousal, reproducing the human paradigm. We found that group-averaged GPT-4o ratings correlated to group-averaged human responses, but observed different distributions of responses. Representations of specific individuals poorly aligned with human ratings, despite using specific demographic data. In general, GPT-4o appears to align fairly well with human emotional responses on average, but work is needed to capture human variability to enable VLMs to emulate the behavior of specific individuals."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Excitatory-Inhibitory Dynamics in Adaptive Decision-Making",
    "presenter": "Veronica Chelu",
    "poster_id": "B86",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Neural circuits rely on excitatory–inhibitory (E/I) interactions to support adaptive learning and decision-making. Here, we investigate how these dynamics contribute to flexible behaviour across three modelling levels. First, using a simplified mean-field model of two-choice decision-making, we examine the computational role of selective excitation and inhibition in stabilizing or amplifying competition between alternative choices. Building on these insights, we embed a similar E/I mechanism into the preference function of a reinforcement learning (RL) agent, showing how inhibitory feedback modulates behavioural adaptation in dynamic tasks. Finally, we assess the scalability of these principles by training RL agents with E/I-constrained recurrent neural networks (RNNs) in changing environments. While a general E/I architecture allows broader forms of inhibitory influence, our results indicate it hinders learning in these settings. In contrast, a structured architecture enforcing locally-constrained inhibition preserves biological plausibility while maintaining robust performance. Together, these findings  suggest that E/I dynamics may provide a feasible computational mechanism for flexible decision-making and continual learning."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Inferring Cognitive Load from Deviations in Simulated Human Planning Behavior",
    "presenter": "May Kristine Jonson Carlon",
    "poster_id": "B87",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Understanding how cognitive load shapes human planning behavior is crucial for building AI systems that collaborate effectively with people. While traditional approaches to measuring cognitive load such as self-report questionnaires or dual-task paradigms are valuable, they often lack real-time responsiveness or introduce artificial task constraints. This work is a proof-of-concept for inferring cognitive load from deviations in planning, thus avoiding intrusive or retrospective measures. We simulate user profiles performing a structured task (summarizing an article), with behavioral noise introduced via repetition, backtracking, pausing, and skipping actions. A Hidden Markov Model (HMM) is used to infer latent cognitive states from the resulting behavioral traces. Results from 100 Monte Carlo trials show that the HMM reliably recovers latent states aligned with intuitive levels of cognitive load. Emission patterns are interpretable, stable across trials, and distinct for each state, capturing predetermined behavioral signatures of low, medium, and high mental effort. State assignments also show alignment with simulated user profiles. Our approach provides a simulation-based foundation for modeling cognitive variability and may inform future work in user modeling, Theory of Mind, and adaptive systems."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Construction and disruption of hippocampal cognitive graphs in human and machine",
    "presenter": "Deng Pan",
    "poster_id": "B85",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Humans build internal cognitive graphs that encode the structural relationships between states, goals, and concepts, supporting flexible behaviour. Both the hippocampus (HPC) and orbitofrontal cortex (OFC) are implicated in cognitive map formation, but their distinct roles remain debated. We hypothesised that the HC encodes relational structure among states (“state–state” associations), while the OFC links each state to their goals (“state–goal” associations). To test this, participants performed a structure reversal learning task during fMRI, requiring adaptation to changing state transitions and goals. Computational modelling showed that participants utilised abstract structural knowledge for inference, and multivariate analyses revealed complementary neural representations: the HPC represented the transition structures while the OFC encoded goals. A recurrent neural network (RNN) trained via meta-reinforcement learning (meta-RL) recapitulated these patterns. Disrupting the human HPC using transcranial ultrasound stimulation (TUS) or lesioning HPC-like units in the RNN selectively impaired transition structure learning. Together, these results demonstrate complementary roles: the HPC constructs the foundation of cognitive graphs, while the OFC uses them to support goal-directed behaviour."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "The Role of Neural Replay in Structure Learning and Value Generalization",
    "presenter": "Fabian Renz",
    "poster_id": "B88",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Humans can quickly learn and update latent task structures, and use them to guide value-based decision-making. In a functional magnetic resonance imaging study, 52 participants learned a latent graph structure requiring non-local value generalization upon reward reversals. Performance was best explained by a latent cause inference model that captures structure learning and admits value generalization. The functional imaging data offered a possible substrate for this generalization by demonstrating that the hippocampus tracked the underlying task structure and exhibited non-local reactivation of unobserved sequences sharing a reward."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Experimental Assessment of Choice-Congruent Confidence Bias in Human Reinforcement Learning",
    "presenter": "Alexandre Lietard",
    "poster_id": "B89",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Confidence constitutes a fundamental signal which aids adaptative processes in both learning and decision-making. Yet, it often deviates from optimality. In the domain of perceptual decision-making, one such deviation, known as the choice congruent bias, reflects the tendency to overweight evidence that supports the chosen option. As a result, confidence tends to increase with the amount of overall evidence, even when accuracy remains unchanged. Here, we examined whether the same biased computation occurs in value-based decisions, by testing whether greater overall evidence leads to higher confidence in a reinforcement learning task.\nThe results demonstrate that increasing the average reward (considered as evidence) successfully elevates confidence levels. Our computational modelling indicates, however, that this effect does not necessarily reflect a biased confidence computation. Therefore, while manipulating average reward may be a useful method for dissociating confidence from accuracy, it cannot serve as a direct test of the choice-congruent bias."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Optimal Foraging by Learning the World Model",
    "presenter": "Roxana Zeraati",
    "poster_id": "B90",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Patch foraging—deciding when to leave a depleting resource to search for alternatives—is a fundamental aspect of animal behavior and offers a window into ethologically grounded decision processes. Several theories, most notably the Marginal Value Theorem (MVT), have proposed strategies for optimal foraging. However, they typically ignore most details of the spatiotemporal structure of the environment, and particularly the dynamics of the replenishment of patches. We investigate optimal patch foraging with richer replenishment timescales. Using average-reward reinforcement learning (RL), we show that under slow replenishment, optimal policies leverage the world model to generate higher reward rates and distinct behavioral statistics from MVT and similar policies. Our results provide testable predictions for future experiments."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "How to reliably estimate collapsing-threshold diffusion model parameters?  A simulation study",
    "presenter": "Amir Hosein Hadian Rasanan",
    "poster_id": "B91",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Evidence accumulation models have become the dominant theory in explaining neural and behavioral constructs of decision-making. The main principle of these models is that a decision-maker accumulates noisy evidence until a constant threshold is reached. However, several behavioral and neuroscientific findings, besides some theoretical motivations like optimality, have led to alternative proposals, such as ``collapsing threshold\" models. Usually, these models offer a more accurate fit to empirical data. However, a major issue with these models is the unreliability of parameter estimation. Due to this, researchers have relied solely on model fit comparisons, avoiding interpretation of the parameter values -- leading to controversial findings in the literature that support these models. This work introduces a reliable model estimation framework by linking the non-decision time to external measurements. In this modeling framework, we consider a joint likelihood function for behavioral measurements and the non-decision time measurement, constraining the non-decision time estimation. The results of a parameter recovery study showed that the proposed joint model makes the collapsing threshold parameters identifiable."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Trial-by-trial Fluctuations in Decision Criterion Shape Decision Confidence",
    "presenter": "Robin Vloeberghs",
    "poster_id": "B92",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Many choices we make are accompanied by a sense of confidence. Within Signal Detection Theory (SDT), confidence is traditionally conceptualized as the absolute distance between a decision variable and a decision criterion. Whereas this criterion is typically modeled as being stable over time, increasing evidence suggests that it undergoes trial-to-trial fluctuations. Based on theory and model simulations, we predict that fluctuations in the decision criterion shape confidence. Using the Hierarchical Model for Fluctuations in Criterion (hMFC) to obtain single-trial criterion estimates, we found robust evidence for this hypothesis across 15 datasets. When analyzing each dataset individually the effect was found in 13 out of 15 datasets, indicating a stable pattern across a variety of paradigms and confidence scales. Our results demonstrate that what has been previously interpreted as noise in confidence ratings, instead reflects variability driven by fluctuations in the decision criterion."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural dynamics of predicting others’ decisions",
    "presenter": "Erik Stuchlý",
    "poster_id": "B93",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Although previous work has found that the same valuation mechanisms are used when making choices for oneself and when predicting the choices of others, other brain signals distinguishing these two types of decisions were also identified. What remains less clear is how these two processes interact and when exactly they emerge. In this study, we used EEG to investigate the dynamics of when these two processes emerge within the course of a decision. Specifically, we compared the event-related potentials (ERPs) of participants when making risky choices for themselves and when predicting the choices of two artificial agents (one with a similar, one with a dissimilar risk preference). We fitted the ERP data with a linear regression model which included the predictors of decision type ($self / similar / dissimilar$), trial-level option value difference and the interaction between the two. We found evidence for a valuation signal occurring in the frontal, central and parietal channels about 0.8-1s post-stimulus presentation, which did not change across the three decision types. Additionally, centro-parietal activity about 0.6s post-stimulus distinguished choice for a dissimilar agent from choices for self. Our findings suggest that the brain may first encode the perceived self-other similarity of the decision recipient, followed by a domain-general value computation."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural Mechanisms of Adolescent Social Decision-Making: Insights from Computational Modeling",
    "presenter": "Jiamiao Yang",
    "poster_id": "B94",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Adolescents often prefer immediate rewards for themselves over delayed rewards for others, but the underlying motivations: impulsivity or selfishness remain unclear. We applied a novel computational model to disentangle these factors by independently estimating discounting rates for self and other and the relative weight of others’ rewards. Using behavioral and fMRI data from 88 adolescents, we found that participants discounted delayed rewards more steeply for others and prioritized self-rewards. Age was associated with reduced impulsivity for others but not for self. fMRI results revealed that value-based decisions involving self and other recruited the right temporoparietal junction (TPJ), while decision difficulty engaged the cingulate cortex, insula, and amygdala. These findings highlight age-related improvements in social decision-making and suggest distinct neural mechanisms for evaluating rewards for self vs. others during adolescence."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Biased Misinformation Distorts Beliefs",
    "presenter": "Rani Moran",
    "poster_id": "B96",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Misinformation, particularly biased news, poses a growing threat to open societies by driving polarization and reinforcing false beliefs. This study explores how individuals process biased information through a reinforcement learning task. Participants (n=200) took part in a \"bandit task,\" receiving feedback from biased (favorable, unfavorable) and unbiased sources. They first learned about the biases of these sources, then used this knowledge to adjust their belief updating. Although participants could identify and account for bias, their corrections were incomplete, leaving residual distortions in their beliefs. Exposure to biased sources also led participants to perceive unbiased sources as biased. These results underscore the difficulty of maintaining accurate beliefs in biased environments and offer strategies for combating misinformation."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Information search strategies in within-domain vs. across-domain decisions",
    "presenter": "Maryam Tohidi-Moghaddam",
    "poster_id": "B95",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Extant theories of value-based decisions, in particular the common-currency hypothesis, assume that the brain computes subjective values for choice options and selects the preferred option on the basis of these value representations. Here, we challenge this view by arguing that the need to compute integrated option values depends on the type of decisions. Specifically, we contrast within-domain decisions, where options share common attributes, and across-domain decisions, where options have fundamentally different attributes. Because attribute-wise comparisons of different options are possible in within- but not across-domain decisions, we expect a higher need to compute option values in across-domain decisions. This should be reflected in different visual search patterns. To test this, we designed a value-based multi-attribute task with two types of within-domain and across-domain decisions. We recorded eye movements during the process of decision making and integrated value formation. Consistent with our hypothesis, participants exhibited more attribute-wise search patterns in within- compared to across-domain decisions, even though behavioral characteristics remained unchanged. Our results suggest that people use different decision strategies, that differ with respect to the need to compute integrated option values."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "MoralNet: Visual representations of moral intuitions in artificial and biological neural networks",
    "presenter": "Tim Lauer",
    "poster_id": "B97",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Humans quickly detect moral situations in visual surroundings, suggesting that moral perception is attuned to features of the sensory environment. Yet, few computational models describe how combinations of stimulus features evoke perceptions of different moral situations. Here, we develop a convolutional neural network that decodes images into 10 distinct moral categories. We train and cross-validate the model on socio-moral images and show that image content is sufficient to predict the category of human morality ratings. In a fMRI study, we demonstrate that patterns of human visual cortex activity encode moral category–related model output and can decode multiple categories of moral scenes. Our preliminary results suggest that category-specific visual features can be reliably mapped to distinct moral intuitions, and they are coded in distributed representations within the human visual system."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neurocomputational Underpinnings of Suboptimal Beliefs in Reinforcement Learning Agents",
    "presenter": "M Ganesh Kumar",
    "poster_id": "B98",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Maladaptive belief updating is a hallmark of psychiatric disorders, yet its underlying neurocomputational mechanisms remain poorly understood. While Bayesian models characterize belief updating in decision-making, they do not explicitly model neural computations or neuromodulatory influences. To address this, we developed a recurrent neural network-based reinforcement learning framework to investigate decision-making deficits in psychiatric conditions, using schizophrenia as a test case. Agents were trained on a predictive inference task commonly used to assess cognitive deficits found in schizophrenia, including under-updating beliefs in volatile environments and over-updating beliefs in response to uninformative cues. The task thus included two conditions: (1) a change-point condition requiring adaptation in a volatile environment and (2) an oddball condition requiring resistance to outliers. We modeled these deficits by systematically manipulating key hyperparameters associated with specific neural theories: reward prediction error (RPE) discounting and scaling (reflecting diminished dopamine responses), network dynamics disruption (reflecting impaired working memory), and rollout buffer size reduction (reflecting decreased episodic memory capacity). These manipulations reproduced schizophrenia-like decision-making impairments and revealed that suboptimal agents exhibited fewer unstable fixed points near network activity in the change-point condition, suggesting reduced computational flexibility. This framework extends computational psychiatry by linking cognitive biases to neural dysfunction and provides a mechanistic approach to studying decision-making impairments in psychiatric disorder."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Modeling Dynamical Vision: A Toolbox for Biologically Plausible Recurrent Convolutional Networks",
    "presenter": "Robin Gutzen",
    "poster_id": "B104",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image recognition and exhibit conceptual similarities to the primate ventral visual pathway. Adding recurrence opens the door to exploring temporal dynamics and investigating mechanisms underlying recognition robustness, attentional modulation, and rhythmic perception phenomena. However, modeling spatiotemporal dynamics of biological vision using CNN-based architectures remains challenging. Incorporating functionally beneficial recurrence, capturing biologically plausible temporal phenomena such as adaptation and subadditive temporal summation, and maintaining topographic organization aligned with cortical structure require significant computational considerations. Although recent advances have incorporated neurobiological constraints, the field lacks accessible tools for efficiently integrating, testing, and comparing these approaches.\nHere, we introduce DynVision, a modular toolbox for constructing and evaluating recurrent convolutional neural networks (RCNNs) with biologically inspired dynamics. Our approach facilitates the incorporation of key visual cortex properties, including realistic recurrent architectures, activity evolution governed by dynamical systems equations, and structured connectivity reflecting cortical arrangements, while maintaining computational efficiency. We demonstrate the framework's utility through systematic analysis of emergent neural dynamics, highlighting how different biologically motivated modifications shape response patterns characteristic of cortical recordings."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Do Dynamics Matter for Neural Alignment? A Comparative study of Video and Static Vision Models",
    "presenter": "Khaled Jedoui Al-Karkari",
    "poster_id": "B105",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Understanding how the brain constructs and updates visual representations from dynamic input is central to our comprehension of perception and cognition. While deep learning has achieved impressive performance in visual tasks, the extent to which these models capture the computational principles of biological vision is unclear. In this paper, we investigate the alignment between representations learned by vision Artificial Intelligence (AI) models and neural activity in biological brains. Using brain fMRI data, we benchmark a diverse range of models, trained on various tasks, in their ability to predict brain responses to image and video stimuli. Our results demonstrate a clear advantage for video-based representations over static image representations across all analyzed brain regions. Our findings suggest that temporal modeling is a key component in the development of models that better align with biological vision, providing new insights into computational modeling of vision."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Examining the precision of infants’ visual concepts by leveraging vision-language models and automated gaze coding",
    "presenter": "Tarun Sepuri",
    "poster_id": "B107",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Infants rapidly develop knowledge about the meanings of words in the first few years of life. Previous work has examined this word knowledge by measuring how much infants look at a named target image over a distractor. Here, we examine the specificity of that knowledge by manipulating the similarity of the target and distractor. We measured looking behavior in 91 14- to 24-month-old infants, enabled by automatic gaze annotation and online data collection. Using a vision-language model to quantify target-distractor image and text similarity, we find that infants' looking behavior is shaped by the high-level visual similarity of competitors: infants' looking to the target image was inversely correlated with image similarity but not with visual saliency. Our findings demonstrate how multimodal models can be used to systematically examine the content of infants' early visual representations."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Unsupervised Future-Predictive Learning in the Connectome-Constrained Drosophila Optical Lobe",
    "presenter": "Naoya Nishiura",
    "poster_id": "B106",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Recent advances in connectome-based modeling of the fruit fly brain have enabled neural network architectures that mirror the anatomical wiring of the optic lobe. Despite the success of supervised approaches in predicting neural activity and performing optic flow estimation, many natural settings lack explicit labels such as motion vectors. In this work, we propose an unsupervised learning strategy in which the input layer (R1--8) receives feedback signals to predict future visual inputs $I_{t + \\Delta t}$ without any external label. We show that this approach partially reproduces ON/OFF direction selectivity in T4/T5 neurons, a hallmark of the fly visual system."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Empirical test of ideal observer models of individual and ensemble spatial perception",
    "presenter": "Dominik Endres",
    "poster_id": "B109",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Individual and ensemble perception are crucial for interacting with objects in our environment. Individual perception processes single objects, while ensemble perception extracts summary information from object groups. To investigate how these two modes of perception work with different set sizes (3, 6, 10) in naturalistic settings, we compare two bayesian models on our data. The first model, a variant of the summation model, is the 'Individual Encoding Model'. The second model is the 'Ensemble Encoding Model', which is related to the automatic averaging model. We conducted an experiment in which participants encoded the position of an individual object or an ensemble position that summarized multiple objects in a 3D rendered scene and indicated its remembered position by mouse click on the screen. The 'Individual Encoding Model' assumes that each object's position is encoded in memory, the ensemble position is only evaluated on demand. In the 'Ensemble Encoding Model', the ensemble position is part of the process that generates the scene and is inferred from the observable object locations. We found that the accuracy of reproducing individual object positions increased as set size increased, while the estimation of the ensemble position (arithmetic mean) only differed between the 6- and 10-object set size conditions, with smaller deviations observed for scenes with 6 objects. The 'Ensemble Encoding Model' generally explains the human behavioral data better. The subject-specific bayes factors in its favor increase with set size. We conclude that in naturalistic scenes the choice between individual versus ensemble encoding is likely driven by the more compact scene representation of the ensemble model."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Iterative Bayesian inference explains the dynamics of perceptual organization of natural scenes",
    "presenter": "Tridib Kalyan Biswas",
    "poster_id": "B108",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Perceptual decision making is classically conceptualized as evidence integration theory – the notion that sensory inputs are perceived by sequentially accumulating noisy samples from the environment and averaging out the noise. Modeling with evidence integration has captured perceptual and neural dynamics elicited by parametric stimuli in simple tasks, but studies of natural vision reveal richer dynamics that remain poorly understood.\n\nIn this study, we propose samples in time are not accumulated from a noisy external environment, but from internal representations formed through Bayesian inference where the statistics of sensory inputs are refined iteratively. Thus, we aim to test if iterative Bayesian inference determines perceptual dynamics when processing natural stimuli.\n\nTo test this, we focus on natural image segmentation. We measured human perceptual segmentation using a recently published experimental design: participants judged whether pairs of regions in an image were in the same segment (‘yes’) or not (‘no’). Subjective segmentation maps were reconstructed for each participant with optimization on ‘yes’/‘no’ responses per pair.\n\nExamining responses where perceived segments were inconsistent with the segments established by the optimal subjective map, we observed that participants presented a bias toward responding ‘yes’ when the two regions were close and ‘no’ when far. Furthermore, decision times increased with distance for ‘yes’ responses, but decreased with distance for ‘no’ responses, and this effect was larger for participants with stronger bias.\n\nFor further inquiry, we developed image-computable segmentation models of the classical evidence integration and iterative Bayesian inference theories. Although both model types fit aggregate decision-time distributions similarly well, we found that the spatiotemporal dynamics observed in the data were captured only by iterative inference incorporating a Bayesian spatial proximity prior. This work highlights the importance of considering iterative Bayesian computations to understand human perceptual dynamics when exact inference is intractable, as in most real-life situations."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Perceptogram: Interpreting Visual Percepts from EEG",
    "presenter": "Virginia R. de Sa",
    "poster_id": "B111",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Recent advances in EEG-based visual decoding utilize diffusion models to generate realistic images from neural activity. Typically, these methods project EEG signals into latent spaces--most commonly, Contrastive Language–Image Pretraining (CLIP)--which define visuosemantic features for subsequent image reconstruction. Prior methods rely on deep and opaque models, overlooking the neural origins of decoded information. Here, we introduce Perceptogram, a unified, interpretable framework that uses paired linear mappings between EEG signals and CLIP latents, leveraging CLIP’s inherent structure. Perceptogram achieves state-of-the-art reconstruction quality and generates latent-filtered EEG maps, isolating neural activity relevant to specific visual attributes. These maps reveal clear spatiotemporal organization: $ \\approx 100\\ \\text{ms} $ post-stimulus, lateral posterior negativity encodes smooth textures and blue hues, while medial negativity captures textured images, red hues, and food semantics; $ \\approx 180\\ \\text{ms} $, lateral negativity signals animate objects. By identifying these distinct neural signatures, Perceptogram transparently delineates how visual features—from basic textures and colors to high-level object categories—are temporally and spatially represented in the brain."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Learning visual cognition from children's visual experiences",
    "presenter": "Klemen Kotar",
    "poster_id": "B110",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Even in infancy, children display sophisticated visual reasoning abilities, prompting long-standing debates over whether they are innate or learned. To study this, recent studies have trained computational models on egocentric video datasets from children, e.g., BabyView. However, they focused on perceptual tasks rather than more sophisticated visual reasoning, and used labels to fine-tune model readout layers, thus limiting the strength of their claims. In this work, we apply the recently developed Local Random Access Sequence (LRAS) framework, which progressively trains a series of self-supervised models. We train LRAS on 800 hours (approximately 0.2 child-years) of BabyView data. Our models successfully perform a range of 3D perceptual tasks for objects, depth, and scenes, as well as cognitive tasks such as simulation of future object motion and viewpoint changes, and physical reasoning about object cohesion, solidity, agent-object motion, and multi-object interactions. Notably, our models perform tasks in a unified, zero-shot manner, thus providing stronger evidence for the learning-based hypothesis. Overall, we establish a computational proof-of-concept that visual cognitive abilities can emerge from developmentally realistic experience through statistical learning with minimal innate priors."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural Dimensionality and Temporal Dynamics of Visual Representations",
    "presenter": "Zirui Chen",
    "poster_id": "B112",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Understanding the temporal dynamics of visual representations in the brain is a fundamental challenge. While research has shown that neural time series data contain rich information where many visual features can be decoded, less is known about how the stimulus representation itself evolves over time and how these dynamics are related to feature decodability. Here, we investigated these questions using EEG recordings from subjects viewing everyday objects. We found that the dimensionality of stimulus-related variance rapidly increases to nearly full rank after stimulus onset and is sustained for several hundred milliseconds. During this time, the underlying representations oscillate, with every latent dimension undergoing multiple sign flips. Interestingly, the time course of feature decodability closely corresponds to the window of high-dimensionality, and temporal-generalization patterns of above- and below-chance decoding accuracies correspond to sign flips of the representational dimensions. Furthermore, we found that behavioral features and neural network representations each capture only a subset of the neural dimensionality, suggesting that significant portions of neural activity represent information not accounted for by current measures. Together, our findings show that natural images elicit rapidly fluctuating high-dimensional representations, encoding rich sensory information that has yet to be explained by state-of-the-art behavioral and computational models."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Integrating Non-Classical Receptive Fields of the Primary Visual Cortex into CNNs Enhances Adversarial Robustness",
    "presenter": "Yalda Mohsenzadeh",
    "poster_id": "B113",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "The adversarial robustness of artificial intelligence models remains a critical challenge for their deployment in real-world applications. Brain-inspired approaches have garnered considerable attention in recent years, given the superior adversarial robustness observed in human vision. In this study, we propose a novel architecture named nCRF-SurroundNet. nCRF-SurroundNet performs surround modulation on the responses obtained after passing images into a Gabor filter bank. Our enhancements are guided by neurophysiology findings showing that V1 receptive fields extend beyond the classical receptive field where input is directly received by the convolutional kernel, to include the non-classical receptive field (nCRF). The nCRF plays a crucial role by either inhibiting or amplifying the output of the classical receptive field depending on the context around the classical receptive field. The proposed model is evaluated against standard convolutional architectures, including AlexNet, ResNet50, and the original VOneNet, using two benchmark datasets, CIFAR-10 and ImageNet100. Performance is assessed under the threat of adversarial attacks, specifically utilizing the projected gradient descent (PGD) and Carlini and Wagner (C\u0026W) attack methods. The results on CIFAR-10 and ImageNet100 demonstrate that our proposed models achieve significantly higher adversarial robustness against PGD attacks compared to existing models, while also maintaining relatively strong performance against CW attacks. Notably, on ImageNet100, our models surpass all other competitors on the PGD attacks and the stronger version of C\u0026W attack."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Video DNNs do not see motion illusions",
    "presenter": "Marvin Rainer Maechler",
    "poster_id": "B114",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "A key goal of computational neuroscience is to develop models that faithfully mimic human brain processing, including the efficiencies behind it. Visual illusions provide crucial test cases for this, as they are often the consequence of such efficiencies under biological constraints (e.g. efficient coding or optimal inference). Here we utilize illusions to investigate whether computer vision models process video inputs similarly to humans. Focusing on the double-drift illusion, we compare the representational geometry of these video models with behavioral and fMRI data from human subjects viewing the same stimuli. Representational similarity analyses reveal that while these models lack behavioral similarity to human observers, they do mimic the representational structure of some brain areas early in the visual processing hierarchy. Our findings demonstrate that, unlike humans, current vision models represent the physical stimulus at all points and do not combine motion and position information in a human-like manner.  We thus find fundamental differences between human vision and DNNs in how temporal visual information is processed at later stages."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Viewpoint Diversity Improves Convolutional Neural Network Generalization and Robustness",
    "presenter": "Yifan Luo",
    "poster_id": "B115",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Although convolutional neural networks (CNNs) reach human-level accuracy on standard object recognition tasks, they perform poorly when faced with changes in viewpoint or corrupted images. In this study, we demonstrate that these two distinct failure modes can be addressed using a single strategy: training on diverse viewpoints. To investigate this, we created artificial image datasets that systematically vary in viewpoint diversity while keeping the dataset size constant, to train and evaluated CNN object recognition performance. Our results reveal a core trade-off between learning speed and generalization performance. On the one hand, models trained on restricted viewpoints exhibit fast learning and achieve near-perfect in-distribution accuracy, but they overfit to specific views, resulting in dramatic performance drops on unfamiliar viewpoints. On the other hand, training with diverse viewpoints slows learning but significantly improves out-of-distribution performance. Notably, exposure to diverse viewpoints also greatly enhances robustness to common image corruptions. These results point to a shared mechanism for achieving robustness to both viewpoint variation and image corruption, and further alignment with human performance."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Contrast Sensitivity Function of Multimodal Vision-Language Models",
    "presenter": "Alexandra Gomez-Villa",
    "poster_id": "B116",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Assessing the alignment of multimodal vision-language models (VLMs) with human perception is essential to understand how they perceive low-level visual features. A key characteristic of human vision is the contrast sensitivity function (CSF), which describes sensitivity to spatial frequency at low-contrasts. Here, we introduce a novel behavioral psychophysics-inspired method to estimate the CSF of chat-based VLMs by directly prompting them to judge pattern visibility at different contrasts for each frequency. This methodology is closer to the real experiments in psychophysics than the previously reported. Using band-pass filtered noise images and a diverse set of prompts, we assess model responses across multiple architectures. We find that while some models approximate human-like CSF shape or magnitude, none fully replicate both. Notably, prompt phrasing has a large effect on the responses, raising concerns about prompt stability. Our results provide a new framework for probing visual sensitivity in multimodal models and reveal key gaps between their visual representations and human perception."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Exploring the Cognitive Space: A Framework for Cognitive Function Mapping",
    "presenter": "Ana Fernanda Ponce",
    "poster_id": "B117",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Obtaining an accurate topography of cognitive functions is a major endeavour in human brain imaging. Recent advancements have been driven by intensive within-subject scanning, known as deep phenotyping, to address inter-subject variability. However, work is still needed to quantify how speciﬁc cognitive functions contribute to brain activation. In this study, we integrate statistical summaries from the Individual Brain Charting project (IBC) with the Cognitive Atlas. Each fMRI contrast map was annotated with cognitive components derived from its associated concepts. We then applied dictionary learning to estimate the contribution, or weight, of each component to a given contrast map. The resulting weights were used as regressors in a General Linear Model (GLM) analysis that incorporated contrast maps from all IBC tasks. This approach provided a more sensitive cognitive mapping of brain functions at the individual level."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Controlled Synthetic Environments for Studying Mid-Level Vision in Artificial and Biological Systems",
    "presenter": "Joshua M. Martin",
    "poster_id": "B119",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "While artificial neural networks have advanced image-computable object recognition models, their ability to model mid-level vision remains limited. A key bottleneck is the scarcity of datasets with dense, high-quality annotations necessary for probing these intermediate computations. Here, we introduce a flexible synthetic image generation pipeline built in Blender that produces richly structured scenes with automatic pixel-level annotations, such as surface normals and segmentation masks. Drawing inspiration from digital embryos and dead leaves stimuli, the pipeline enables controlled manipulation of scene statistics and object properties. This provides targeted inputs for training and evaluating artificial neural networks, enabling detailed analysis of how specific visual features contribute to mid-level perceptual processes."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Response to Affine Transforms of Image Distance Metrics and Humans",
    "presenter": "Paula Daudén-Oliver",
    "poster_id": "B118",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "The standard approach to testing image quality models with deep architectures is through correlation with human opinion of distortions typically found in digital media. RAID-database presents a more human way of testing distorted images with affine transformations that are the ones found in nature. We have selected 6 image quality metrics (2 convenient references and 4 state-of-the-art) to test their alignment with human behavior with the same psychophysical method: Maximum Likelihood Difference Scaling. Although perceptual metrics are designed to predict human perception, we found that none of them accurately replicate human response curves for the three proposed affine transformations. Specifically, we analyzed the ranking regard human responses to different images within the same distortion, the ranking with regard human sensitivity of single images when different affine distortions are applied, and the shape of the MLDS response curve."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Temporal Processing of Spatial Frequencies in Visual Word, Object, and Place Recognition",
    "presenter": "Clémence Bertrand Pilon",
    "poster_id": "B120",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "The temporal dynamics of spatial frequency (SF) processing in visual recognition were studied through three studies involving word (Exp.1), object (Exp.2) and place (Exp.3) recognition tasks in normal adult observers. They had to report the target stimulus presented in a 200 ms display using a four-alternative-forced-choice task (4AFC). The stimuli were made of an additive combination of the signal (target stimuli) and of a visual white noise patch wherein the signal-to-noise ratio (SNR) varied randomly across stimulus duration. Four SF conditions were defined with center frequencies of 1.2, 2.4, 4.8 and 9.6 cycles per degree. Contrary to the \"coarse-to-fine\" theory (Bar, 2003), the results indicated a more complex, non-sequential processing pattern which varies across stimulus classes. In Exp.1 (words), the highest SF range dominates early processing, with a shift toward lower SFs later on. In Exp.2 (objects), initial processing was dominated by the 4.8 cpd band, followed briefly by 9.6 cpd, then 1.2 cpd, and finally 2.4 cpd. In Exp.3 (places), processing was dominated first by the 2.4 cpd band, followed by 9.6 cpd and 4.8 cpd bands. These findings challenge the coarse-to-fine model. Also, all studies found that SF processing is modulated not only by the passage of time but also by the oscillatory frequency spectrum of the stimulus, indicating that visual recognition involves an intricate interaction of temporal and SF factors."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Would the same inferences about visual memory have been made with LFPs as compared to spikes?",
    "presenter": "Catrina M. Hacker",
    "poster_id": "B121",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "A central premise of basic neuroscience research is that insights about the healthy brain may eventually inform treatments for neurological and neuropsychiatric disorders. While much of the recent progress in systems neuroscience has relied on densely sampled, high spatial resolution measures of neural activity (like spikes), most neural recordings available in humans are field potentials. Consequently, bridging the divide between animal and human neuroscience requires understanding how neural representations compare in those different types of data. To fill this gap, we analyzed a dataset with established spiking representations of visual memory to answer a simple question: would the same inferences about the neural representations supporting memory have been made if measures were limited to field potentials? Using spike and local field potential (LFP) data simultaneously recorded in inferotemporal cortex (ITC) of four macaque monkeys performing a visual memory task, we show that the neural representations of three variables are aligned across spikes and high-gamma activity: memory, memorability, and contrast. In addition, we show that condition-specific, and in some cases image-specific, neural representations are matched across both measures. These results suggest that data can be meaningfully compared across animals and humans in support of translational work, such as in developing brain computer interfaces and closed-loop therapeutic stimulation devices."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "The role of motion cues in object representation: A study of visual area specializations using deep learning",
    "presenter": "Nastaran Darjani",
    "poster_id": "B123",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "The human brain processes both static and motion-defined visual cues for object representation, yet most computational models emphasize static information. We investigated neural responses to motion-defined object stimuli (\"object kinematograms\") by comparing brain activity with a dual-pathway artificial neural network that separates slow- and fast-varying visual information. Our findings show that while this dual-stream network captures aspects of biological motion processing, integration of slow and fast information improves similarity to brain in some regions but not others. These results highlight the functional diversity across visual areas in dynamic object representation."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Training on Ecologically Relevant Tasks Improves Alignment Between Artificial Neural Network and Human Similarity Judgments",
    "presenter": "Aidan J. Seidle",
    "poster_id": "B124",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Artificial neural networks (ANNs) have emerged as leading models for predicting human behavior and neural data. While these models have been extensively studied in the visual domain, their effectiveness in modeling audition is comparatively underexplored. Recent work has found that some ANNs can predict aspects of auditory cortical processing, however it is not clear whether these models capture task-invariant representations of sounds. Here, we used human judgments of similarity as a benchmark for the generalization of auditory model representations. We hypothesized that similarity scores computed from models that best predicted neural activation patterns would strongly correlate with human similarity judgments. We compared human similarity judgments of pairs of sounds to cosine similarity calculated from different layers of seventeen ANNs, as well as a basic spectrotemporal model. The ANNs exhibited a wide range of variability in their correlations with human similarity judgments, and the best models were those trained on multiple tasks. Although there was a significant correlation between the ability of a model to predict fMRI data and the alignment with human similarity measurements, some models showed diverging values. This result suggests that separate criteria for correspondence to human behavior, neural data, and higher-level psychological processes may be necessary."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Evolution of Low-Level and Texture Human-CLIP Alignment",
    "presenter": "Pablo Hernández-Cámara",
    "poster_id": "B125",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "During the training of multi-modal models like CLIP, we observed an intriguing phenomenon: the correlation with low-level human image quality assessments peaks in the early epochs before gradually declining. This study investigates this observation and seeks to understand its causes through two key factors: shape-texture bias alignment and classification accuracy drop under noise. Our findings suggest that CLIP initially learn low-level visual features, enhancing its alignment with low-level human perception but also increasing its sensitivity to noise and its texture bias. As training progresses, the model shifts toward more abstract shape-based representations, improving noise robustness but reducing alignment with low-level human perception. These results suggest that these factors shared an underlying learning mechanism and provide new insights into optimizing the trade-off between perceptual alignment and robustness in vision-language models."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment",
    "presenter": "Jose Manuel Jaén-Lorites",
    "poster_id": "B126",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Vision Transformers (ViTs) achieve remarkable performance in image recognition tasks, yet their alignment with human perception remains largely unexplored. This study systematically analyzes how model size, dataset size, data augmentation and regularization impact ViT perceptual alignment with human judgments on the TID2013 dataset. Our findings confirm that larger models exhibit lower perceptual alignment, consistent with previous works. Increasing dataset diversity has a minimal impact, but exposing models to the same images more times reduces alignment. Stronger data augmentation and regularization further decrease alignment, especially in models exposed to repeated training cycles. These results highlight a trade-off between model complexity, training strategies, and alignment with human perception, raising important considerations for applications requiring human-like visual understanding."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Arrow of Time: an intensive fMRI dataset of brain responses to brief video presentations",
    "presenter": "Tomas Knapen",
    "poster_id": "B122",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. To this end, we introduce the Arrow of Time dataset, a novel resource that captures high-quality functional magnetic resonance imaging (fMRI) responses to thousands of richly annotated short (2.5s) naturalistic videos. Our goal is to provide a deep open-by-design dataset of brain responses to the field. \nParticipants are exposed to up to 2000 video stimuli while ensuring strict fixation, across up to 15 scanning sessions. The extensive and varied stimuli are annotated using neural network-based object segmentation, action recognition, and semantic descriptions. This allows researchers to investigate how the brain processes and represents complex visual information, such as object recognition, scene understanding, and semantic processing. \nThe dataset is generated using whole-brain 7 Tesla fMRI optimized for SNR, acquired at 1.6s and an isotropic voxel resolution of 1.7mm. After custom preprocessing geared towards optimal across-session coregistration, beta weights are estimated for each single video presentation using GLMsingle which projects out sources of noise and estimates the shape of the HRF at the single-voxel level. Initial quality assessments show noise ceiling values on a par with those in NSD. \nThis type of high-quality fMRI data allows for rigorous testing and refinement of models related to brain function and cognition. As a reusable resource, the Arrow of Time dataset will help elucidate the neural basis of naturalistic visual processing in the hands of researchers in the fields of cognitive neuroscience, psychology, and artificial intelligence."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Disentangling redundant and synergistic interactions in the alignment between auditory brains and machines",
    "presenter": "Christian Ferreyra",
    "poster_id": "B127",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Artificial neural networks (ANNs) have become increasingly useful for modeling how the brain builds representations from the natural world, yet the nature of their representational alignment with dynamic brain activity remains underexplored. Here, we introduce an information-theoretic framework to decompose representational geometries into redundant and synergistic components using partial information decomposition (PID). Combining magnetoencephalography (MEG) recordings from participants listening to natural sounds, and two sound-processing ANNs with categorical (CatDNN) and continuous (SemDNN) semantic outputs, we analyze time-varying brain-model alignment for two optimized stimulus sets. For low-agreement stimulus sets, where mutual information between models is minimized, SemDNN reveals higher mutual information with brain activity. PID further shows greater redundancy and synergy for SemDNN, suggesting sustained temporal integration of intermediate semantic features that can potentially afford a more accurate readout of the auditory environment. These results highlight the value of representational decomposition for detailing shared and complementary components of the alignment between brains and ANNs."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Recurrent processing strengthens feature representations throughout visual cortex",
    "presenter": "Floris P De Lange",
    "poster_id": "B128",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Visual processing begins with a feedforward sweep that creates an initial perceptual representation, which is then refined through recurrent (lateral and feedback) processing. While recurrent signals in visual regions are abundant, their functional role in perceptual inference remains largely unclear. Using functional magnetic resonance imaging (fMRI) and artificial neural network (ANN) modeling, we aimed to examine whether recurrence modulates how images are represented in early and late visual regions. Participants were briefly presented with novel ambiguous images that were challenging to categorize. A visual mask followed these images either immediately or after a delay, thereby blocking or allowing for recurrent processing. We found that activity in early visual regions was best encoded by early layers of a convolutional neural network. These representations could no longer be observed when images were immediately masked. Conversely, activity in later ventral and dorsal visual regions was best encoded by later layers, and remained robust in the immediate masking condition. Comparing the brain alignment of ANNs with different recurrent dynamics revealed that activity in later ventral regions under delayed masking was best explained by a model with both lateral and feedback recurrence, suggestive of a role for recurrence in perceptual inference. The general strengthening of feature-specific representations with delayed masking likely reflects an interplay between early and late visual cortex, where lateral recurrence may help denoise low-level features to form more accurate high-level interpretations, which in turn may help disambiguate low-level features through feedback."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "The BabyView dataset: High-resolution egocentric videos of infants’ and young children’s everyday experiences",
    "presenter": "Bria Lorelle Long",
    "poster_id": "B129",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Human children far exceed modern machine learning algorithms in their sample efficiency, achieving high performance in key domains with much less data than current models. This \"data gap'' is a key challenge both for building intelligent artificial systems and for understanding human development. Egocentric video capturing children's experience—their \"training data''—is a key ingredient for comparison of humans and models and for the development of algorithmic innovations to bridge this gap. Yet there are few such datasets available, and extant data are low-resolution, have limited metadata, and importantly, represent only a small set of children's experiences. Here, we provide the first release of a large developmental egocentric video dataset—the BabyView dataset—recorded using a high-resolution camera with a large vertical field-of-view and gyroscope/accelerometer data. This 868 hour dataset includes egocentric videos from children spanning 5 months to 3 years of age in longitudinal, at-home contexts. We provide gold-standard annotations for the evaluation of speech transcription, speaker diarization, and human pose estimation, and evaluate models in each of these domains.  We train self-supervised language and vision models and evaluate their transfer to out-of-distribution tasks including syntactic structure learning, object recognition, depth estimation, and image segmentation.  Although performance in each scales with dataset size, overall performance is relatively lower than when models are trained on curated datasets, especially in the visual domain. Our dataset stands as an open challenge for robust, human-like AI systems: how can such systems achieve human-levels of success on the same scale and distribution of training data as humans?"
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural networks and brains share the gist but not the details",
    "presenter": "Michael Bonner",
    "poster_id": "B130",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Artificial neural networks (ANNs) excel in complex visual tasks and exhibit a notable alignment with neural representations in the ventral visual stream. Despite these advances, debates continue over whether ANNs fully capture the intricacies of the visual cortex. In this work, we explore test-time augmentation (TTA) as a novel strategy to enhance model-brain similarity without modifying model architectures. Traditionally employed to improve prediction accuracy through the averaging of augmented inputs, TTA here is leveraged to generate feature maps that more accurately predict neural responses to visual stimuli in the high-level ventral visual stream. We demonstrate that TTA consistently improves the prediction of neural activations across diverse architectures—including AlexNet, ResNet50, Vision Transformers, and robustified models—irrespective of their training datasets. Remarkably, averaging features from semantically similar but structurally varied augmentations, including those generated via diffusion models conditioned on text, can outperform representations derived from the original images. These results suggest that the conceptual gist, rather than detailed visual properties, underpins the enhanced model-brain alignment facilitated by TTA. We discuss potential mechanisms driving this effect and outline future directions for dissecting the interplay between augmented representations and neural processing in the visual cortex."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Deep predictive coding networks partly capture neural signatures of short-term temporal adaptation in human visual cortex",
    "presenter": "Amber Marijn Brands",
    "poster_id": "B131",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Predictive coding is a prominent theory of cortical function that proposes that the brain continuously generates predictions about sensory inputs through a hierarchical network of top-down and bottom-up connections. Prior studies demonstrate that PredNet, a deep neural network built on predictive coding principles, indeed captures key characteristics of neural responses observed in primate visual cortex. However, one widespread neural phenomenon that remains unexplored in this context is short-term visual adaptation: the modulation of neural activity over time in response to static visual inputs that are prolonged or repeated. Here, we investigate whether PredNet exhibits two hallmark signatures of temporal adaptation previously identified in human intracranial recordings. We find that, similar to human visual cortex, activations of error units in the first layer of PredNet exhibit subadditive temporal summation to prolonged stimuli, reflecting nonlinear accumulation of response magnitude with increased stimulus duration. However, unlike the neural data, PredNet shows systematic responses to stimulus offsets. For repeated stimuli, PredNet exhibits slight response suppression for consecutively presented images, but no repetition suppression, a stronger response reduction to identical than non-identical images that is robustly observed in visual cortex. These discrepancies are consistent across different training diets, optimization strategies and model unit types. Overall, our results show that PredNet's activation dynamics only partly capture short-term temporal adaptation signatures in human visual cortex, suggesting that this particular instantiation of predictive coding does not fully account for neural adaptation phenomena."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Efficient spatial learning in the hippocampus via left-right theta sweeps in the entorhinal cortex",
    "presenter": "Callum Marshall",
    "poster_id": "B132",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Recent findings show that grid cells in the medial entorhinal cortex (MEC) generate an internal positional signal that sweeps outward from the animal’s current location into the surrounding environment. These sweeps alternate stereotypically between leftward and rightward directions across successive theta cycles, suggesting an intrinsic mechanism for spatial sampling. Here, we propose that left-right sweeps in grid cells support efficient spatial learning of recurrent connectivity between place cells in hippocampal region CA3. We extend a recent model that produces left-right sweeps in grid cells to show that: (1) synchronised left-right sweeps across grid modules drive coherent sweeps in downstream hippocampal place cells; (2) such sweeps accelerate the learning of place cell connectivity, facilitating rapid spatial map formation; and (3) disruption of theta oscillations abolishes sweep dynamics and impairs map formation. These findings suggest that grid cell theta sweeps provide an intrinsic scaffold for efficient spatial learning in the hippocampus. The model also generates testable predictions for how circuit-level disruptions—such as those arising during normal ageing or in the early stages of Alzheimer's disease—may lead to spatial deficits."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Putamen Representation of Habit as Predictive Marker of Compulsivity",
    "presenter": "Amogh Johri",
    "poster_id": "B133",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Compulsivity, marked by rigid and repetitive behaviors, is hypothesized to arise from overreliance on habitual control systems. We combined a habit-learning task with fMRI and multivariate pattern analysis to test whether the neural signatures of habitual control overlap with compulsive traits, as measured by the OCI-R. A cluster in the left posterior putamen reliably decoded Habitual versus Goal-Directed participants and classified high versus low OCI-R groups independently of behavioral habit expression. Cross-decoding revealed shared neural representations linking habitual control and compulsivity. Our results highlight overlapping neural substrates of habitual control and compulsivity, providing insight into the dimensional mechanisms of compulsive behavior."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Human predictions of event sequences for novel videos",
    "presenter": "Caroline Lee",
    "poster_id": "B134",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Prior knowledge facilitates predictions in novel situations. Schematic templates for typical sequences of events, called event scripts, support event segmentation and linking of information across related contexts. Here, we present preliminary data comparing recall based on episodic memory and predictions based on event scripts. Participants viewed the first segment of instructional videos, some of which they had previously viewed, and then verbally recalled or predicted subsequent events. We used large language models (LLMs) to quantify the semantic content of the responses, enabling comparison between episodic memory and prediction-based knowledge."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neuromodulatory systems partially account for the topography of cortical networks of learning under uncertainty",
    "presenter": "Alice Hodapp",
    "poster_id": "B135",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Human learning in a dynamic and stochastic environment relies on computational variables such as confidence and surprise. If the learning process is shaped by neuromodulation, then the spatial distribution of receptors and transporters across the brain could put constraints on the spatial distribution of learning-related neural activity. Here, using fMRI data from four probabilistic learning studies and a Bayesian ideal observer model, we reveal a strong spatial invariance across tasks for the functional correlates of confidence, and to a lesser extent, surprise. Using 20 PET receptor/transporter density maps, we then show that this invariance could be partly explained by the chemoarchitecture of the cortex. We identified multiple receptors and transporters whose distribution aligned with the spatial distribution of neural activity in the cortex. While many of these receptors/transporters are in line with previous proposals of neuromodulation of learning, the results also revealed novel associations that can be targeted in experimental studies."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural and computational evidence for a predictive learning account of the testing effect",
    "presenter": "Haopeng Chen",
    "poster_id": "B136",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Testing enhances memory more than studying. Although numerous studies have demonstrated the robustness of this classic effect, its neural and computational origin remains debated. Predictive learning is a potential mechanism behind this phenomenon: Because predictions and prediction errors (mismatch between predictions and feedback) are more likely to be generated in testing (relative to in studying), testing can benefit more from predictive learning. We shed light on the testing effect from a multi-level analysis perspective via a combination of cognitive neuroscience experiments (fMRI) and computational modeling. Behaviorally and computationally, only a model incorporating predictive learning can account for the behavioral patterns and the robust testing effect. At the neural level, testing and prediction error both activate the canonical reward-related brain areas in the ventral striatum, insula, and midbrain. Crucially, back sorting analysis revealed that activation in the ventral striatum, insula, and midbrain can enhance declarative memory. These results provide strong and converging evidence for a predictive learning account of the testing effect."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Evaluating view-invariant place recognition in humans and machines",
    "presenter": "Nathan Kong",
    "poster_id": "B138",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "We are able to perceive spatial structure in the world around us. This ability supports a range of downstream behaviours---from navigation to memory retrieval---and is thought to rely on a network of 'scene-selective' cortical structures. Feedforward deep learning models are often thought to provide a suitable approximation of these perceptual abilities. This human-model correspondence, however, has largely been evaluated in classification tasks. Here we develop a novel behavioural assay which reveals a profound gap between these vision models and human abilities. We collect a corpus of naturalistic scenes (panorama captures from Google maps) and format these environments into 'oddity' tasks: participants are presented with two different viewpoints from one location (A), alongside an image from a different location (B), and must identify the odd-one-out (B). Critically, we manipulate the angular difference between views and perceptual similarity between environments. Through a series of experiments, we find that humans substantially outperform models on this benchmark, and that human reaction times scale linearly with task difficulty. These data highlight the temporal dynamics of place recognition, challenging common assumptions about the feedforward underpinnings of this foundational human ability."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Hexadirectional modulation of grid cell firing by firing rate adaptation",
    "presenter": "Guglielmo Reggio",
    "poster_id": "B137",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Grid cells are crucial for encoding spatial knowledge and supporting navigation. To non-invasively detect grid cell activity in humans, hexadirectional modulation (HM) methods have been developed. These methods look for macroscopic brain activity that varies with movement directions with six-fold rotational symmetry. Despite their widespread use with functional neuroimaging, the cellular origin of HM remains unclear. Here, we analysed over 1,000 grid cells recorded from freely foraging rats to evaluate two proposed mechanisms: (1) firing rate adaptation and (2) directionally tuned conjunctive grid cells. We observed significant HM after controlling for variability in speed and location using a Poisson multiplicative model. Consistent with the adaptation hypothesis, firing was reduced when grid fields were visited in close succession, a pattern more common during grid-aligned movement. These results offer a physiological explanation for HM and inform the design of future experiments aiming to detect grid cell activity non-invasively."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Olfactory Neurotechnology: A Perspective on Awareness Biomarker Discovery for Mild Cognitive Impairment and Dementia",
    "presenter": "Hubert Kasprzak",
    "poster_id": "B140",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Early detection of dementia is crucial. We propose a novel approach using neurotechnology to assess olfactory awareness, hypothesizing that deficits in this domain precede traditional cognitive decline markers. Employing EEG and EBG to monitor olfactory bulb activity during scent presentation, we aim to capture real-time neural responses. This shift from scent recognition to awareness provides deeper insight into cognitive processing in mild cognitive impairment and dementia, potentially enabling earlier and more sensitive detection."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Decoding Signatures of Meta-Learning Abstract Structure",
    "presenter": "Sreejan Kumar",
    "poster_id": "B141",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Humans excel at learning abstract structure from limited data and applying it to novel situations—a capacity often attributed to meta-learning. While behavioral evidence supports this ability, the neural mechanisms by which abstract concepts are acquired and refined during learning remain unclear. In this study, we use magnetoencephalography (MEG) to examine how the brain dynamically constructs abstractions while learning a set of tasks generated with a compositional grammar. Through MEG decoding, our results show evidence of learning the grammar structure across multiple timescales, both within and across different trials. These findings provide neural evidence for meta-learning in humans, showing that abstract representations emerge during learning."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Visual and Goal Vector Signals Interact to Shape Behavior and Spatial Representations",
    "presenter": "Sandhiya Vijayabaskaran",
    "poster_id": "B142",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Spatial navigation relies on a variety of signals across different sensory modalities to guide movement towards a goal. While these signals can sometimes be redundant, they are crucial in the face of uncertainty, where navigating agents may have to switch between these signals or integrate them to determine the moving direction. We model these interactions in a deep reinforcement learning agent that uses two signals, vision and goal-vectors, to navigate. By analysing the agent's behavior and spatial representations, we show that it can successfully navigate using each signal independently or by integrating both. We show that this flexibility enables the agent to successfully cope with changing environments or with signals becoming contaminated with noise. Interestingly, our model also highlights a trade-off --- when integration is unnecessary, such as in an unchanging environment, relying on a single stable signal improves navigation. We use this insight to explain counterintuitive experimental results. Additionally, we show that the place-cell-like spatial representations emerging in the network are shaped by both signals, albeit to varying degrees."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Multi-task Learning in the Human Brain",
    "presenter": "Tobias Ludwig",
    "poster_id": "B143",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "As humans we are naturals at learning multiple related tasks together. Which mechanisms do we use to transfer knowledge between tasks and generalize to new ones? Here, we conducted an MEG study in which N=42 participants virtually fed animals with fruits. The animals had different preferences, serving as tasks, while the position of the fruits could be transferred. Behaviorally, we found that people are using a hybrid model with transferable features and task-specific values. In MEG data, we found enhanced activation of the parietal cortex for successful transfer learning compared to within-task learning."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Humans and Mice Navigate Mazes Alike. Can AI Beat Them?",
    "presenter": "Rogério Guimarães",
    "poster_id": "B144",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Mice are natural navigators, capable of few-shot learning in complex environments. Here, we explore how their performance in maze navigation can serve as a benchmark for comparing learning across species and artificial agents. We tested human participants on a virtual binary maze game adapted from a prior mouse study and found not only similar performance, but also striking parallels in learning dynamics. Like mice, humans rapidly optimized reward acquisition, exhibited sudden insights about the maze structure, and showed knowledge of the path home from their very first maze incursion. We then used this embodied navigation task to compare AI agents with both species. We showed that two canonical agents — a Deep Q-Learning (DQN) and a Large Language Model (LLM) — were outperformed by the biological learners. These results highlight the potential of naturalistic learning tasks for cross-species comparisons, and expose challenges and opportunities for advancing AI."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Replay of factorized temporal journey",
    "presenter": "Xuanlong Zhu",
    "poster_id": "B139",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Time is a fundamental dimension of episodic memory, structuring the sequence of events that form our experiences. While replay of spatial paths and item sequences has been extensively studied in recent years, the role of temporal replay remains unclear. Here, we asked whether the brain replays time in a factorized manner. Our results revealed content-independent temporal trajectories that were replayed both during memory retrieval and post-retrieval rest, with on-task replay supporting immediate recall and off-task replay  contributing to the consolidation of weaker memories. Furthermore, the alignment between cortical replay and sharp wave ripples in the hippocampal reveals that hippocampal-cortical replay may serve as a unifying mechanism for organizing \"where\", \"what\", and  \"when\" of episodic memory."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "A Biologically Plausible Computational Model of Hippocampal Neurogenesis and Pattern Separation in Memory",
    "presenter": "Jiachuan Wang",
    "poster_id": "B146",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Pattern separation, essential for encoding distinct memories of overlapping contexts, relies on dentate gyrus coding shaped by entorhinal input and strong lateral inhibition. Although synaptic plasticity and adult hippocampal neurogenesis have been implicated in this process, their precise contributions remain unclear. The Cbln4-Neo1 complex, which mediates plasticity at entorhinal cortical synapses in the dentate gyrus without affecting basal signal transmission, offers a unique target for investigation. In this study, we selectively deleted Cbln4 from inputs to the mouse dentate gyrus. We found that Cbln4 is required for behavioral pattern separation and suppresses activity-dependent neurogenesis. We then developed a biologically plausible computational model incorporating an entorhinal cortex-dentate gyrus circuit in a reinforcement learning framework.  Simulations suggested that either impaired synaptic plasticity or increased neurogenesis alone was sufficient to disrupt behavioral pattern separation by elevating representational similarity in the dentate gyrus. These findings highlight the role of Cbln4 in memory encoding and dissociate the contributions of synaptic plasticity and neurogenesis through computational modeling."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Rat on a sphere: Exploring alternate grid cell representations on a hemisphere",
    "presenter": "Muhammad Sahal Goolam",
    "poster_id": "B145",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Grid cells in the medial entorhinal cortex are fundamental to mammalian spatial navigation, forming periodic, hexagonal firing patterns that serve as an internal map. While these cells exhibit isotropic firing in flat two-dimensional environments, their behaviour in three dimensions is more irregular, with the mechanisms governing spatial cognition beyond planar surfaces remaining largely unexplored. This study examines how spatial encoding adapts to a curved two-dimensional surface embedded in three dimensions. Specifically, we simulate a rat’s movement on a hemispherical surface under varying gravitational strengths and consider three spatial encoding strategies based on the Successor Representation framework: (i) aligned with the sphere’s curvature (logarithmic projection); (ii) relative to the horizontal plane tangent to the hemisphere at its origin (orthogonal projection) and (iii) using polar coordinates. Our results show that orthogonal projection does not elicit particularly salient or regular grid patterns, which compromises localisation. In contrast, logarithmic projection improves localisation while encoding in polar coordinates enhances grid regularity at a cost to localisation. These findings highlight the need to account for third-dimensional effects on grid cell organisation even on two-dimensional surfaces. Our model establishes a theoretical foundation for future empirical studies and offers extensions to computational models exploring grid cell formation on flat two-dimensional surfaces (embedded in two-dimensional space)."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Dynamic Changes in Neural Manifolds During Working Memory",
    "presenter": "Aniol Santo-Angles",
    "poster_id": "B147",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "How cognitive operations supporting working memory, the ability to actively hold and manipulate information, are implemented through neural computations remains a topic of ongoing debate. Neuronal recordings in non-human primates suggest that working memory contents are encoded and maintained in low-dimensional representations, known as neural manifolds, with cognitive operations occurring through dynamic changes within these manifolds. In the present study, we tested this hypothesis using magneto- and encephalography data from human participants collected during a working memory task. In the task, a retro-cue  required maintaining, inhibiting or updating memory representations of two stimulus features: orientation and shape. We found that the size of neural subspaces encoding stimuli during the delay period dynamically changed with task demands, expanding or shrinking based on shifts in the relevance of stimulus features. These findings support the role of manifold dynamics in working memory."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural subspaces for motor planning and execution in Parietal Cortex",
    "presenter": "Stefano Diomedi",
    "poster_id": "B149",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Understanding how the brain plans and executes movements\nremains a major challenge in neuroscience. The\nsuperior parietal lobule (SPL), with its central role in sensorimotor\nintegration and visuomotor transformations, is\na key region in unraveling these mechanisms. We used\ndimensionality reduction techniques to investigate the\nstructure of neural subspaces underlying latent dynamics\nduring movement planning and execution in SPL. Specifically,\nwe tested three alternative hypotheses about the\nrelationship between subspaces in the two phases: (i)\nneural subspaces completely overlap between the two\nphases; (ii) planning and execution are characterized by\nindependent activity patterns along orthogonal dimensions;\nand (iii) the dynamics are partially shared and partially\nexclusive. We analyzed population activity recorded\nfrom three SPL areas (V6A, PEc, PE) in macaques performing\na delayed reaching task. Our results reveal that\nin areas V6A and PEc, shared neural patterns coexist\nwith phase-specific subspaces, while in PE, the activity\nis organized along largely orthogonal subspaces. This\ndistinction reflects the different sensorimotor processing\nroles of these regions and enhances our understanding\nof how parietal cortex contributes to motor control."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Distributed Working Memory in a Computational Model of the Human Brain",
    "presenter": "Rares A. Dorcioman",
    "poster_id": "B148",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Working memory is a fundamental cognitive function that allows us to transiently store and manipulate relevant information in memory. While traditionally associated with localized prefrontal activity, recent electrophysiological and imaging studies reveal distributed activity across multiple brain regions. To uncover the mechanisms behind this distribution, we developed a detailed, data-constrained model of the human brain by integrating diverse large-scale datasets. Our model demonstrates that distributed working memory patterns emerge primarily through long-range synaptic projections rather than solely from local recurrent connectivity. We found that the network operates optimally near a critical region at the edge of a bifurcation, consistent with recent experimental and modeling findings, and explains approximately 60\\% of the observed variability among brain areas involved in working memory. Furthermore, simulations of task-specific conditions, such as verbal and spatial working memory, indicate that high agreement with experimental data is achieved only when higher cortical regions modulate the network or when recurrent connectivity is enhanced across multiple circuits. These results suggest that working memory performance is the product of an interplay between distributed projections and context-dependent modulation, offering new insights into the neural substrates of human cognition."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Modelling the Effect of Audience Tuning on Generative Episodic Memory",
    "presenter": "Aya Altamimi",
    "poster_id": "B150",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Episodic memory is highly malleable and shaped by social interactions.\\ Rather than storing exact representations, it reconstructs incomplete traces using semantic information influenced by motivation and context. In shared reality situations, as shown by the saying-is-believing (SIB) paradigm, verbalizing information affects both audience perception and the speaker’s memory. Yet, most computational models assume faithful storage and neglect social influence. We have developed a generative episodic memory model combining a VQ-VAE for visual perception, a masking module for attention and hippocampal storage, and a transformer for semantic completion. Images are encoded, partially stored, and later reconstructed into plausible, not necessarily accurate, scenarios. The model introduces key innovations: blending multiple memory traces, incorporating emotional valence and biased reconstruction, and handling ambiguous stimuli. These features allow it to simulate SIB effects and social influences on memory, offering insights into how communication and context shape what and how we remember."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Non-Monotonic Plasticity in Large Language Models",
    "presenter": "Camila Kolling",
    "poster_id": "B151",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Neural representations in biological memory systems change systematically during associative learning. \nThe Non-Monotonic Plasticity Hypothesis (NMPH) proposes that these changes follow a surprising U-shaped pattern based on how strongly two items are initially related, with initially-moderately-related items becoming significantly more distinct after learning, rather than more similar.\nWe provide the first evidence that large language models (LLMs) also exhibit this non-monotonic pattern of representational change, aligning with the NMPH observed in humans.\nUsing an in-context associative learning paradigm, with no changes to model weights, we show that moderately similar token pairs significantly differentiate, and this differentiation occurs when accuracy is both highest and most stable across repeated item presentations.\nOur results suggest that LLMs can model human associative learning, offering a framework to study representational change during learning."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Classification of Mental Workload Spatial Effects using Riemannian Manifold",
    "presenter": "Agathe CHOPLIN",
    "poster_id": "B152",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "This study investigates the use of Riemannian geometry to classify mental workload from an EEG dataset collected in an aeronautical context. The analysis, based on EEG data recorded from 16 participants performing a Simon task, aimed to differentiate low and high workload conditions. Using covariance matrices and a Minimum Distance to Mean (MDM) classifier, the results demonstrate spatial effects of mental workload irrespective of the investigated spectral domain. This demonstrates that spatial information is distributed evenly across all explored frequency bands."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Neural heterogeneity shapes the temporal structure of human working memory",
    "presenter": "Daria Kussovska",
    "poster_id": "B153",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "The circuit computations that support memory maintenance in humans remain poorly understood. Persistent activity has long been thought to underlie the maintenance of information in working memory (WM). However, most supporting evidence has relied on trial-averaged neural responses, often overlooking the potential role of cellular heterogeneity. To address this, we analyzed single-trial spiking activity from intracranial recordings of neurosurgical patients performing a WM task. We developed a method for putative cell-type classification and examined the temporal dynamics of interneurons and pyramidal cells across encoding and maintenance task phases. Our findings reveal distinct cell-type-specific activity profiles during working memory maintenance and suggest that stimulus-specific information can be retained with minimal persistent firing, highlighting flexible and potentially energy-efficient circuit computations that support information storage in the human brain."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Reverse Spatiotemporal Hierarchy during Cross-modal Memory Recall and Imagery",
    "presenter": "Yu Hu",
    "poster_id": "B155",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Recalling past events is often accompanied by mental imagery of those experiences. Based on previous research, this process engages memory- and sensory-related brain areas. However, the underlying spatiotemporal dynamics remain poorly investigated. Here, we used naturalistic videos of audiovisual events and recorded fMRI data during the tasks in which human participants recalled visual contents when hearing associated sounds and recalled sounds when watching silent videos, after they had well memorized the video contents. With time-resolved fMRI multivariate pattern analyses, we observed reverse spatiotemporal hierarchy during the visual memory recall and imagery: the neural activity in primary visual cortex was delayed compared with high-order visual areas. A similar trend was found during auditory memory recall and imagery, where the activity of the high-level superior temporal gyrus was earlier than the mid-level planum temporale. However, the primary auditory area was not involved, suggesting modality differences in the role of primary sensory areas in corresponding memory recall. We also observed the activity of the hippocampus, the parahippocampal cortex, the retrosplenial cortex, and the precuneus. We found their associated temporal dynamics were consistent across sensory modalities during memory recall, which were distinct from that during memory encoding. Overall, our study provided both spatial and temporal accounts of neural activity during the cross-modal memory recall and imagery."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Resolving the tension between exemplar and structure learning through a mixture-of-experts model of hippocampal-PFC interactions",
    "presenter": "Dhairyya Singh",
    "poster_id": "B154",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "The hippocampus has been strongly implicated in both taking snapshots of individual experiences and extracting common structure across these experiences—functions often in tension. Prior evidence suggests an anatomical division of labor: the trisynaptic pathway (TSP) employs pattern-separated representations that store episodes while the monosynaptic pathway (MSP) uses overlapping representations to support statistical learning. A fundamental mystery remains, however: how does the brain recruit the right representation at the right time? Prefrontal cortex (PFC) has been proposed to exert control over hippocampal outputs. Here, we introduce a stimulus-computable mixture-of-experts system featuring MSP- and TSP-like neural network experts, along with a PFC-inspired gating network that controls their outputs. The system performs exemplar recognition and categorization simultaneously, and learns to adaptively combine expert outputs. We found that joint training of the experts and the gating network is necessary and simple mixing is insufficient. This framework illustrates how PFC control may harness hippocampal specialization to resolve opposing computational objectives."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Exploring the Neural and Phenomenological Landscapes of Self-Incongruent Autobiographical Memories",
    "presenter": "Alicja Wicher",
    "poster_id": "B157",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Autobiographical memory recall depends on integrating personal experiences with the self-model, ensuring a coherent identity. Yet self-incongruent memories—particularly those involving shame—disrupt this integration and alter phenomenological characteristics. Building on previous diary study findings linking self-incongruence to changes in recall perspective, our ongoing work develops a novel fMRI paradigm to investigate neural correlates of self-memory integration. In this study, participants recall autobiographical episodes eliciting pride, shame, contentment, or fear, with each memory cued by personalized words. Memory recall is accompanied by free verbal narration and simultaneous recording of physiological measures, including cheek temperature, heart rate, and galvanic skin responses, along with self-report ratings on valence, arousal, and memory experience. Representational Similarity Analysis examines distinct neural signatures across emotional states, while semantic analyses illuminate differences in narrative content. This multimodal, ongoing approach elucidates how self-conscious emotions modulate memory integration across neural, experiential, and semantic dimensions."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Dynamics in the primary sensory, medial temporal and striatal regions during implicit learning of temporal regularity",
    "presenter": "Wei Tang",
    "poster_id": "B156",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Temporal regularities in sensory inputs can sharpen our perception of the input. How the brain develops sensitivity for temporally structured stimuli through statistical learning (SL) remains unknown. In this study, we investigate brain activities in the early visual, medial temporal and striatal regions during repeated exposure to a temporal structure embedded in sequential visual stimuli. In the late stage of learning, we observed increased sensitivity to stimulus category in V1, and such sensitivity was mirrored by activity in the entorhinal cortex. The entorhinal activity also became correlated with that of the nucleus accumbens towards the end of the task. These results reveal complementary roles of the above regions in the hierarchical processing for learning temporal regularity."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Human brain networks encode creativity state and level",
    "presenter": "Rikki Rabinovich",
    "poster_id": "B159",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Creativity is an essential human cognitive process with an unclear neural basis. Here, we combine invasive intracranial recordings in the human brain with machine learning tools to explore the network-level representations underlying creative thinking. Our investigations unearthed widespread encoding of cognitive state, revealing unique brain states that underlie creative thinking vs. mathematical reasoning. Further, we identified nonlinear, high-dimensional representations of moment-by-moment creativity level. Finally, our findings hint at distinct roles of different cortical networks in promoting creativity, with default mode network “gating” creativity and dorsal attention network “regulating” the quality of creative output."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Biophysical Deep Learning: Laminar-resolved Cortical Columns as Neural Network Units",
    "presenter": "Dasja de Leeuw",
    "poster_id": "B158",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Cognitive computational neuroscience strives to develop models that achieve both biological and cognitive fidelity. Propelled by the success of deep neural networks (DNNs) in emulating human functional capacities and neural representations, the field increasingly utilizes DNNs for generating, testing, and refining theories of the neurocomputational processes. However, these neuroconnectionist models often lack neuroanatomical detail and neuronal population dynamics, factors that provide important constraints on the neurocomputational solution space. To address this, we introduce a biophysics-informed neuroconnectionist modeling approach with powerful learning capabilities. Our approach constructs neural networks from laminar-resolved cortical columns with neuroanatomically-realistic internal connectivity. In a proof of concept, we show that these networks can be succesfully trained to reproduce firing rates of neuronal populations in a perceptual decision-making task and achieve high accuracy in classification tasks. This work demonstrates the feasibility of embedding function in biophysics-informed models and introduces a new class of neuroconnectionist models striking a meaningful balance between biological realism and cognitive function."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Improving Prediction of Cognitive Abilities through Integrated Resting-State and Task fMRI",
    "presenter": "Yujing Jiang",
    "poster_id": "B161",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Using resting-state fMRI (rs-fMRI) and task-fMRI are two common approaches of predicting cognitive abilities, yet conventional methods offer limited accuracy. We propose a transformer-based framework that unifies rs-fMRI and t-fMRI, drawing inspiration from Large Language Models (LLMs) known for integrating diverse sequential inputs. Instead of treating these modalities separately, our approach encodes both as continuous temporal sequences and applies self-attention to learn Dynamic Activity Signatures that capture neural processes common to spontaneous and task-evoked activity. This unified latent space produces consistent predictions across rs-fMRI and t-fMRI, even in the absence of task data, while eliminating the need for multiple models. Evaluated on the Human Connectome Project (HCP) and Alzheimer’s Disease Neuroimaging Initiative (ADNI) datasets, our framework demonstrated high predictive accuracy and robust generalization, proving the effectiveness of a unified model that seamlessly integrates rs-fMRI and t-fMRI."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Systematic differences between brain organisation revealed by task and rest",
    "presenter": "Caroline Nettekoven",
    "poster_id": "B160",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Capturing brain organization using functional Magnetic Resonance Imaging (fMRI) is a fundamental goal in neuroscience. Brain parcellation, gradient, or connectivity models all rely on the voxel by voxel correlation matrices. However, it is unclear which behavioral paradigms and analysis approaches result in a correlation matrix estimate that would be most predictive of regional co-activation patterns across a variety of cognitive states.\nThis study systematically compares resting-state and task-based fMRI data along with commonly used filtering and analysis methods. Using task-based and resting-state fMRI data from 17 subjects, we show that task-based data, particularly when subjected to a GLM, is biased by the particular task contrast being used. However, when including an increasing number of diverse tasks, this bias disappears. We show that such multi-task approach results in better prediction for novel tasks than resting-state data, while the latter is easier to acquire and more reliable. We conclude that task-based approaches, especially with diverse task sets, are better suited for understanding brain function across many behavioral states."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Long delays reduce the need for precise weights in spiking neural networks",
    "presenter": "Danyal Akarca",
    "poster_id": "B162",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Recent work has shown that the performance of spiking neural networks (SNNs) on temporally complex tasks improves significantly when axonal delays are treated as learnable parameters. This raises an important question: If temporal delays improve a network's computational capacity, how precise do synaptic weights need to be? In this work, we investigate the relationship between delay-based computation and weight precision by combining quantized synaptic weights with a range of learnable delays on a challenging neuromorphic audio task. Our results reveal that short delays contribute little to performance, whereas medium to long delays are critical. Building on this insight, we introduce a learnable thresholding mechanism to suppress short delays that can be effectively compensated for by weights. These findings suggest that delays can reduce the burden on weight precision, highlighting a promising direction for energy-efficient SNN design and offering new perspectives on the role of delay in biological and neuromorphic computation."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "High-level information integration in the brain via large-scale attractor dynamics",
    "presenter": "Tamas Spisak",
    "poster_id": "B163",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Understanding how high-level information integration arises from large-scale brain activity requires bridging computational principles with neural dynamics. We propose a theoretical framework where large-scale brain dynamics emerge as trajectories around attractors in course-grained recurrent networks whose dynamics precisely map to computations.\nThe core network model in our framework combines principles of self-organization with attractor network theory and Bayesian inference, offering a recursive, multi-level description, applicable to large-scale empirical data. A key feature of these networks is the emergent orthogonality of the attractors, which maximizes storage capacity and computational efficiency. Crucially, this orthogonality allows mapping complex attractor dynamics onto simpler, interpretable bipartite architectures, revealing how a wide variety of computations can be implemented implicitly by network-wide stochastic attractor dynamics. We propose this framework as a model for large-scale brain dynamics. Our approach aligns with previous literature and is supported by emerging evidence, such as observations of orthogonal brain attractors, akin to canonical resting state networks. The framework yields testable predictions and offers a principled yet simple approach to understanding, explaining, and predicting large-scale brain dynamics and corresponding behavior."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "The Causal Structure of Band-limited Dynamics in the Human Cortex",
    "presenter": "Paul Hege",
    "poster_id": "B164",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Measurements of the synchronous variation of neural activity in different areas of the brain provide fundamental insights into large-scale human brain function. However, research has so far largely focused on instantaneous correlations, providing little information about the causal structure of neural interactions. Here, we addressed this by systematically characterizing temporally directed interactions of band-limited neural activity across the human cortex using source-reconstructed magnetoencephalography (MEG). We found links between the structure of lagged correlations and models of large-scale brain organization, with cross-frequency lagged interactions varying along a tripolar gradient that closely resembles structural and functional MRI gradients. We also identified discrete brain networks with spatially and spectrally specific interaction profiles that were selectively modified during cognitive tasks."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Balanced Gamma Oscillations Support Ultra-Slow Dynamics in the Cortex at Rest",
    "presenter": "Francisco Páscoa dos Santos",
    "poster_id": "B165",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "While it has been suggested that gamma ($\\sim$40Hz) oscillations are an important driver of the spontaneous dynamics of the large-scale cortical networks, whole-brain models often neglect fast excitatory AMPA synapses, responsible for the generation of gamma rhythms through reciprocal excitatory-inhibitory (E-I) interactions. Importantly, these interactions are balanced through homeostatic plasticity mechanisms, ensuring stable activity. However, the joint role of gamma oscillations and E-I balance in supporting large-scale cortical dynamics has not been tested systematically. Therefore, we built a large-scale model of the human cortex with E-I homeostasis and fast and slow excitation through AMPA and NMDA receptors, respectively. By selectively knocking out fast excitation and E-I homeostasis, we demonstrate that models with both features better reproduce the resting-state dynamics of the human cortex, measured through ultra-slow blood-oxygenation-level–dependent (BOLD) signals. While EI homeostasis ensures the emergence of empirical connectivity networks, their dynamic aspect is best captured in models with gamma oscillations generated by AMPA-mediated excitation. Therefore, our results help elucidate the emergence of collective dynamics in the cortex, advancing balanced gamma oscillations as a fundamental generative mechanism behind ultra-slow fluctuations of cortical activity."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Toward Generating Plausible Artificial Electroencephalography Data: Evaluating the Effects of Convolution-based Upsampling Methods on Data Quality",
    "presenter": "Clemens Kaiser",
    "poster_id": "B166",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Electroencephalography is key for clinical and cognitive research, yet limited data availability restricts deep learning (DL) applications. This study compares thee upsampling techniques in convolution-based Generative Adversarial Networks {--} transposed convolutions (TC), interpolation with convolutions (IC), and a mixed approach {--} that are used to transform noise vectors into artificial EEG data. We evaluate artificial signal quality with EEG-specific metrics across time, frequency, and spatial domains. Kolmogorov–Smirnov tests indicate that the mixed approach mitigates the high-frequency noise commonly introduced by TC, while better preserving lower-frequency components and inter-channel dependencies than IC. Moreover, the findings underline the importance of EEG-specific evaluation metrics \nfor guiding the development of more explainable and efficacious generative models, advancing DL applications in neuroscience."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Tracking time by simulating the sensory world",
    "presenter": "Aaron Kaltenmaier",
    "poster_id": "B167",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Theories of oscillatory tracking propose that neural oscillations model external temporal structure by phase-coupling to environmental rhythms. However, methodological challenges make evidence for these proposal sparse, particularly in the visual domain which has predominantly focused on how theta and alpha oscillations parse perception independently of external temporal structure. Using a new empirical approach we aimed to address this question. Participants attended rhythmic visual displays and we used rate-specific phase coherence MEG measures as well as multivariate decoding to investigate the cortical tracking of this temporal structure. We show rate-specific phase-coupling of motor regions to the tracked rhythm, specifically when timing is task-relevant. Crucially, this explicit tracking of temporal structure relies on temporally precise sensory predictions in visual areas that interestingly appear regardless of task instruction. We propose a mechanism by which automatic sensory simulation yields an information envelope that is read out by motor areas when required to derive temporal estimates. Under this view, temporal predictions are simply derivatives, and sensory predictions are their necessary bedrock."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Peripheral Beta-Blockade Differentially Enhances Cardiac and Respiratory Interoception",
    "presenter": "Ashley Tyrer",
    "poster_id": "B168",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Interoception, the perception of internal visceral states, arises from complex brain-body interactions across the central and peripheral nervous systems. Despite noradrenaline’s key role in these interactions, its specific contribution to interoceptive processes remains unclear. In a placebo-controlled, randomised, within-subject study (N = 50), we employed computational modelling of interoceptive psychophysics to determine how pharmacological beta-adrenoceptor antagonism controls interoception across cardiac and respiratory domains. Both cardio-selective bisoprolol and non-selective propranolol improved cardiac perceptual sensitivity, with bisoprolol exerting an enhanced effect on cardiac metacognition. In contrast, both beta-blockers increased respiratory perceptual precision, with no corresponding changes in sensitivity or metacognition. These findings reveal a novel dissociation between central and peripheral beta-adrenergic mechanisms in interoception, highlighting the pivotal role of peripheral noradrenaline in regulating multi-organ brain-body interactions. Our results suggest that beta-blockers may provide promising routes for modulating distinct facets of interoception, potentially opening new avenues for intervention in conditions characterised by disrupted bodily self-awareness."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Connectivity Effects of cTBS to the DLPFC:  Examining beta phase synchronisation between the left and right DLPFC",
    "presenter": "Veronica Forslund",
    "poster_id": "B170",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Transcranial magnetic stimulation (TMS) can induce lasting change in neural activity at both the stimulation site and brain-wide circuits.  However, the efficacy of repetitive protocols in inducing brain-wide neuromodulation can vary. This pilot study used EEG to explore connectivity effects in the form of phase synchronisation between the left and right DLPFC after administration of continuous theta burst stimulation (cTBS) at the left Dorsal Lateral Prefrontal Cortex (DLPFC). To do this we used single-pulse TMS to probe connectivity effects before and after cTBS or sham cTBS. Unlike other forms of repetitive TMS (rTMS) and intermittent theta burst stimulation (iTBS), we found no change in connectivity following cTBS, highlighting the variability of neural outcomes that can follow apparently similar intervention protocols. \n\nKeywords: Functional connectivity; DLPFC; Neurostimulation; TMS; TBS; EEG; Phase synchronization"
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Cortically-Embedded RNNs for integration of cortex-wide neuroscience data into recurrent neural network models",
    "presenter": "Aswathi Thrivikraman",
    "poster_id": "B169",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Current state-of-the-art recurrent neural network models can capture complex neural dynamics during the performance of higher cognitive tasks. However, they largely overlook anatomy, limiting their ability to make species-specific and anatomically-precise predictions for experimentalists. Cortex-wide dynamical models increasingly integrate anatomical features including connectivity, dendritic spines and receptors, but are incapable of solving most cognitive tasks. Here, we introduce Cortically-Embedded Recurrent Neural Networks (CERNNs), which embed artificial neural networks into a species-specific cortical space, facilitating direct comparisons to empirical neuroscience data across the entire cortex and allowing the incorporation of biologically-inspired constraints. We trained CERNNs, with macaque or human anatomy, to perform multiple cognitive tasks (e.g. working memory, response inhibition). CERNNs were trained with different architectural constraints and biologically-inspired loss functions. We evaluated CERNNs on (1) task performance, (2) alignment of connectivity with the macaque mesoscopic connectome, and (3) task-evoked activity patterns. The best performing models penalized both long-distance connections and deviations from empirical spine density. These results suggest that distributed cognitive networks may arise naturally as the brain attempts to solve complex tasks under wiring constraints with systematic gradients of single neuron properties. More broadly, CERNNs constitute a framework by which artificial neural networks can be integrated with cortex-wide neuroanatomy, physiology and imaging data to  produce anatomically-specific testable hypotheses across species."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Can We Identify the What, When and Where of Sensorimotor Activity in EEG Recordings?",
    "presenter": "Marije ter Wal",
    "poster_id": "B171",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "The origin of sensorimotor EEG signatures remains to be elucidated. Here, we present a model that spans the gap from neuronal spiking to the EEG and is based on known cortical anatomy and physiology. We use this model to simulate EEG signals, in order to enhance the understanding of human EEG recordings during rest and motor tasks."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Temporal Prediction in Non-Deterministic Continuous Environments: Investigating the Role of Oscillatory Entrainment and Interval Learning",
    "presenter": "Elmira Hosseini",
    "poster_id": "B172",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Interaction with our continuously changing environment relies on anticipating timing of events, enhancing information processing efficiency. Abundant research has investigated temporal prediction in deterministic environments such as isochronous rhythms, where the presumed mechanism is Oscillatory Entrainment (OE) to external rhythms. However, in everyday life, continuous streams lack fully-deterministic temporal regularities. Previous research of temporal prediction in uncertain environments has focused on isolated intervals, suggesting a Distributional-Learning (DL) model. However, in non-deterministic streams, if and under which conditions either of these mechanisms drives prediction is unclear. To address this, we combined computational modeling of the two mechanisms (OE and DL) and human behavioral experiments. We found that while models are affected differently by the degree of variability in the environment, they lead to more overlapping predictions in lower degrees of variability. Next, we used the models generatively to create streams with differential temporal predictions by these two mechanisms, and presented targets at either predicted timepoint to participants conducting a speeded response task. Participants’ behavior followed OE predictions in environments with relatively lower degrees of variability to which they were sequentially exposed. Overall, these results highlight the inherent differences between OE and DL mechanisms in dealing with uncertainty, and reveal the flexibility of OE in adapting to partial irregularities, and its independence from DL."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "TeDFA-$\\delta$: Temporal integration in deep spiking networks trained with feedback alignment improves policy learning",
    "presenter": "Jorin Overwiening",
    "poster_id": "B174",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Limitations in deep spiking reinforcement learning models hinder our understanding of how biological systems learn control policies. We address this by developing a biologically plausible deep reinforcement learning agent (TeDFA-$\\delta$) that combines spiking neurons with local Tempotron learning and global Direct Feedback Alignment and Temporal Difference error optimization. Despite using a suboptimal learning rule, TeDFA-$\\delta$ outperforms backpropagation-trained MLPs on cartpole, acrobot, and dynamic bandit tasks. This improvement stems from temporal integration of states in spiking neurons rather than the learning algorithm itself, based on ablation studies. The network develops structured spatiotemporal representations where policy and value information coexist, with optimal performance at intermediate membrane time constants ($\\tau \\ll T$). Our results demonstrate that biological systems may compensate for imperfect credit assignment through temporal dynamics, suggesting neural representations outweigh learning rule optimality for control tasks. This framework enables new studies of biological learning while advancing neuromorphic computing."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Learning neuronal manifolds for interacting neuronal populations",
    "presenter": "Akshey Kumar",
    "poster_id": "B173",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Understanding how neuronal populations interact to process information and generate behavior is a central goal of neuroscience. However, high dimensionality, dense interactions, and unobserved factors complicate this task. The neuronal manifold hypothesis suggests that relevant dynamics occur on a lower-dimensional manifold, but it offers limited insight into the interactions among subsystems. We introduce a BunDLe-Net-based architecture that embeds distinct neuronal populations into separate latent dimensions. By leveraging BunDLe-Net’s Markovian embedding, we ensure that every point in the latent space retains predictive information about future behavioral dynamics. We apply our method to C. elegans neuronal data categorized into sensory, motor, and interneurons. The manifold not only reveals recurring motifs in the dynamics but also shows how different populations orchestrate these motifs. From the manifold, we can read off which populations encode information and drive the dynamics in each behavioral state. Thus, we present a powerful visual tool that reveals how information is processed and relayed across populations."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Artificial Neural Networks trained on human cognitive data as network models of cognition in health and mental disorders",
    "presenter": "Oliver Frank",
    "poster_id": "B175",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Cognitive functions are mental processes essential for goal-directed behavior. Impairments in these functions are common in psychiatric disorders and significantly impact quality of life. \n\nArtificial Neural Networks (ANNs), trained on cognitive test data from human individuals, offer a new model-based approach to study potential causal links between brain network structure, cognitive function and brain architecture. In this study, we collected longitudinal cognitive data from healthy individuals and patients (schizophrenia, depression, autism spectrum disorder) to train individualized ANNs and analyse their emerging network properties. \n\nOur results show that ANNs can learn participants’ behavior and, when initialized with suitable architectures, exhibit a balance of integration and segregation in their hidden layers, mirroring the brain’s topological organization. Network topologies remain mostly robust across randomized training iterations, and topological marker distributions differ significantly (5 out of 6 comparisons (t-test), p \u003c .05). Our findings suggest that ANNs trained on cognitive-behavioral data may serve as tools to understand (brain) network properties underlying human cognitive function in health and mental disorder."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Not all solutions are created equal: An analytical dissociation of functional and representational similarity in deep linear neural networks",
    "presenter": "Erin Grant",
    "poster_id": "B176",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "A foundational principle of connectionism is that perception, action, and cognition emerge from parallel computations among simple, interconnected units that generate and rely on neural representations. Accordingly, researchers employ multivariate pattern analysis to decode and compare the neural codes of artificial and biological networks, aiming to uncover their functions. However, there is limited analytical understanding of how a network’s representation and function relate, despite this being essential to any quantitative notion of underlying function or functional similarity. We address this question using fully analysable two-layer linear networks and numerical simulations in nonlinear networks. We find that function and representation are dissociated, allowing representational similarity without functional similarity and vice versa. Further, we show that neither robustness to input noise nor the level of generalisation error constrain representations to the task. In contrast, networks robust to parameter noise have limited representational flexibility and must employ task-specific representations. Our findings suggest that representational alignment reflects computational advantages beyond functional alignment alone, with significant implications for interpreting and comparing the representations of connectionist systems."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Fast fMRI signals up to 1Hz vary across brain states  and predict spontaneous neural activity",
    "presenter": "Leandro P. L. Jacob",
    "poster_id": "B177",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "fMRI signals were traditionally seen as very slow, with most resting-state studies only investigating signals slower than 0.1Hz, but task-based studies have shown that fMRI signals up to 0.75Hz reflect stimulus-induced neural activity. Here, we investigate whether high-frequency fMRI signals can index spontaneous neural activity across brain states. Using simultaneous EEG-fMRI in 21 humans drifting between sleep and wakefulness, we found an increase in fMRI spectral power during NREM sleep (compared to wakefulness) across frequency ranges as fast as 1Hz. Using machine learning, we found that these fast fMRI signals predict fluctuations in canonical neural rhythms measured with EEG, in subjects held-out from the training set. Since fMRI signals and neural rhythms are sensitive to systemic physiology, we tested whether this predictive fast fMRI information specifically represented neurovascular coupling, or was also present in the ventricles. We found that fMRI signals as fast as 0.9Hz (for alpha rhythm predictions) and 0.8Hz (for delta rhythm predictions) contained unique neural information above what was present in the ventricles. These results reveal that high-frequency spontaneous fMRI signals are coupled to neural activity that varies across brain states and index cognitive processes, pushing the boundaries of fMRI’s abilities to reveal brain dynamics underlying cognition."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Error Forcing in Recurrent Neural Networks",
    "presenter": "A Erdem Sağtekin",
    "poster_id": "B178",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "How can feedback improve learning outcomes? Traditionally, feedback signals are thought to directly drive parameter (synaptic) changes. Yet, biophysically, the same signals also affect the activity of neurons. Here, we use this observation to develop a new algorithm termed error forcing (EF) for learning in recurrent neural networks, where feedback influences both synaptic plasticity and the network state. We geometrically contrast our approach with the established teacher forcing framework, and further provide an interpretation of its function from a Bayesian standpoint. EF learning outperforms traditional approaches in scenarios with temporally sparse feedback when the output is weakly constrained by the task. These benefits generalize across tasks and are maintained in a biologically-constrained approximation of error forcing."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Inferring Virtual Height-Induced Changes in Postural Control via Inverse Optimal Control",
    "presenter": "Tahmineh A. Koosha",
    "poster_id": "B180",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Humans adapt their postural control strategies in response to fear, but traditional sway metrics cannot directly reveal the underlying control objectives. We outline a preprocessing pipeline to enable inference of latent cost functions through our ongoing inverse optimal control (IOC) analysis. We exposed participants to ground (GC) and height (HC) conditions in virtual reality, while recording joint kinematics using Kinect-based motion capture. After aligning and denoising the data, we extracted joint angles (hip, knee, ankle) and computed summary metrics such as Mean, RMS, and Mean Power Frequency (MPF). Using Bayesian estimation, we found condition-dependent shifts in joint angle distributions, including reduced hip flexion and increased ankle stability under height. Our findings provide evidence of postural adaptation under perceived threat and lay the groundwork for the modeling of control strategies that govern balance in fear-inducing environments."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Mouse-tracking Reveals Individual Differences in the Dynamics of Belief Updating Under Volatility",
    "presenter": "Aurora Katharina Delz",
    "poster_id": "B179",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Decision-making under uncertainty can require dynamic updating of beliefs about the state of the world over time. While previous work has often used two-alternative forced choice tasks to investigate this process, here we introduce a novel mouse-tracking paradigm that tracks belief updating in real time . Participants (N=30) adapted their belief updating across environments with low and high levels of volatility in keeping with a normative model employing a non-linear form of evidence accumulation, and exhibited slow-timescale belief updating dynamics that substantially lagged those observed on a simple sensory-motor task with matched motor requirements. Interrogation of single-subject belief dynamics also revealed marked individual differences: while some participants produced a highly-resolved range of reported beliefs consistent with the normative model, others exhibited strong clustering of beliefs suggestive of a more limited set of categorical commitment states. These findings highlight the sensitivity of mouse-tracking to otherwise hidden individual differences in belief updating, showcasing a novel tool for dissecting computational and neural mechanisms of this key cognitive function"
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Similarity-based Representation Factorization for Understanding Representations in Minds, Brains and Machines",
    "presenter": "Florian P. Mahner",
    "poster_id": "B181",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Understanding representations is a major aim in cognitive computational neuroscience, yet existing data-driven methods are limited in providing interpretable dimensions that also capture the underlying data structure. Here we propose Similarity-based Representation Factorization (SRF), a method that reliably decomposes data structures into interpretable, non-negative components based on similarity matrices. Through simulations and empirical data, we demonstrate that SRF is robust to noise and capable of revealing interpretable dimensions in both synthetic and behavioral similarity data. SRF opens new possibilities for uncovering the dimensions that underlie similarity even in smaller and noisier datasets, thus offering a principled approach for interpreting representational structure."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "How to test Bayesian models using neural data",
    "presenter": "Ralf M Haefner",
    "poster_id": "B182",
    "topic_area": "Methods and Computational Tools",
    "abstract": "The Bayesian Brain hypothesis suggests that the brain can be understood in terms of Bayesian computations. While many studies have provided perceptual and sensorimotor evidence for this hypothesis, the question of whether neural responses can also be understood in Bayesian terms remains open. Answering this question requires the specification of two main unknowns: (1) what is the generative model that relates the variables inferred by some population of neurons to the sensory observations, and (2) what is the 'neural code', i.e. what is the relationship between posterior beliefs and neural responses? Much attention has been directed at answering the second of these questions while ignoring the first question, however without reaching consensus. At least in part this is because a given set of observed neural responses can imply different codes under different assumptions about the generative model. Here, we propose answering both questions in the opposite order. First, we present a method to test a given generative model using metamers -- stimuli that give rise to the same posterior under this model -- and confirming that they elicit the same neural responses. This approach can be interpreted as a special case of representational similarity analysis, and generalized accordingly. Second, we propose a `mixture method' that tests whether the relationship between posteriors for different stimuli matches the relationship for the measured neural responses to the same stimuli. If applied to the full response distribution, model and data are only expected to match for neural sampling codes. If applied to average neural responses, they are expected to match for any linear distributional code, including neural sampling and distributed distributional codes, but not probabilistic population codes. We illustrate our approach using simulations where the ground truth is known -- both for a sparse coding model of V1, and a hierarchical motion model for area MT."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Catalyzing in silico neuroscience with a toolkit of accurate encoding models of the brain",
    "presenter": "Domenic Bersch",
    "poster_id": "B183",
    "topic_area": "Methods and Computational Tools",
    "abstract": "_In silico_ neural responses generated from encoding models increasingly resemble _in vivo_ responses recorded from real brains, enabling the novel research paradigm of in silico neuroscience. In silico neuroscience scales beyond what is possible with in vivo data, allowing to explore and test scientific hypotheses across vastly larger solution spaces. To catalyze this emerging research paradigm, here we introduce the Brain Encoding Response Generator (BERG), a resource consisting of multiple pre-trained encoding models of the brain and a Python package to generate accurate in silico neural responses to massive amounts of arbitrary stimuli with a few lines of simple code (https://github.com/gifale95/BERG). We show that BERG’s encoding models accurately predict neural responses to visual stimuli, and that these in silico responses reproduce key neural signatures of visual processing. This opens the doors to using in silico neural responses for scientific discovery, which we envision will lead to a more efficient and reproducible science."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "A BOLD Sampling Scheme to Improve the Estimation of Voxel-wise Encoding Model Parameters",
    "presenter": "Jochem W. Rieger",
    "poster_id": "B185",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Encoding models are a widely used data driven technique to derive functional models like the sensitivity profiles of voxels to sets of stimulus features. They are typically estimated using linear  regression techniques to find a model  that uses sets stimulus features as input to predict fMRI BOLD activity of a voxel as output.  The regression weights represent the functional model (e.g. a voxel’s receptive field). Sampling rate differences between stimulus and BOLD which can render the estimated model uninterpretable. This is typically counteracted by temporally down sampling the stimulus, which is undesirable as it causes information loss. Here we use simulations to first demonstrate that higher stimulus than fMRI sampling rates combined with regular BOLD sampling make the estimation of encoding models noisy and hard to interpret. We then demonstrate that a novel re-sampling based technique that samples the BOLD response at irregular temporal intervals alleviates these problems and allows to use the stimulus feature space with full temporal resolution for encoding model estimation."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Quantifying Psychedelic Visual Phenomena: An AI-Driven Computational Framework for Analyzing Form Constants and Representational Competencies",
    "presenter": "Cynthia-Maria Kanaan",
    "poster_id": "B186",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Psychedelic experiences are widely reported to induce vivid visual phenomena characterized by recurring geometric patterns, known as form constants (Bressloff et al., 2002). These hallucinations have been shown to provide therapeutic benefits (Aynsworth et al., 2017; Gattuso et al., 2022; Leptourgos et al., 2020) and offer insights into the neural mechanisms of altered perception, through their ability to disrupt normal states of consciousness (Carhart-Harris et al., 2016; Greco et al., 2025; Suzuki et al., 2017). However, the processes underlying these effects are not yet fully understood as existing approaches lack a standardized, quantitative framework to characterize and study them (Castelhano et al., 2021; Gattuso et al., 2022). This study addresses these limitations by introducing a proof-of-principle methodology that leverages generative artificial intelligence (AI) to extract, quantify, and classify perceptual features commonly associated with experiences of visual hallucinations.\nA generative pre-trained transformer was developed to generate image-to-text descriptions focusing on underlying form constants and their mathematical properties, while avoiding surface-level content. The custom model was employed to analyse 40 psychedelic artworks inspired by subjective experiences (i.e., containing hallucinatory patterns), and 40 non-psychedelic images collected from online sources. Texual outputs were converted into semantic embeddings using a transformer-based encoder in the R language. Hierarchical clustering and cosine similarity analyses revealed strong within-group and low between-group similarity for both categories (Figure 1). Principal component analysis (Figure 2) further confirmed the AI’s ability to capture distinctive themes characteristic of psychedelic imagery, demonstrating how the proposed computational method can systematically analyse subjective visual phenomena.\nThis contributes to bridging the gap between subjective psychedelic experiences and objective quantitative analysis, offering a novel tool for understanding neurological and perceptual mechanisms underlying altered states of consciousness. It lays the groundwork for future research into how artificial systems may simulate and process high-dimensional sensory data, advancing cross-system representational competencies."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "An Equivalence Between Representational Similarity Analysis and Centered Kernel Alignment",
    "presenter": "Alex H Williams",
    "poster_id": "B184",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Centered kernel alignment (CKA) and representational similarity analysis (RSA) of dissimilarity matrices are two popular methods for comparing neural systems in terms of representational geometry. Although they follow a conceptually similar approach, typical implementations of CKA and RSA tend to result in numerically different outcomes. Here, we show that these approaches become equivalent after incorporating a mean-centering step into RSA. This equivalence holds for both linear and nonlinear variants of these methods. By unifying these measures, this paper hopes to simplify a complex and fragmented literature on this subject."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Toward Real-World Emotion Decoding: A Transformer-Based Approach Using Movie fMRI",
    "presenter": "Bo-Gyeom Kim",
    "poster_id": "B187",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Real-world emotion recognition arises through continuous interactions among multiple sensory cues—dynamics often missed by standard laboratory paradigms. To investigate these dynamics, we applied a Transformer-based deep-learning model (SwiFT combined with Perceiver IO) to functional MRI data from 512 youths (ages 5–21) watching a 10-minute movie. By modeling neural signals as continuous time-series, we tracked short-term (~40s, 50 TRs) changes in seven emotions (e.g., positive, fear). Longer sequence windows and explicit hemodynamic modeling (double-gamma HRF) improved decoding accuracy, highlighting the importance of extended temporal context and precise BOLD-delay modeling. The prominent contribution of the visual cortex suggests reliance on low-level visual features within rich audiovisual stimuli. These findings demonstrate that flexible sequence-to-sequence methods effectively capture the temporal dynamics of emotion recognition under realistic conditions, deepening our understanding of real-world emotional processing."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Predicting Stroke Recovery with Nonlinear Low-Dimensional Embeddings of Behavioral Profiles",
    "presenter": "Jax Skye",
    "poster_id": "B188",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Stroke is a leading cause of disability worldwide. Predicting functional outcomes is challenging due to heterogeneity in post-stroke deficits and recovery profiles. We assessed prediction accuracies of 101 chronic outcomes from 78 acute behavioral measures and (hypothesizing redundancy in the predictors) from low-dimensional embeddings thereof. Nonlinear 2D UMAP embeddings yielded predictions comparable to those from all predictors. We identified brain damage patterns associated with specific behavioral profiles (extrema of the patient distribution in UMAP embeddings). We show that predictions based on only four acute tests—chosen as best linear approximations to UMAP embeddings—matched prediction accuracies from all 78 tests, suggesting nonlinear dimensionality reduction offers novel and interpretable tools for understanding behavioral outcomes of brain lesions and clinical assessment."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Multiencoder VAE for cross-subject alignment of brain responses",
    "presenter": "Angeliki Papathanasiou",
    "poster_id": "B189",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Neural responses to identical stimuli vary considerably across individuals despite similar behavioral outcomes. Recent research demonstrates preserved latent neural dynamics in motor cortical populations across monkeys performing identical motor tasks. Inspired by these observations we introduce a multiencoder variational autoencoder (VAE) to model visual cortex responses. Our approach transforms subject-specific fMRI responses from natural scene viewing into a common latent space while predicting artificial neural network (ANN) activations elicited by identical stimuli. Using the Natural Scenes Dataset (NSD), our method outperforms traditional alignment techniques by capturing cross-subject representational similarities. The VAE architecture implements subject-specific encoders which project occipitotemporal cortex responses into a shared latent manifold that preserves semantic organization while accommodating neuroanatomical variability. Simultaneously, the decoder establishes a computational correspondence between this latent representation and ResNet-50 activations. This approach creates a framework for investigating shared neural representations across individuals while quantifying systematic relationships between biological and artificial NNs."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Functional Templates in fMRI: Building Accurate and Interpretable Group-Level Decoders",
    "presenter": "Pierre-Louis Barbarant",
    "poster_id": "B191",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Anatomical and functional inter-individual variability poses a significant challenge for group analyzes in neuroimaging studies. While anatomical templates help mitigate morphological differences by coregistering subjects in fMRI, they fail to account for functional variability, often leading to blurred activation patterns on the template due to group-level averaging. To address this, hyperalignment identifies fine-grained correspondences between functional brain maps of different subjects, with Procrustes analysis and optimal transport being among the most effective approaches. However many hyperalignment-based imaging studies rely on selecting a single target subject as the reference for aligning all other subjects’ data. We argue that a functional template obviates the need for this arbitrary selection, effectively encapsulating population similarities while preserving anatomical coherence. Using the Individual Brain Charting dataset, we assess the benefit of hyperalignment in template estimation, and the classification accuracy on template aligned-data. Our results show that (a) functional templates produce more localized activation clusters than traditional anatomical averaging, improving the interpretability of population-level studies, and (b) they preserve or enhance the semantic content of brain activations, leading to comparable or higher classification accuracies across most tasks compared to anatomical or pairwise functional alignment."
  },
  {
    "start": "2025-08-13T01:00:00Z",
    "end": "2025-08-13T16:00:00Z",
    "title": "Information Transfer in the Brain Is Synergistic",
    "presenter": "Sebastian Dohnány",
    "poster_id": "B190",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Complex behaviour relies on the coordination of distributed neural processes, enabled by information transfer between brain regions. Despite progress in time-resolved and directed connectivity analysis, how information actually flows in the brain remains unclear. Here, we use a novel decomposition of information transfer based on Partial Information Decomposition (PID) to analyse spontaneous BOLD fMRI dynamics. We find that transfer is dominated by temporally and informationally integrated, synergistic interactions. These findings offer a fine-grained and interpretable approach to brain dynamics, opening new potential links to cognition."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Probabilistic representations fail to emerge in task-optimized neural networks",
    "presenter": "Ishan Kalburge",
    "poster_id": "C5",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "While mounting evidence indicates that human decision-making follows Bayesian principles, the underlying neural computations remain unknown. Recent work proposes that probabilistic representations arise naturally in neural networks trained with non-probabilistic objectives \\cite{OrhanMa}. However, prior analyses did not explicitly examine whether the neural code merely re-represents inputs or performs useful transformations that prioritize three criteria for a probabilistic representation: generalization, invariance, and representational simplicity \\cite{WalkerEtAlReview, PohlEtAl}. Using a novel probing-based approach, we show that training feed-forward networks to perform cue combination and coordinate transformation without probabilistic objectives leads to  Bayesian posteriors being decodable from their hidden layer activities. However, we also show that these networks fail the generalization, invariance and representational simplicity criteria: they do not generalize out-of-sample, compress their inputs, or develop easily decodable representations. Therefore, it remains an open question under what conditions truly probabilistic representations emerge in neural networks."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Revisiting Cost Functions in Sensorimotor Decision-Making",
    "presenter": "Tobias F. Niehues",
    "poster_id": "C1",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Human decision-making in sensorimotor tasks is characterized by perceptual uncertainty, motor variability, prior beliefs, and the goal of a task, as well as by other factors like the effort required to act. Distributions and costs in these tasks are usually assumed to be normally distributed or of quadratic shape to maintain analytical tractability for mathematical convenience. However, there are no guarantees whether these assumptions correctly represent the functional relations underlying human behavior.\nRecent work on inverse decision-making makes it possible to overcome these limitations while still allowing inference of behavioral parameters from data with arbitrary cost functions and sensory encoding. \nHere, we extended this approach to a hierarchical model, thereby allowing model comparison at the task level instead of a per-subject level. In all data sets, asymmetric cost functions describe human behavior better than quadratic costs, and in four out of five cases, the cost function contains explicit effort costs, contrary to previous investigations."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "How the Brain Automatically Encodes Global Predictability: Temporal Generalization Evidence for Stable Representations",
    "presenter": "Yun-Yi Qi",
    "poster_id": "C4",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "The brain can automatically track global information, yet the underlying mechanisms remain under debate. Although global processing, particularly of high-level structural regularities, is commonly associated with late components such as the N400, its temporal dynamics are not well understood. In this study, we employed a hierarchical roving oddball paradigm to manipulate global predictability while controlling for physical features and overall stimulus probability. EEG signals were collected while participants were passively presented with auditory inputs. ERP analysis revealed canonical MMN and P300 components. To investigate the temporal evolution of global encoding, we trained a time-resolved linear discriminant analysis to decode global predictability across time points. The resulting temporal generalization matrix showed significant cross-temporal decoding from 50 to 500 ms post-stimulus, indicating a temporally stable neural representation of global predictability. These findings suggest that the brain encodes global regularities in a sustained and temporally generalized manner, even in the absence of attention."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Bistable perception emerges from loopy inference in strongly coupled probabilistic graphs",
    "presenter": "Alexandre Garcia-Duran",
    "poster_id": "C3",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "During perception, the brain continuously processes sensory input, selecting among competing interpretations and assigning certainty -confidence- to each. Typically, confidence correlates with the strength of sensory evidence. In bistable perception, however, one interpretation is confidently perceived at a time, yet perception alternates despite no changes in the stimulus. We investigate which properties of visual stimuli drive this dissociation between evidence strength and confidence. We propose that bistability arises from approximate probabilistic inference over an internal representation of a stimulus with strongly coupled features. Using the Necker cube as an example, we model how perceived depth at each vertex is coupled with its neighbors, reflecting natural co-occurrence statistics. We analyze the dynamics of three inference algorithms. In all cases, strong feature coupling introduces loops in the internal representation that stabilize one percept, while internal noise drives perceptual switches. This creates a double-well potential, with perception fluctuating between high-confidence states. To test this, we designed a bistable stimulus in which feature coupling and sensory strength were independently manipulated. Our results show that stronger coupling leads to higher reported confidence, even when sensory evidence is weak. These findings suggest that bistable perception results from internal inference dynamics when stimulus features are tightly coupled."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Predictive Perception via Simultaneous Learning and Inference",
    "presenter": "Mahdi Enan",
    "poster_id": "C2",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "The brain is continuously faced with noise in the soundscape and uncertainty in the input to the ears. This has led to the proposal that perception is not a direct consequence of sensory input, but rather an inferential process to determine the most probable state of the world. However, the algorithm through which the brain could realize this inferential process as well as its implementation are not yet well  understood. In this work, we developed a novel framework for simultaneous learning and inference from first principles using exact inference and local Hebbian-like learning rules. We show that this framework allows unbiased inference and flexible model updating under noise and changing dynamics. Further, we show how this approach can reproduce local and global effects of prediction and prediction error in noisy environments. This model can be used to test (in silico) hypotheses related to predictive processing in noisy and non deterministic environments."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Sensorimotor Affordances in a Global Latent Workspace",
    "presenter": "Nicolas Kuske",
    "poster_id": "C6",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Understanding the role of embodiment in cognition is critical for advancing both neuroscience and artificial intelligence. While biological systems rely on multimodal sensorimotor interactions to ground meaning, artificial models often lack this grounding, limiting their ability to generalize across tasks and environments. In this work, we investigate the emergence of sensorimotor affordances within a Global Latent Workspace (GLW)—a multimodal deep learning architecture inspired by the Global Workspace Theory of consciousness. We train a reinforcement learning agent to perform a simulated embodied task (Obstacle Tower Challenge), and use its sensory-motor data to train a GLW multimodal representation (based on an encoder-decoder structure linked with each modality). We compare the GLW representation of images (from the agent's point of view) with the same image representations from a variational autoencoder. Our analysis reveals that the sensorimotor GLW compresses visual information into a structured motor latent manifold, naturally clustering affordance-relevant representations. Notably, these affordances enable zero-shot visual scene generation based on motor states, providing preliminary empirical support for sensorimotor theories of consciousness. By embedding affordances in a shared latent space, the GLW framework offers a biologically inspired path toward more generalizable and grounded artificial perception."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Decoding Movement from Neural Spike Trains: A Comparison of Linear and Nonlinear Models across Brain Regions and Temporal Delays",
    "presenter": "Minsung Cho",
    "poster_id": "C8",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Neural spike trains, which represent the spiking activity of neural population over time, provide critical insights into how the brain encodes information and generates behavior. Despite significant advances, the extent to which these spike trains encode  behavioral variables—particularly movement—remains not fully understood. In this study, we compare the performance of linear and nonlinear models in predicting behavior from neural spike trains, focusing on how prediction accuracy varies with different temporal lags between neural activity and movement onset. Furthermore, we examine how prediction performance depends on the specific brain regions from which the neural signals are recorded. Our findings provide new insights into the behavioral decoding with respect to both the temporal structure of neural spike activity and the specificity of brain regions."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Dynamics and Structure of Generalization During Reinforcement Learning in Human Brains and Artificial Networks",
    "presenter": "Shany Grossman",
    "poster_id": "C7",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Goal-directed decision making amidst an overwhelming stream of sensory input requires learning learn internal representations that capture a task’s underlying structure. Importantly, such internal abstractions enable generalization. Representing an object’s shape but ignoring its color, for instance, means that anything learned about a green triangle will generalize to red triangles. Here, we investigate this dynamic interaction of task representation learning and generalization. Human participants and artificial neural networks were trained with the same contextual reinforcement learning task. Analyses of human data reveals that participants learned an abstract task structure, which becomes detectable in the orbitofrontal cortex (OFC) after learning. Recurrent neural networks trained on the same learning curriculum exhibit similar abstractions of task representations over time. Notably, we find that the similarity structure of the networks internal task representations affects how weight updates after a single example alters network behavior and representations on other trials. The networks progressing context differentiation in its internal layers hence leads to generalization of single experiences to other events within the same context. Ongoing work aims to gain a mechanistic understanding of model observations and contrast them with learning dynamics in the human brain."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Proprioceptive Position Inference as Velocity Integration",
    "presenter": "Erik Skjoldan Mortensen",
    "poster_id": "C9",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "We investigate proprioceptively guided inference of hand position and velocity during movement using a virtual reality (VR) experiment. \nParticipants performed a paced continuous movement task, with visual feedback limited to a brief interval after movement onset. We demonstrate that brief visual offsets induce semi-persistent deviations in movement paths, suggesting that inferred hand position is heavily influenced by the integration of proprioceptively sensed velocity over time. \nTo further explore this, we present a generative model based on Bayesian sensory integration and compare the movement characteristics of three distinct model versions to human data. \nOur findings show that combining velocity integration with position sampling produces movement patterns that closely resemble human behaviour, highlighting the importance of velocity-based integration in proprioceptive inference."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Controlling PFC dynamics for slow and fast learning",
    "presenter": "Michał J Wójcik",
    "poster_id": "C10",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "The prefrontal cortex (PFC) exhibits a remarkable capacity to employ two distinct  strategies when engaging in cognitive tasks. Upon encountering a novel task, it leverages high-dimensional representations, well positioned for rapid linear decoding. However, with growing task familiarity, the PFC transitions to employing generalisable low-dimensional neural codes. Through a system-level modelling approach, we propose that these properties emerge naturally in recurrent neural networks (RNNs) that learn on two distinct timescales: (i) on a faster timescale an external controller drives RNN dynamics to generate task-encoding but relatively unstructured, high-dimensional representations, which is then followed by (ii) a slower optimisation of recurrent connections and consequently more structured, low-dimensional representations. We validated these predictions by comparing model representations to neural recordings from the prefrontal cortex of non-human primates that were trained to learn a complex cognitive task from scratch. In summary, our results suggest a learning-dependent control of prefrontal dynamics via a separate brain-region for high-to-low representational switching."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Long Range Cortical Interactions During Comparison of Sensory and Cognitive Information",
    "presenter": "Camila Losada",
    "poster_id": "C12",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Adapting our behavior to our environment depending on contextual demands involves large, distributed networks of cortical and subcortical areas. Prefrontal (PFC) and lateral intraparietal area (LIP) process both sensory and cognitive factors in very similar manner. However, their respective contributions to visual processing and cognitive control remain largely unclear. In this study, macaque monkeys performed a modified delay-match to sample task while we simultaneously recorded neuronal activity from V4, LIP and PFC. We show differential PFC/LIP dynamics: PFC exhibits successive encoding of different cognitive processes, while LIP shows more stable representations that integrate both sensory and cognitive signals."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Dissociating Confidence Bias and Confidence Noise in Perceptual and Knowledge-Based Decisions",
    "presenter": "Matteo Lisi",
    "poster_id": "C13",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "People often misjudge how reliable their decisions are, leading to confidence errors. These errors can be due to systematic distortions (confidence bias) or random variability (confidence noise). In this study we use a dual-decision method, in which participants are required to use confidence in a prior decision to inform expectations about subsequent choices, to examine the importance of these two sources of confidence error in both perceptual and knowledge-based tasks. Across a reanalysis of published data and two new studies, we find that perceptual tasks elicit under-confidence relative to Bayesian optimal predictions, while knowledge-based tasks exhibit increased confidence noise. Additional conditions using calorie estimation tasks suggest that some domains blend perceptual and knowledge-based decision features. These findings provide novel insights into the computational structure of confidence, suggesting that different cognitive domains are subject to distinct metacognitive constraints."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Metacognition as modal cognition",
    "presenter": "Kevin O'Neill",
    "poster_id": "C11",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "An influential perspective is that metacognitive judgments involve forming propositional confidence in a self-centered frame of reference, evaluating the plausibility of propositions regarding one's own cognition and mental states. Here we build on this framework to propose that, because metacognition involves the consideration of alternative possibilities or hypotheses, it is an instance of a more general capacity known as modal cognition. By extension, using the Pearl causal hierarchy we distinguish between metacognition targeting conditional, interventional, and counterfactual probabilities, each of which allows one to make different kinds of inferences about one's own cognition. This view stresses the relevance of research on modal cognition for metacognition, highlights underexplored targets of metacognition, and helps explain differences between metacognitive phenomena."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Combining Recurrent and Bayesian Models for Action Anticipation with Multiple Cues",
    "presenter": "Mariia Zimokha",
    "poster_id": "C15",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Understanding human action prediction requires modeling rapid temporal integration and deliberative reasoning. Inspired by human studies on multisensory cues, we propose a dual-process model with Reservoir Computing (RC) for temporal processing and Bayesian Networks (BN) for uncertainty-aware probabilistic decisions. The RC integrates sensory cues while the BN processes the output RC states to refine predictions. We tested this integrated framework using simulated reaching tasks with cues such as gaze direction, hand movement, and hand shape. The results indicate that our combined system replicates key aspects of human behavior."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Predicting Response Inhibition: A Deep Learning Approach Using Pre-Response Single-Trial EEG Data",
    "presenter": "Anna Grabowska",
    "poster_id": "C16",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Inhibitory control is a key component of executive functions. While it primarily depends on the extended motor network, early sensory processing of stimuli also plays a critical role in inhibition. This study examined whether deep neural networks could predict stop-signal task performance from early stop-related EEG signals in 225 participants, and whether including early go-related signals would enhance prediction accuracy. The best-performing model combined both go- and stop-related EEG data, revealing that successful inhibition was associated with reduced sensory processing of go stimuli and enhanced perception of stop signals. These results underscore the dynamic interplay between go and stop-signal processing and represent the first successful prediction of inhibition outcomes using non-motor EEG signals."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "The neural basis of abstraction representations in humans and non-human primates",
    "presenter": "Théo Morfoisse",
    "poster_id": "C17",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "The ability to recognize and manipulate abstract representations appears to be a fundamental aspect of human cognition, present since the origins of our species and transcending cultural barriers. In contrast, this capacity is very limited in non-human primates and artificial neural networks. To explore this distinction, we presented visual stimuli depicting the same concepts (e.g., faces, objects) at varying levels of abstractions (e.g., photos, drawings, words) to both humans and monkeys while recording neural activity using 7T fMRI and MEG in humans, and intracranial recordings in monkeys. Our findings reveal that while monkeys demonstrated a limited capacity for generalization – restricted mostly to faces – humans display a robust ability to abstract across all levels of abstraction, underscoring a fundamental specificity in conceptual processing."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Probability Distortions Reflect Boundary Repulsions  in Noisy Inference",
    "presenter": "Saurabh Bedi",
    "poster_id": "C18",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Probability distortions—the apparent overweighting of small probabilities and underweighting of large ones—is central to decision-making under risk, but its normative and mechanistic origins remain unclear. Traditionally seen as irrational, we propose that probability distortion instead emerges from optimal but noisy inference on bounded quantities. In our proposed account, repulsions arise at natural boundaries of probabilities (0 and 1) due to both resource-rational efficient encoding and Bayesian optimal decoding. Our account predicts that experimental manipulations of boundaries and noise should systematically reshape both probability distortions and behavioral variability, in both risky choice and probability perception. We confirm these predictions in three pre-registered experiments. Our findings reframe probability distortion as a normative consequence of bounded noisy inference and offer a unified mechanistic explanation for its presence across valuation and perception."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Pupil-linked arousal tracks belief updating in a dynamic auditory enviroment",
    "presenter": "Lars Kopel",
    "poster_id": "C19",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Bayesian theory prescribes that highly surprising outcomes should prompt rapid belief updating. Here we developed a novel passive belief updating protocol, suitable for humans and potentially mice. We observed multiple pupil-based signatures of belief updating: (i) pupil response magnitude was enhanced for stimuli that were unexpected given the recent history, and (ii) the relationship between pupil response magnitude and Bayesian change-point probability (surprise), derived from a normative belief updating model, reflected the participant’s belief about the volatility of the environment. We conclude that phasic arousal supports belief updating when encountering unexpected incoming information."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Computational Models of Dual System Reasoning",
    "presenter": "Zoe Purcell",
    "poster_id": "C14",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Dual system theories propose that human reasoning arises from two interacting systems: a fast, automatic process (System 1) and a slower, deliberative process (System 2; Kahneman, 2011; De Neys, 2022). Despite their influence across domains, including economics and moral psychology, these accounts remain largely verbal and underspecified. Existing computational models of reasoning tend to be narrow in scope, lack rigorous fitting, and inadequately capture interactions between System 1 and System 2—particularly how and when deliberative processes are triggered. We develop six computational models formalizing dual system assumptions, including mechanisms such as inhibition and metacognitive monitoring. Preliminary results suggest that intuitive and deliberative reasoning may rely on similar underlying processes."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Hierarchical systems in the default mode network when reasoning about self and other mental states",
    "presenter": "Isaac Ray Christian",
    "poster_id": "C20",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Humans spend time contemplating the minds of others. But this ability is not limited to external agents – we also turn the lens for reading minds inward, reflecting on our own thoughts, emotions, and sense of self. Some processes involved in reasoning about minds may rely on shared mechanisms, while others may be specific to the agent under consideration. Using fMRI and multi-voxel pattern analysis, we found that ventral regions in the DMN selectively decoded mental state inference patterns for self, but not other, whereas a region in posterior cingulate cortex differentiated the target of mental state inference. Using a cross-classification analysis, we also found patterns in the dorsomedial prefrontal cortex, ventromedial prefrontal cortex, and right temporoparietal junction were sensitive to mental state reasoning in general, regardless of the target agent. These findings highlight one process reflecting reasoning specific to the agent and another reflecting the reasoning process itself."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Confidence in absence as confidence in counterfactual visibility",
    "presenter": "Matan Mazor",
    "poster_id": "C21",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "When things are perceived clearly they can be detected with confidence. But when can one be confident that something is absent? Here, we used a meta-perceptual illusion to show that confidence in absence scales the belief that a target would have been visible if present. We manipulated stimulus size in two near-threshold detection tasks with confidence ratings. While participants believed that detection was easier for large stimuli (measured with prospective confidence ratings and post-experiment debriefing), their perceptual sensitivity was in fact higher for small stimuli. Accordingly, while confidence in presence scaled with true visibility, confidence in absence scaled with beliefs about visibility. Moreover, the effect of size on confidence in absence, but not in presence, correlated with a meta- perceptual parameter from an ideal observer model. Overall, we conclude that confidence in absence tracked model-derived expectations about the visibility of counterfactual stimuli."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Metacognitive Judgements of Confidence in Effortful Task Are Susceptible to Momentary Fluctuations of Fatigue",
    "presenter": "Katarzyna Dudzikowska",
    "poster_id": "C22",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Classical theories describe prospective confidence as a readout of the probability of success based on reinforcement history. However, research suggests that confidence may be susceptible to fatigue despite continued success. Here, we test if confidence changes throughout an effortful task and whether the moment-to-moment fluctuations in fatigue can account for these changes. Participants exerted physical effort (30, 30, 48% of maximum voluntary contraction - MVC) and reported confidence in their ability to succeed. Across three studies, we show that confidence declines over time and fluctuates on a trial-by-trial basis, despite consistently successful performance. Decreases in confidence and increases in fatigue ratings (Study II) were both related to exerted effort. We introduce a novel computational model in which latent fluctuations in fatigue drive changes in confidence and show that it outperforms models based solely on past performance or time on task. We also show that the relationship between confidence and fatigue is distinct from boredom (Study III). Thus, computations of confidence are susceptible to fluctuating levels of fatigue, even when the probability of success remains high."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Bayesian Comparisons Between Representations",
    "presenter": "Heiko H. Schütt",
    "poster_id": "C23",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "Which neural networks are similar is a fundamental question for both machine learning and neuroscience. Here, it is proposed to base comparisons on the predictive distributions of linear readouts from intermediate representations. In Bayesian statistics, the prior predictive distribution is a full description of the inductive bias and generalization of a model, making it a great basis for comparisons. This distribution  directly gives the evidence a dataset would provide in favor of the model. If we want to compare multiple models to each other, we can use a metric for probability distributions like the Jensen-Shannon distance or the total variation distance. As these are metrics, this induces pseudo-metrics for representations, which measure how well two representations could be distinguished based on a linear read out. For a linear readout with a Gaussian prior on the read-out weights and Gaussian noise, we can analytically compute the (prior and posterior) predictive distributions without approximations. These distributions depend only on the linear kernel matrix of the representations in the model. Thus, the Bayesian metrics connect to both linear read-out based comparisons and kernel based metrics like centered kernel alignment and representational similarity analysis. The new methods are demonstrated with deep neural networks trained on ImageNet-1k comparing them to each other and a small subset of the Natural Scenes Dataset. The Bayesian comparisons are correlated to but distinct from existing metrics. Evaluations vary slightly less across random image samples and yield informative results with full uncertainty information. Thus the proposed Bayesian metrics nicely extend our toolkit for comparing representations."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli",
    "presenter": "Mattson Ogg",
    "poster_id": "C25",
    "topic_area": "Language and Communication",
    "abstract": "Emotions exert an immense influence over human behavior and cognition in both commonplace and high-stress tasks. Discussions of whether or how to integrate large language models (LLMs) into everyday life (e.g., acting as proxies for, or interacting with, human agents), should be informed by an understanding of how these tools evaluate emotionally loaded stimuli or situations. A model’s alignment with\nhuman behavior in these cases can inform the effectiveness of LLMs for certain roles or interactions. To help build this understanding, we elicited ratings from multiple popular LLMs for datasets of words and images that were previously rated for their emotional content by humans. We found that when performing the same rating tasks, GPT-4o responded very similarly to human participants across modalities, stimuli and most rating scales (r = 0.9 or higher in many cases). However, arousal ratings were less well aligned between human and LLM raters, while happiness ratings were most highly aligned. Overall LLMs aligned better within a five-category (happiness, anger, sadness, fear, disgust) emotion framework than within a two-dimensional (arousal and valence) organization. Finally, LLM ratings were substantially more homogenous than human ratings. Together these results begin to describe how LLM agents interpret emotional stimuli and highlight similarities and differences among biological and artificial intelligence in key behavioral domains."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Large Language Models Are Good In-Context Function Learners",
    "presenter": "Elif Akata",
    "poster_id": "C26",
    "topic_area": "Language and Communication",
    "abstract": "Human cognitive neuroscience has revealed that humans are capable of flexibly learning complex functions. Large Language Models (LLMs) are increasingly being compared to humans in terms of their intelligence and behavioural sophistication. Here, we use a principled framework to examine whether Large Language Models are able to flexibly learn functions in-context. We find a human-like behavioural motif, in which LLMs are better able to learn smoother, more predictable functions with less noise, and that their in-context learning accuracy approaches the theoretical maximum in the limit."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Sparse Encoding of Grammatical Gender in LSTM Language Models",
    "presenter": "Conor Houghton",
    "poster_id": "C27",
    "topic_area": "Language and Communication",
    "abstract": "Neural network language models excel at capturing the complexities of natural language, yet their internal representations remain poorly understood. A key question is whether such models form structured, human-like abstractions that support generalization. We investigate how LSTM language models encode grammatical gender—an ideal test case, as gender is lexically fixed and generally not inferable from semantics. We focus on long-distance dependencies and various gender agreement configurations. \n\nWe conduct single-unit ablation to identify neurons critical for grammatical gender agreement. Across eight LSTM models, we find between one and five units whose removal significantly disrupts performance—by over 40\\% in some constructions involving gender-interfering nouns. These units are essential for both noun-adjective and noun-past-participle gender agreement. Neuron activity analyses reveal that these units exhibit category-specific effects, with some showing a preference for default gender forms, such as masculine nouns.\n\nOur findings show that LSTMs develop sparse and structured representations of grammatical gender, reminiscent of grandmother cells in neuroscience. These results suggest that abstract grammatical categories can emerge naturally in LSTM training. More broadly, this work contributes to our understanding of how language models encode linguistic structure, with implications for model interpretability and parallels between artificial and biological computation."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Tracking Time-Varying Syntax in Birdsong with a Sequential Autoencoder",
    "presenter": "Nadav Elami",
    "poster_id": "C28",
    "topic_area": "Language and Communication",
    "abstract": "Songbirds are excellent models for studying sensorimotor sequence learning. Their songs are composed of vocal units, called syllables. The ordering of syllables in song is governed by syntax rules that determine syllable transition probabilities. We recently used regression analysis to show that canaries, a seasonal songbird, change transition probabilities across days and afford a new model for studying how the brain adapts syntax rules. But, regression analyses, which calculate transition probabilities in neighboring song batches, are noise-limited in small subsets of songs. \n\nHere, to overcome this limitation and study the dynamics of syntax rules in fine temporal resolution we develop a neural filtering approach that infers time-varying transition probabilities from birdsong sequences. Inspired by deep learning methods for analyzing neural spiking data, we designed an autoencoder that treats each song as an observation from a probabilistic syntax model whose parameters change between song bouts. We carried simulated experiments, modeling both simple Markov and second-order dependencies of transitions, and demonstrate that our method accurately tracks syntax changes. \n\nThese findings underscore the potential of our approach to reveal the neural mechanisms underlying dynamic sensorimotor sequence generation."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Proactive Counterfactual Inference in Flexible Decision Making",
    "presenter": "Peiyue Liu",
    "poster_id": "C24",
    "topic_area": "Predictive Processing and Cognitive Control",
    "abstract": "In complex, uncertain environments, individuals must flexibly integrate multiple sources of information to adapt to changing task demands. While prior research has primarily focused on confidence formation and rule inference within a single task, less is known about how information across multiple tasks is integrated. Here,we designed an experiment to address this question by asking participants to infer task rules while switching between two tasks.  We found that participants were able to maintain cognitive control in the face of task-irrelevant information, ensuring smooth task performance. However, when such irrelevant information could potentially support task rule inference, individuals can flexibly adjust their strategies, leveraging this information to optimize the decision-making process. Participants' beliefs about the current task rule (rule belief) modulated this cognitive flexibility, influencing how they prioritized, processed, and integrated information. Neural data revealed that the dorsal anterior cingulate cortex (dACC) plays a central role in these processes, specifically in: (1) encoding both task-relevant and task-irrelevant evidence; (2) updating rule beliefs and (3) modulating functional connectivity with the human fourth visual area and middle temporal area (hV4/MT). To probe the underlying mechanisms, we trained a recurrent neural network (RNN) model. We showed that within a trial, these neurons operate under an attention bottleneck, which serves as a constraint and mimics the potential attention-splitting process observed in humans.\nAs with human participants, the effect of task-irrelevant information on rule belief updating was observed, but with a stronger effect. Together, these findings reveal a neural process in the human brain, particularly in the dACC, for integrating and updating beliefs about tasks, and how individuals flexibly adjust their strategies based on both relevant and irrelevant information."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Integration of internal linguistic information with sensory input via weakly entrained oscillations",
    "presenter": "Rong Ding",
    "poster_id": "C29",
    "topic_area": "Language and Communication",
    "abstract": "Rhythmic neural activity has been widely observed across cognitive domains, including language. Yet, debate continues on how such activity supports speech processing: entraining to external stimuli for optimised tracking, or driving the generation of internal representations. By introducing graded sensory entrainment to a fixed oscillator model of Ten Oever and Martin (2021), our study examined how it influences phase coding—the timing-based differentiation of word nodes from internal feedback—and how these codes bias ambiguous input interpretation. Simulations show that moderate coupling supports reliable phase coding while preserving sensitivity to unexpected inputs. Our model shows how the brain could coordinate top-down linguistic representations with bottom-up sensory processes during speech processing.\n\nReference:\n\nTen Oever, S., \u0026 Martin, A. E. (2021). An oscillating computational model can track pseudo-rhythmic speech by using linguistic predictions. eLife, 10, e68066. https://doi.org/10.7554/eLife.68066"
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Step-by-step analogical reasoning in humans and neural networks",
    "presenter": "Joonhwa Kim",
    "poster_id": "C30",
    "topic_area": "Language and Communication",
    "abstract": "Both humans and large language models (LLMs) perform better on some reasoning tasks when encouraged to think step by step. However, it is unclear whether these performance gains are based on similar principles. Testing both humans and LLMs on a novel word analogy task, we find that interference caused by semantic similarity hurts performance in both and drives humans to engage in a sequential reasoning process. These findings pave the way for investigation into the mechanisms that underlie the benefit of chain-of-thought and the decision process behind sequential thinking."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Disentangling of Spoken Words and Talker Identity in Human Auditory Cortex",
    "presenter": "Akhil Bandreddi",
    "poster_id": "C31",
    "topic_area": "Language and Communication",
    "abstract": "Complex natural sounds such as speech contain many\ndifferent types of information, but recognizing these distinct\ninformation sources is computationally challenging\nbecause sounds with shared information differ widely in\ntheir acoustics. For example, variation across talkers\nmakes it challenging to recognize the identity of a word,\nwhile variation in the acoustics of different words makes\nit challenging to recognize talker identity. How does the\nhuman auditory cortex disentangle word identity from\ntalker identity such that each type of information can be\ncoded invariant to acoustic variation all other information\nsources? To address this question, we measured neural\nresponses to a diverse set of 338 words spoken by 32 different\ntalkers using spatiotemporally precise intracranial\nrecordings from the human auditory cortex. We developed\na simple set of model-free experimental metrics for\nquantifying representational disentangling of word and\ntalker identity, both within individual electrodes as well\nas across different dimensions of the neural population\nresponse. We observed individual electrodes that show a\nrepresentation of words that is partially robust to acoustic\nvariation in talker identity, but no electrodes or brain\nregions showed a robust representation of talker identity.\nHowever, at the population level, we observed distinct dimensions\nof the neural response that nearly exclusively\nreflected either words or talker identity, and were completely\ninvariant to acoustic variation in the non-target dimension.\nThese results suggest that while there is partial\nspecialization for talker-robust word identity in localized\nbrain regions, robust disentangling is accomplished\nat the population level with distinct representations of\nwords and talker identity mapped to distinct dimensions\nof the neural code for speech."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Moderate evidence for large language models reflecting human neurocognition during abstract reasoning",
    "presenter": "Christopher Pinier",
    "poster_id": "C32",
    "topic_area": "Language and Communication",
    "abstract": "Large language models (LLMs) have shown alignment with human brain activity during language tasks, but it remains unclear whether this correspondence extends to higher-order cognition such as abstract reasoning. In this study, we compared human EEG responses— specifically fixation-related potentials (FRPs) time-locked to gaze fixations onset —to the internal activations of eight open-source LLMs performing a visual abstract reasoning task. Intermediate LLM layers showed clear differentiation across reasoning pattern types, suggesting potential specialization. While the best-performing models reached human-level accuracy, they did not consistently align with human behavioral patterns. Representational similarity analysis revealed only moderate correlations between model activations and FRP data. This may reflect a lack of neural alignment in LLMs and/or that there is only some relevant cognitive signal in the FRPs. These findings highlight both the promise and limitations of using LLMs as models of human abstract reasoning."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Neural encoding of lexical stress in human speech cortex",
    "presenter": "Ilina Bhaya-Grossman",
    "poster_id": "C33",
    "topic_area": "Language and Communication",
    "abstract": "Lexical stress – what distinguishes the noun “PRE-sent” from the verb “pre-SENT” – critically facilitates word recognition and comprehension in speech perception. To understand the neural mechanisms that enable the perception of lexical stress, we collected high-density intracranial electrocorticography recordings (ECoG) while ten English speaking participants performed two experiments: 1) passively listening to sentences with natural stress patterns and 2) actively identifying the stressed syllables in isolated spoken words. In Experiment 1, we identified neural populations that significantly encoded whether a syllable was stressed in natural speech. In Experiment 2, we found that stress-encoding neural populations were both sensitive to prior syllable contexts and categorical. Our findings characterize the distinct neural populations that process lexical stress, providing insight into the complex neural mechanisms that underlie this fundamental linguistic skill."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "The role of context in neural representational alignment to audio- and text-based language systems",
    "presenter": "Marianne De Heer Kloots",
    "poster_id": "C34",
    "topic_area": "Language and Communication",
    "abstract": "Speech understanding requires integrating the current input with surrounding context. Prior research has found that increasing context size in artificial text-based language systems leads to improved predictivity of human brain activity. Here, we investigate (i) how the type of context (unidirectional; bidirectional) influences brain alignment; (ii) how the information contained in speech and text model embeddings changes as a function of context size and context type; (iii) what changes in model representations could explain brain alignment. We recorded intracranial EEG of participants listening to audiobooks, and extracted corresponding layerwise embeddings from a speech model (Wav2Vec2) and a language model (RoBERTa) under different context sizes and types. We find that context type rather than size has the biggest influence on the linear decodability of linguistic structure, the intrinsic dimensionality of the underlying representations, and ultimately, brain alignment. This work represents an important step towards understanding the representational basis of model-brain alignment, and identifies context type as an important driver of models extracting brain-relevant information."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Encoding Brain Regions with Sentiment-Relevant Circuits in LLMs",
    "presenter": "Nursulu Sagimbayeva",
    "poster_id": "C35",
    "topic_area": "Language and Communication",
    "abstract": "Large language models (LLMs) generate representations that effectively predict brain responses to natural language, yet the specific circuits within LLMs that drive this alignment remain largely unexplored. We here apply techniques from mechanistic interpretability (MI) to\nidentify LLM circuits (i.e., attention heads) causally relevant to sentiment processing and assess their impact on LLM–brain alignment. Our results show that removing sentiment-related attention heads leads to a greater decrease in alignment with language-processing brain regions compared to random head removal, although this difference does not reach statistical significance. Ongoing work aims to further improve LLM circuit identification in naturalistic settings, enabling more precise mapping of circuits to plausible brain mechanisms and ultimately providing deeper insights into LLM–brain alignment."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Inducing bias from bilingual brains into language models",
    "presenter": "Anuja Negi",
    "poster_id": "C37",
    "topic_area": "Language and Communication",
    "abstract": "Recent studies have shown that inducing bias from neural data in language models can enhance their ability to encode brain activity and improve performance on language tasks. However, these approaches have mainly focused on a single language. Given recent evidence that semantic representations are shared across languages in the bilingual brain, we ask whether brain-informed fine-tuning can reveal latent multilingual capabilities in language models. To test this, we fine-tune pretrained monolingual Transformer models (English and Chinese BERT) using fMRI data from bilingual individuals. We find that fine-tuning improves downstream performance not only in the language used for training but also in the other language, indicating cross-linguistic generalization. Further, the encoding performance of the fine-tuned model across other participants remains the same, suggesting that the brain bias introduced by fine-tuning is shared across bilingual individuals."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Resolving Communicative Uncertainty through Computational Inference of Partner Intentions",
    "presenter": "Tengfei Zhang",
    "poster_id": "C36",
    "topic_area": "Language and Communication",
    "abstract": "Effective social communication demands that individuals adeptly align their conceptual representations through the precise use of \nlanguage.  Yet, how individuals resolve uncertainty in selecting context-appropriate utterances remains a core question in cognitive science and a significant challenge for large language models (LLMs).  In this study, 60 participants (30 same-gender dyads) performed a collaborative word generation task designed to capture the dynamics of open-ended, two-way communication. Our results show that human interlocutors can effectively resolve communicative uncertainty and achieve mutual understanding, even in unconstrained, ambiguous exchanges. Furthermore, drawing on established psycholinguistic theories, we developed computational models within the cohort-based, selection-by-competition framework to test two competing mechanisms. The findings suggest a functional division of labor: statistical learning (SL) facilitates the generation of candidate lexical cohorts, while pragmatic reasoning (PR) predominantly governs word selection within the cohort."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Rate-Coding Bundle Memory: A Unified Model of Memory and Control for Language Comprehension",
    "presenter": "Teun van Gils",
    "poster_id": "C38",
    "topic_area": "Language and Communication",
    "abstract": "Neurobiological and cognitive models often tackle individual facets of cognition, yet few successfully integrate across multiple domains. For instance, (pre-transformer) connectionist approaches explain a broad range of cognitive phenomena, including graded semantics and context sensitivity, but struggle with symbolic reasoning, compositionality and dynamic variable binding, necessary components of language comprehension (Fodor \u0026 Pylyshyn, 1988; Marcus, 1998; Lake et al., 2017; Kazanina \u0026 Poeppel, 2023). Conversely, symbolic models excel at these tasks but often lack a biologically inspired explanation of how these symbolic operations might be implemented in\nthe brain (Do \u0026 Hasselmo, 2021). We propose a unified model, inspired by (psycho-)linguistic theory (McElree et al., 2003; Seuren, 2009), that integrates these aspects, focusing on the interplay between memory, unification and control (Baggio \u0026 Hagoort, 2011). This model, which we call Rate-Coding Bundle Memory (RCBM), is designed to be both biologically plausible and capable of addressing a variety of cognitive tasks, including those that require the incremental integration and differention of entities in linguistic descriptions of scenes. We demonstrate the model’s performance on a range of tasks, including controlled storage, memory retrieval, tracking multiple memories, and semantic inference; and show, by lesioning different components of the model, that the various memory and control components are crucial for the model’s ability to perform these tasks. Our model improves upon existing working memory models of the prefrontal cortex\n(Manohar et al., 2019; Fiebig et al., 2020), and shows parallels with properties of grid- and place cells during hippocampal replay (Do \u0026 Hasselmo, 2021; Kurth-Nelson et al., 2023; Kazanina \u0026 Poeppel, 2023)."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Modelling Demonstrates Phase-Reset is an Important Feature of Neural Speech Processing",
    "presenter": "Andrew Shannon",
    "poster_id": "C39",
    "topic_area": "Language and Communication",
    "abstract": "Syllable segregation and source separation are foundational components of neural speech processing, yet consensus on their underlying mechanism remains elusive. Several hypotheses have been proposed, suggesting that the brain may align its activity to incoming linguistic stimuli via evoked responses, entrainment of endogenous oscillations, or some combination of the two. We investigate the origin of oscillatory behaviour in syllable segregation by modelling the dynamical response to periodic linguistic stimuli. We compare a biophysically accurate neural mass model and a phase-resetting oscillator with prior experimental EEG data. We find that a correlation between neural activity entrainment strength and the sharpness of incoming phonemes, identified in the EEG experiment, is readily reproduced by both the neural mass model and the oscillator. However, when the phase-resetting dynamics are removed, the oscillator fails to reproduce the correlation. This demonstrates that phase-resetting is required for sharpness specific tuning of neural entrainment to speech. Identifying the neural correlates of this phenomenon may be possible through interrogation of the biophysical features of the neural mass model."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Causal Discovery and Inference through Next-Token Prediction",
    "presenter": "Eivinas Butkus",
    "poster_id": "C41",
    "topic_area": "Language and Communication",
    "abstract": "Some argue that deep neural networks are fundamentally _statistical_ systems that fail to capture the causal generative processes behind their training data. Here we demonstrate that a GPT-style transformer trained for next-token prediction can simultaneously discover instances of linear Gaussian structural causal models (SCMs) and learn to answer counterfactual queries about them. First, we show that the network generalizes to counterfactual queries about SCMs for which it saw _only_ strings describing noisy interventional data. Second, we decode the implicit SCM from the network's residual stream activations and use gradient descent to intervene on that “mental” SCM with predictable effects on the model's output. Our results suggest that neural networks trained using statistical prediction objectives on passively observed data may nevertheless discover and learn to use causal models of the world."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "On whether the relationship between large language models and brain activity is language-specific",
    "presenter": "Sertug Gürel",
    "poster_id": "C40",
    "topic_area": "Language and Communication",
    "abstract": "Using large language models (LLMs), such as GPT-2, to study language processing in both machines and humans has become increasingly prevalent. Existing literature demonstrates that these models are strong predictors of human brain activity (Schrimpf et al., 2021), which has been taken to indicate that LLMs are good models for language processing in the human brain. The current study aimed to assess whether these models’ predictive performance of brain activity is specific to brain regions involved in language processing and whether or not the prediction of functionally different brain regions relies on different features of the LLMs' hidden layers. Our results suggest that LLMs' ability to predict brain activation does not strongly differ between language and non-language-related brain areas. The set of features that drive prediction performance across areas is not entirely the same, but there is a considerable correlation between the features that language-related and non-language-related regions rely on for brain predictions. Hence, we suggest that more research is needed to understand the nature of the information that drives brain predictions in LLMs."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Dissociable Neural Signatures for Surprisal and Entropy Reduction in Mandarin Speech Comprehension",
    "presenter": "Qifei Wang",
    "poster_id": "C43",
    "topic_area": "Language and Communication",
    "abstract": "The neural response to words is modulated by the amount of information conveyed by the words. For example, it is well established that neural activity monotonically increases as a function of the surprisal of a word. Another important information-theoretic measure, entropy reduction (ER), quantifies how each incoming word constrains subsequent  content interpretations. Here we examined whether and when surprisal and ER modulated neural activity during naturalistic speech comprehension. Through magnetoencephalographic recordings (MEG) of naturalistic Mandarin comprehension, we demonstrate that ER accounts for neural responses that cannot be explained by surprisal alone. Initial observations show that ER elicits a later (400-600 ms) cortical response compared to the N400 surprisal effect. These preliminary results suggest that ER may involve neural computations distinct from those underlying surprisal, with early resolution of surprisal followed by later ER. Our findings extend predictive processing frameworks to tonal languages and highlight entropy reduction as a key component of neural language models, operating beyond surprisal."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Spectral Encoding Profile for multilevel linguistic predictions",
    "presenter": "Clément Sauvage",
    "poster_id": "C44",
    "topic_area": "Language and Communication",
    "abstract": "Speech processing is believed to rely on two types of information. First, the predictions, which are endogenous and flowing down the cortical hierarchy in a top-down manner, and second, the prediction errors, computed as the difference between the effective and the predicted inputs at each stage of the said hierarchy, in a bottom-up flow of information. The putative role of neural oscillations to mediate those signals is still a question of debate. Here we recorded intracranial EEG activity of 45 epileptic patients, while they listened to ecological speech. We used a Large Language Model to extract proxies of both prediction uncertainty and prediction errors at three linguistic levels, phonemes, syllables and words. We found that encoding of prediction errors and prediction uncertainty peaks respectively in high-gamma and beta bands, only in Primary Auditory Cortex and Superior Temporal Gyrus."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Voxel-wise Encoding of Visual and Social Meaning During Silent Movie Viewing in Deaf and Hearing Participants.",
    "presenter": "Maria Zimmermann",
    "poster_id": "C45",
    "topic_area": "Language and Communication",
    "abstract": "In deaf individuals, higher auditory regions such as the superior temporal cortex are thought to be repurposed for visual processing. In previous study we showed that these regions are recruited for processing rich visual meaning during silent films (Zimmermann et al., 2024). To investigate which specific features drive this reorganization, we applied a voxel-wise encoding model to fMRI data from deaf and hearing participants as they watched a silent animated movie.\nThe model included a range of visual, social, and affective features. It significantly explained variance across the whole brain in both groups, revealing patterns consistent with prior findings. In regions that showed differential intersubject synchronization between groups, we observed higher prediction performance scores (R² ) in deaf participants. This was particularly evident for social interaction features in the right superior temporal sulcus (STS) and Theory of Mind features in the right posterior STS. These findings suggest that reorganization in the temporal cortex in deaf individuals may reflect an expansion of nearby visual and social feature representations into formerly auditory regions."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Computational models of vision do not explain the effect of expertise on neural processing of visual Braille",
    "presenter": "Filippo Cerpelloni",
    "poster_id": "C42",
    "topic_area": "Language and Communication",
    "abstract": "The human visual stream adapts to process letters and words at different processing stages (Vinckier et al., 2007), even when the stimuli do not share canonical script features, like Braille (Cerpelloni et al., 2024). This supports an interactive account of the Visual Word Form Area (VWFA). Here we expand these findings to test the organization of peculiar visual features in computational models. By training a benchmark convolutional neural network (AlexNet) to classify words in the Latin script (literacy) and then in the Braille script (expertise), we model the processing of reading visual Braille and explore the network’s representations at different stages. We observe a similar degree of clustering between models before and after training on Braille. The lack of alignment between the visual processing of the computational models and the effect of expertise highlighted by neural data suggests that the fundamental processing of reading cannot be fully explained by the visual characteristics of the script, but necessarily relies on other mechanisms, among which language connections."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A Two-Dimensional Space of Linguistic Representations Shared Across Individuals",
    "presenter": "Greta Tuckute",
    "poster_id": "C46",
    "topic_area": "Language and Communication",
    "abstract": "Humans learn and use language in diverse ways, yet all typically developing individuals acquire at least one language and use it to communicate complex ideas. This fundamental ability raises a key question: Which dimensions of language processing are shared across brains, and how are these dimensions organized in the human cortex? To address these questions, we collected ultra-high-field (7T) fMRI data while eight participants listened to 200 linguistically diverse sentences. To identify main components of variance in the sentence-evoked brain responses, we performed data decomposition and systematically tested which components generalize across individuals. Only two shared components emerged robustly, together accounting for about 32% of the explainable variance. Analysis of linguistic feature preferences showed that the first component corresponds to processing difficulty, and the second—to meaning abstractness. Both components are spatially distributed across frontal and temporal areas associated with language processing but, surprisingly, also extended into the ventral visual cortex. These findings reveal a low-dimensional, spatially structured representational basis for language processing shared across humans."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A Neural Model for Word Repetition",
    "presenter": "Daniel Dager",
    "poster_id": "C47",
    "topic_area": "Language and Communication",
    "abstract": "It takes several years for the developing brain of a baby to fully master word repetition — the task of hearing a word and repeating it aloud. Repeating a new word, such as from a new language, can be a challenging task also for adults. Additionally, brain damage, such as from a stroke, may lead to systematic speech errors with specific characteristics dependent on the location of the brain damage. Cognitive sciences suggest a model with various components for the different processing stages involved in word repetition. While some studies have begun to localize the corresponding regions in the brain, the neural mechanisms and \\textit{how} exactly the brain performs word repetition remain largely unknown. We propose to bridge the gap between the cognitive model of word repetition and neural mechanisms in the human brain by modeling the task using deep neural networks. Neural models are fully observable, allowing us to study the detailed mechanisms in their various substructures and make comparisons with human behavior and, ultimately, the brain. Here, we make first steps in this direction by: (1) training a large set of models to simulate the word repetition task; (2) creating a battery of tests to probe the models for known effects from behavioral studies in humans, and (3) simulating brain damage through ablation studies, where we systematically remove neurons from the model, and repeat the behavioral study to examine the resulting speech errors in the \"patient\" model. Our results show that neural models can mimic several effects known from human research, but might diverge in other aspects, highlighting both the potential and the challenges for future research aimed at developing human-like neural models."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Probing compositional learning during spontaneous exploration of a reconfigurable 3D environment",
    "presenter": "Tzuhsuan Ma",
    "poster_id": "C49",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "In natural settings, animals navigate richly-structured sensory surroundings and rapidly adapt to changes in these surroundings. While many studies have explored navigation in mazes and open arenas, relatively little is known about how animals navigate in terrain that lacks defined routes and is too complex to memorize. Here, we probe the structure of mouse behavior in a complex, reconfigurable 3D arena in darkness and without explicit reinforcement.\n\nWithin the first several hours, mice quickly explore the whole arena and converge on a sparse set of running and jumping paths. Surprisingly, after this initial phase of exploration, mice continue to generate new long paths for several days. To capture this structure, we develop a hierarchical segmentation algorithm that compresses raw behavioral trajectories into a compact set of composable sub-paths, or ``motifs''. We find that the behavior is highly compressible, indicating that mice create long paths by combining reusable motifs, rather than through random exploration.\n\nTo study the evolving dynamics of these behavioral compositions, we first show that mice combine motifs in a non-random manner, generating temporal structure that is not captured by a Markov-chain that preserves the average transition probabilities between motifs. Next, we examine different phases of behavior in generating novel compositions. We find diverse dynamics that involve the rapid creation and extinction of compositions, as well as slower and more subtle refinements such as morphing, short-cutting, streamlining, and reinforcing a composition. These results suggest that mice use diverse learning rules to configure compact behavioral trajectories through space."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Neural Signatures of Tree Search",
    "presenter": "Nastaran Arfaei",
    "poster_id": "C48",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "From the moon landing to meal preparation, people plan sequences of actions to achieve their goals. A leading theoretical framework proposes that human planning follows an algorithm akin to tree search. This framework predicts that during the decision-making process, the brain updates its estimate of the value of the current state by iteratively expanding a tree of future states. To test this prediction, we collected whole-brain BOLD activity from participants as they planned their move in a strategic game. Activity in the ventral striatum was modulated by the state value estimated by a tree search model. Moreover, in this area, the activity earlier (later) in the trial was more strongly modulated by the value predicted by the model earlier (later) in the tree search. Searchlight analysis revealed areas associated with the Default Mode Network to represent features critical for planning. This work contributes to an emerging understanding of the neural basis of tree search."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Neural generalization principles of working memory in humans and recurrent neural networks",
    "presenter": "Qing Yu",
    "poster_id": "C50",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "A fundamental endeavor in cognitive neuroscience is to understand how information can be rapidly abstracted through shared perceptual or structural knowledge to facilitate efficiency and learning. Working memory (WM) provides a flexible mental workspace for these computations, yet how generalization is realized within WM remains largely unexplored. Here, using functional MRI (fMRI) and recurrent neural network (RNN) modeling, we investigated how stimulus and rule information generalize within WM. Across two experiments, participants performed two WM tasks with shared stimulus structure but distinct stimulus sets (location and object), either without (Experiment 1) or with (Experiment 2) explicit mapping. In each task, they flexibly switched between maintenance and manipulation of stimulus information following task rules. Leveraging multivariate decoding and state space analyses, we revealed separate neural substrates in the generalization of stimulus and rule information in WM: the posterior parietal cortex represented mnemonic information across stimulus domains, with enhanced generalization of mnemonic information during memory manipulation compared to maintenance. In contrast, frontal subregions encoded abstract rules that were generalizable across tasks. RNN simulations replicated the key generalization patterns. Together, our findings reveal the neural generalization principles of WM that enable flexible maintenance and manipulation of information for goal-directed behavior."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Spatial Navigation Engages a Distributed Neural Representation",
    "presenter": "Enny H. van Beest",
    "poster_id": "C52",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Navigation is a complex goal-directed behavior that relies heavily on sensory processing. A key center is the hippocampal formation, where well-known ‘place cells’ and ‘grid-cells’ encode the animal’s position. Correlations with spatial position have also been found in other brain regions, including retrosplenial, visual, and even olfactory cortex. However, spatial processes are often not distinguishable from sensory, motor, and reward processes. To distinguish the contribution of different processes to the spiking activity of individual neurons distributed across the brain, we use an experimentally controllable virtual reality corridor, Neuropixels recordings, and reduced-rank ridge regression. We find that indeed, spatial, but also sensory, motor, and reward processes, are distributed widely across the brain."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "From Sound to Source: Human and Model Recognition in Auditory Scenes",
    "presenter": "Sagarika Alavilli",
    "poster_id": "C51",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Our ability to recognize sound sources in the world is critical to daily life, but it is not well documented or understood in computational terms. We developed large-scale behavioral benchmarks of human environmental sound recognition, built signal-computable models of sound recognition, and used the benchmarks to compare models to humans. The behavioral tests measured how sound recognition abilities varied with the source category, audio distortions of different types, and concurrent sound sources, all of which influenced recognition performance in humans. Artificial neural network models trained to classify sounds in multi-source scenes reached near-human accuracy and qualitatively matched human patterns of performance in many (but not all) conditions. By contrast, traditional models of the cochlea and auditory cortex produced worse matches to human performance. The results suggest that many aspects of human sound recognition emerge in systems optimized for the problem of real-world recognition. The benchmark results clarify the factors that constrain human recognition, setting the stage for future explorations of auditory scene perception involving salience, attention, and memory."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "The Role of Landmarks and Travel Pauses in the Generation of Hexadirectional fMRI Signals during Spatial Navigation",
    "presenter": "Siyuan Mei",
    "poster_id": "C53",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "As humans navigate in physical, conceptual, or even social domains, functional magnetic resonance imaging (fMRI) signals from the hippocampal formation vary with movement directions, fluctuating in strength with a periodicity of 60 degrees. While grid cells are believed to underlie these hexadirectional fMRI signals, the exact mechanism linking the two is unclear. Three hypotheses have been proposed: conjunctive grid-by-head-direction cells, repetition suppression, and nonlinear transformations of grid-cell activity into BOLD signals.\nWe aim to combine theoretical analysis and fMRI experiments and identify key design parameters for distinguishing among the three hypotheses. Two critical factors emerge from our analysis: the presence of landmarks as a precondition, and pauses between linear path segments as a way to distinguish among the hypotheses. First, we modeled the trajectories represented by grid-cell activity using an extended Kalman filter fitted to behavioral data. Under all three hypotheses, hexadirectional signals emerge when landmarks are present, reflecting the landmarks' role in providing correct directional information. With correct directions, the theoretical analysis further predicts that the pauses measurably increase the hexadirectional strength under the repetition suppression hypothesis, diminish it under the nonlinearity hypothesis, and have no effect under the conjunctive grid-by-head-direction cell hypothesis. We are conducting experiments to examine the predictions."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Stage-like Emergence of Task Strategies in Animals and in Neural Networks Trained by Gradient Descent",
    "presenter": "J Tyler Boyd-Meredith",
    "poster_id": "C54",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Humans and animals learning a task often appear to adopt a series of distinct strategies before reaching expert performance. This progression could result from deliberately testing distinct hypotheses about task contingencies. However, stage-like strategy changes can also be produced by artificial neural networks (ANN) learning by gradient descent (GD) without any explicit notion of task strategy. In this setting, apparent strategies correspond to saddle points in the loss dynamics around which learning slows before accelerating toward the next fixed point. We trained mice to perform a previously developed discrimination task, which they acquired in a series of stage-like behavioral transitions. We then developed an ANN model that recapitulated these transitions. By measuring the magnitude of the gradients during learning, we determined when the network approached (decreasing norm) and escaped saddle points (increasing norm) before reaching expert performance. Our modeling results show that even simple connectionist models without explicit hypotheses can be tailored to produce stages of learning that match what we observe in animals. We propose to develop and apply a method to identify saddle points of the loss and the likely transitions between them by performing gradient descent, not on the loss function, but on the magnitude of its gradient. For this abstract, we show how this tool identifies saddle points and their connections in a toy example."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Sensitivity to network structures via associate learning in sleeping neonates",
    "presenter": "Claire Njoo-Deplante",
    "poster_id": "C55",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Extracting structural patterns is essential for all humans to make sense of the world - particularly so for infants, who must decipher language to acquire it. While it has been shown that infants can learn both low-level statistical regularities and higher-order structures, the neural mechanisms supporting these learning processes remain underexplored. A recent study in adults suggests that a low-level associative learning mechanism may underlie both types of learning. In this study, we investigated whether a similar mechanism is present in neonates by examining their ability to encode network structures in auditory sequences during sleep. We passively presented them with sequences of tones organised in a community network structure while recording their brain activity using electroencephalography (EEG). The preliminary results of our pilot analysis using multivariate pattern analysis (MVPA) reveal that we can successfully decode the network structure at the individual level. Complementary analysis should confirm these findings at the group level. Beyond deepening our understanding of neonates’ sensitivity to regularities, this study sheds light on the neural mechanisms behind complex structure learning, potentially bridging learning mechanisms across different structural levels within a unified theoretical framework."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Lower-Dimensional, Optimized Representations of High-Level Information in Chess Experts",
    "presenter": "Andrea I Costantino",
    "poster_id": "C56",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Chess provides a powerful framework to investigate how expertise shapes neural representations. We conducted an fMRI study with 20 expert and 20 novice chess players who viewed 40 boards that systematically varied across three main feature categories. Using Representational Similarity Analysis (RSA), we found that while both groups encoded low-level visual features similarly, experts showed distinctly more clustered representations of strategic and higher-level properties. A dimensionality compression measure (Participation Ratio) further revealed that experts’ neural signals were concentrated in fewer dimensions, suggesting more efficient coding in experts. Taken together, these findings suggest that expertise may result in optimized, lower-dimensional representations within regions involved in both domain-specific (chess-related) and domain-general processing, enabling more effective representations of complex stimuli – which may be the basis of Expertise behavioral effects."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A semantic bias for accurate predictive learning in multidimensional environments",
    "presenter": "Euan Prentis",
    "poster_id": "C57",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "To make accurate inferences about our multidimensional world, humans must distinguish observations of causal processes from spurious associations. We investigated the role of inductive biases in shaping memory around causal information, specifically testing for a semantic bias that leverages existing semantic structure to direct learning. Participants completed a predictive learning task in which both causal and spurious associations were observed. Results showed that spurious inferences were suppressed when the causal associations were defined within semantic categories, indicating that a semantic bias directed learning. Simulations of a feature-based successor features model further demonstrated that this bias should have a more dramatic benefit in more naturalistic environments, with high-dimensional states and deep causal processes. In all, this work demonstrates that inductive biases that act on multidimensional transition dynamics may be essential for learning in our complex world."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Evidence of fear conditioning in virtual reality revealed by eye movements",
    "presenter": "Catherine Kim",
    "poster_id": "C59",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Understanding how humans form associations between outcomes and environmental cues is well studied, but it is not clear how these associations are learned in real-world settings. Virtual reality (VR) provides a robust and ecologically valid platform for investigating contextual influences on learning, memory, and emotion. In this study, we use VR to study contextual fear conditioning. We conducted an integrated VR and eye-tracking study with 11 adults. To understand the effect of fear conditioning on behavior and emotional state, we analyzed gaze fixation data and self-report ratings of valence and arousal. We trained a long short-term memory (LSTM) classifier to differentiate responses to conditioned and neutral stimuli. We found that participants exhibited significantly higher gaze concentration on conditioned stimuli compared to neutral stimuli, demonstrating an enhanced emotional engagement in the fear-inducing virtual environments. Furthermore, the LSTM model not only successfully differentiated between fear and neutral conditions with high accuracy, but also revealed different patterns of fear generalization between pre- and post- fear session."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Human-in-the-loop synthesis of behaviorally salient “super-distractors”",
    "presenter": "Devarajan Sridharan",
    "poster_id": "C58",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Working memory(WM) is the ability to maintain and manipulate information that is no longer present in the environment. The resilience of WM to distraction is largely tested by studies employing simple stimuli (e.g., gratings, shapes, isolated objects). Hence, what kinds of complex, naturalistic images make for potent WM distractors remains unknown. Here we leverage recent advances in deep generative models to synthesize naturalistic images that powerfully disrupt WM. Our approach generates synthetic images with a class-conditional generative adversarial network (GAN), while concurrently testing the efficacy of these images as distractors on participants(n=16) performing a spatial WM task (human-in-the-loop). With a genetic algorithm for optimization, we identify the most salient feature combinations and refine them over generations to produce powerful “super-distractor” images. Our study demonstrates the feasibility of generating novel kinds of images optimized for specific behaviors, with a human-in-the-loop paradigm."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A Unified Benchmark for Human-Like Memory in Artificial Agents",
    "presenter": "Lucas Gruaz",
    "poster_id": "C60",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Human memory exhibits a diverse range of well-documented phenomena, including forgetting curves, interference effects, and schema-based distortions. While existing computational models attempt to capture aspects of these phenomena, they are often evaluated in isolation using task-specific experimental setups, limiting their generalizability and comparability.\n\nWe develop a unified benchmark for systematically evaluating memory models based on their ability to reproduce human-like memory phenomena. Our approach includes: (1) analyzing and formalizing a diverse set of memory phenomena in generalizable terms, independent of specific experimental paradigms, and (2) developing an evaluation framework that tests these phenomena within a common environment. This allows to test all phenomena on the same memory-augmented agent.\n\nWe test different memory models on schema-based distortion, memory conjunction errors, contiguity and recency effects.\nWe find that none of the tested memory models qualitatively matches human memory behavior on all these four phenomena, and we identify promising directions for future research on memory models."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Representational dynamics of concurrent visuospatial working memories in Schizophrenia",
    "presenter": "Ines Pont Sanchis",
    "poster_id": "C62",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Schizophrenia (SCZ) is a neuropsychiatric disorder whose core symptomatology includes impairments in working memory (WM) and higher executive functions. A wide range of WM function alterations in SCZ have been described, such as increased distractibility, decreased capacity and precision, differential biases, and more. In this ongoing study, we use a novel visual dual-task paradigm to examine the potential crosstalk between concurrent WM representations in SCZ versus healthy controls. Preliminary behavioral results indicate a possible attraction between concurrent WM items as well as an effect on response variance. Using simultaneous EEG and eye-tracking and representational similarity analysis, we explore the possible neural mechanisms underlying these effects, as well as the dynamics of maintained orientation representations. Finally, we examine previously observed oscillatory WM phenomena in SCZ, such as aberrant frontal gamma oscillations, and their relation to task performance."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Statistical Knowledge Transfer Across Stimulus-Response Associations",
    "presenter": "Cintia Anna Nagy",
    "poster_id": "C63",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Probabilistic information appears in various forms in our environment. Research on transfer in changing conditions generally shows success, but it is unclear whether people acquire the rule itself or the underlying structure. Our study explored this by testing participants in a two-session statistical learning task with different stimulus-response associations. The rule transfer group learned the same sequence in both sessions, while the structure transfer group learned different sequences with the same structure. The control group learned only in the second session. Neither rule nor structure knowledge improved learning in the second session, but both experimental groups showed greater acceleration than the control group, indicating a dissociation between statistical learning and visuomotor performance. Thus, relearning occurs rather than transfer, but visuomotor performance generalizes. Our findings suggest that transfer is evident in visuomotor learning but remains limited in statistical learning."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Trait depression predicts negatively biased encoding and retrieval of ambivalent movie: interoceptive and lexical analysis",
    "presenter": "Jinwoo Lee",
    "poster_id": "C64",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Ambivalent affect, the most naturalistic emotion in daily life, is largely modulated by individual traits. In particular, trait depression weights negative affect in the recall of ambivalent events. However, as trait-affect interplay is differently expressed across affective contexts and memory stages - encoding and retrieval, their relationship and mechanism should be specified along with these dimensions. We predicted that 1) altered interoception would mediate trait depression and negative encoding of affective context and 2) its negatively biased retrieval would be manifested in free languages. To test them, we combined movie-watching and free-recall paradigm, deep representational learning of EEG and ECG, and LLM-based sentiment analysis of recall text. We found that trait depression predicted ambivalent encoding of pleasant scenes via inter-subject similar interoceptive representation. Also, trait depression predicted negatively biased recall of ambivalent scenes. We discuss the potential of ambivalent processing as a depression-specific risk marker and its clinical implications."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Behavioral timescale synaptic plasticity, replay, and emergent behavioral choice",
    "presenter": "Suhee Cho",
    "poster_id": "C65",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Animals can adapt to novel environments with minimal exploration and rapidly adjust to change. While the hippocampus is thought to encode spatial and other behaviorally relevant information to support this, how it does so with such efficiency remains unclear. Here, we show that behavioral time-scale synaptic plasticity (BTSP) coupled with replay can allow the hippocampus to learn a predictive map—known as the successor representation (SR)—after minimal exploration. Reward-induced BTSP events bias this map to over-represent reward locations, biasing behavior based on this representation toward reward, and replay events extend this bias to locations farther from the reward. Notably, the representation dynamically adjusts when reward locations shift, supporting rapid behavioral adaptation. Together, our findings offer a biologically plausible account of how BTSP and replay jointly enable quick adaptation of the SR, supporting efficient and adaptive learning."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "How your brain learns penguins are birds - neural patterns of exception learning",
    "presenter": "Rebekka Heinen",
    "poster_id": "C66",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "We know that, despite their bird-like appearance, bats are mammals. How does the brain learn such exceptions? We conducted an fMRI study where participants categorized stimuli, including exceptions. We analyzed learning behavior using prototype and exemplar learning models. Initially, both models equally predicted behavior, but the exemplar model later outperformed the prototype model. Combining stimulus similarity patterns from the learning models with a whole-brain searchlight using representational similarity analysis (RSA), we found that the prototype model matched neural patterns in frontal and temporal regions, while the exemplar model matched patterns in the visual cortex. Our findings support the idea of flexible recruitment of multiple learning systems involved in concept learning, based on task demands."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Dynamic Multi-level Learning in a Control-Learning Environment",
    "presenter": "Junwu Chen",
    "poster_id": "C67",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Humans are often confronted with multiple learning contingencies in real-life situations. Previous studies suggested that people tend to learn on different levels of abstraction sequentially rather than in parallel. In a reward learning environment where reward is only contingent on the task sequence (task switching/repetition), we employed a hierarchical reinforcement learning (HRL) model to investigate if individuals dynamically shifted the level at which they choose to learn over time. Our modelling analyses suggested that participants gradually shifted their priority from task-level learning to learning task sequences. Together, our findings are consistent with the sequential multilearning hypothesis."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "What does spatial tuning tell us about the neural computation in the hippocampus?",
    "presenter": "Maxime Daigle",
    "poster_id": "C68",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "The hippocampus has long been regarded as a neural map of physical space, with its neurons categorized as spatially or non-spatially tuned according to their response selectivity. However, growing evidence suggests that this dichotomy oversimplifies the complex roles hippocampal neurons play in integrating spatial and non-spatial information.\nThrough computational modeling and in-vivo electrophysiology in macaques, we show that neurons classified as spatially tuned, primarily encode linear combinations of spatial and non-spatial features, while those labeled as non-spatially tuned rely on nonlinear mechanisms. Moreover, we demonstrate that nonlinear recurrent connections are crucial for capturing the response dynamics of non-spatially tuned neurons. \nThese findings challenge the traditional dichotomy of spatial versus non-spatial representations and instead suggest a continuum of linear and nonlinear computations."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Task-Relevant Information is Distributed Across the Cortex, but the Past is State-Dependent and Restricted to Frontal Regions",
    "presenter": "Lubna Shaheen Abdul",
    "poster_id": "C69",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Cognitive flexibility is the ability to adapt our decisions to changing demands (Braem \u0026 Egner, 2018). This behavioral feat requires matching internal states to ongoing task demands (Ashwood et al., 2022). Here, we examine the behavior and single-unit activity recorded simultaneously from 7 regions of the monkey brain (PFC, FEF, LIP, MT, V4, IT, and parietal cortex) while two monkeys performed a context-dependent decision-making task. The relevant task was cued by a specific shape on every trial, indicating whether color or motion was relevant for decision making. Using Hidden Markov Models, we identified three latent cognitive states that captured dynamic shifts in engagement and strategy throughout the task (Hulsey et al., 2024). Specifically, we found two context-dependent states, marked by high accuracy in one context. Additionally, we also found a context-independent state (“default state”) in which both stimuli were integrated and marked by higher lapse rates.  Previous-trial stimuli and responses biased current-trial responses in all states, but in opposite directions and stronger in the default state. Preliminary neural decoding analysis reveals that current stimuli are broadly represented across cortical areas, while past trial features are only encoded in frontal regions (PFC and FEF). Together, these findings highlight the role of higher-order regions and internal cognitive states in shaping perceptual decisions."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Collaborative Encoding of Visual Working Memory",
    "presenter": "Huang Ham",
    "poster_id": "C70",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Collaboration helps humans surmount individual cognitive limitations by distributing information over many minds. However, figuring out when and how to collaborate is not trivial. This study examines whether dyads split up information in a collaborative visual working memory task when doing so improves performance. Participants (N=356) memorized grids of 4, 16, or 36 images both alone and with a partner. We used a visual working memory model to estimate how much dyads would benefit from splitting up a grid of images, rather than each memorizing the grid independently. Our model predicts that participants should split up grids that are neither too easy nor too difficult to benefit from collaboration. Indeed, participants tacitly adopted conventions to split up medium and large grids---and were more accurate in these conditions when they worked together than when they acted alone---but not small grids where individual performance was already at ceiling. Our work provides a first step to understand how decisions about when and how to collaborate are shaped by the adaptive use of cognitive resources."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Good and consequential counterfactual outcomes are prioritized during learning",
    "presenter": "Kate Nussenbaum",
    "poster_id": "C71",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "People can learn from actions not taken by leveraging mental models to imagine their potential consequences. However, for any given choice, the number of possible, alternative actions often exceeds the brain's capacity for simulation. Here, we develop a new task to measure behaviorally whether people selectively prioritize the counterfactual updates that are most likely to improve their future decisions. Our initial results (N = 69) indicate that people most strongly consider high-magnitude alternatives as well as those that are better than the option they selected, suggesting that people do indeed consider alternative possibilities strategically."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Experience supports performance by abstraction learning in recurrent networks",
    "presenter": "John C Bowler",
    "poster_id": "C72",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Our prior experience affects the strategies we adopt during future problem solving, however, in complex problem spaces it can be difficult to isolate the key features of past experience that are critical to future progress. Therefore, we asked: how does past experience alter cognition in ways that facilitate (or hinder) future task performance? We trained Recurrent Neural Networks (RNNs) to model a complex odor timing task, using constraints derived from prior reports detailing mouse behavior and shaping procedures. RNNs subject to well designed pre-training develop lower dimensional network activity and learn a key abstraction about the temporal structure of the task, resulting in improved future performance after training on the full task. The compositional nature of learning suggests that assembling fundamental building blocks from past experiences is essential for future problem solving; however, we demonstrate that training on arbitrary sub-components of the full task is insufficient to aide learning. We replicate these findings in both the behavior and neural dynamics of mice performing the task. Additionally, analysis of the dynamical mechanisms that RNNs learn after shaping predicted unanticipated responses to novel trial types that may translate to animal behavior–which we confirm experimentally."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Computational Model for Episodic Timeline Based on a Spectrum of Synaptic Decay Rates",
    "presenter": "Zoran Tiganj",
    "poster_id": "C73",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Human episodic memory enables the retrieval of temporally organized past experiences. Retrieval cues can target both semantic content and temporal location, reflecting the multifaceted nature of episodic recall. Existing computational models provide mechanistic accounts for how temporally organized memories of the recent past (seconds to minutes) can persist in neural activity. However, it remains unclear how episodic memories can be stored and retrieved while preserving temporal structure within and across episodes. Here, we propose a computational model that uses a spectrum of synaptic decay rates to store temporally organized memories of the recent past as an episodic timeline. We characterize how the memories can be retrieved using either a memory of the recent past, specific semantic cues, or temporal addressing. This approach thus bridges short-term working memory and longer-term episodic storage, offering a computational model of how synaptic dynamics can maintain temporally structured events."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Novelty-seeking Guides Formation of Disentangled Representations",
    "presenter": "Pingsheng Li",
    "poster_id": "C74",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Disentangled representations are common in the brain, where many neurons are tuned to single factors of task variation, such as place cells or object-vector cells. Previous work has shown that neural networks trained with biological constraints can also learn disentangled representations if trained on disentangled data, i.e., data generated by independent factors. However, in real-world, open-ended environments, such neatly disentangled data may not always be available. This raises a fundamental question: how can agents collect experiences that help them form disentangled representations? Intrinsic motivations, such as novelty, efficiently guide humans and artificial agents during exploration of unfamiliar environments but it is unclear whether they also support disentangled representation learning. Using a novel method to extract representation-specific novelty signals, we compute novelty signals from the latent representations of autoencoders (AEs) and discrete variational autoencoders (D-VAEs) and use them as intrinsic exploration rewards for an artificial agent performing unsupervised learning. We show that these novelty signals favor exploration of disentangled over entangled data, and help the agent learn disentangled representations."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Interplay of social and self interests during learning in early adolescence",
    "presenter": "Cong Wang",
    "poster_id": "C75",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Navigating social environments requires individuals to consider how their actions impact both themselves and others, and to dynamically adjust expectations and actions in ways that satisfy social goals. Yet, little is known about how self-interest and social interests interact to shape learning in early adolescence—a time of significant social development. Here, we studied how 13- and 14-year-olds learn from outcomes relevant to both themselves and others. Compared with self-regarding learning, learning from social outcomes was generally weaker but exhibited substantial individual differences. These variations were captured by an error-driven learning process incorporating individual-level social preferences, supported by a social preference-weighted prediction error encoded in frontoparietal network. These data suggest a neurocomputational mechanism by which early adolescents reconcile multiple, and sometimes competing, social motives during learning."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A closed-loop model for the coordination of gaze control and decision-making",
    "presenter": "Tianming Yang",
    "poster_id": "C76",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "The study of neural circuitry in visually-guided decision-making has generated extensive research and theoretical models of decision formation. However, the role of gaze in enhancing focal sampling and facilitating shifts to sample alternatives remains unclear. We propose a closed-loop model that integrates decision formation with gaze signals to enhance the sampling of visual options. Visual input is first projected to decision populations, implementing competition via mutual inhibition. The output from these populations drives gaze populations, generating visual shifts that feedback into the decision process.\nWe present simulations based on a two-alternative bundle task, showing that gaze and decision outputs align with behavioral performance in terms of decision accuracy and gaze shift occurrences. The model can be extended to study the role of gaze in decision-making both at behavioral and at neural levels, to test predictions such as whether fewer gaze shifts correlate with shorter reaction times, or if gaze shifts coincide with changes in neural value encoding. Additionally, the model can be applied to explore whether gaze and decision align in sequential alternative presentations or when distractors are present."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Uncovering the Structure of Trial-to-Trial Variability in Perceptual Decision-Making Using Disentangled Recurrent Neural Networks",
    "presenter": "Anne Urai",
    "poster_id": "C77",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Perceptual decision-making shows considerable trial-to-trial variability, which can be captured by latent variable models of fluctuating decision strategies. Recently, disentangled Recurrent Neural Networks (DisRNNs) have achieved data-driven discovery of such trial-to-trial latent decision strategies in multi-armed bandit tasks. \nIn this work, we investigate the applicability of DisRNNs for uncovering trial-to-trial structure of perceptual decision-making data. We fit DisRNNs on simulated Diffusion Decision Model (DDM) data, where the starting point or drift parameters depend on past choices. We show that the trace of the starting point and drift can be recovered in the latent variables, and that the shape of the trial-to-trial dependency of these parameters can be interpreted from the update rules learned by the network. This sets the stage for data-driven discovery of the sources of across-trial variability in real perceptual data."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Tracking of dynamic neural representations  during goal-directed action",
    "presenter": "Jet Lageman",
    "poster_id": "C78",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Theoretical accounts of goal-directed action disagree on whether actions are represented by their motor commands or sensory outcomes. Here we use a novel analysis technique combining linear decoding of EEG data with representational similarity analysis to shed light on this matter. Preliminary findings (n=5) show that it is possible to track neural conjunctive representations that integrate goals, actions, and sensory outcomes over time during goal-directed action, which can be related to learning outcomes. These findings help pave the way towards a mechanistic understanding of how the brain plans actions to reach goals."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Correspondence between reinforcement learning phenotypes and transdiagnostic clinical symptomatology across development",
    "presenter": "Noam Goldway",
    "poster_id": "C61",
    "topic_area": "Memory, Spatial Cognition and Skill Learning",
    "abstract": "Adolescence is marked by neurocognitive changes and heightened vulnerability to psychiatric disorders. Yet, it remains unclear how age-related changes in learning processes known to contribute to adult psychopathology may contribute to the emergence of psychiatric symptoms. In a sample of 889 individuals aged 10–25, we related reinforcement learning computational phenotypes to psychopathology measured along multiple dimensions of dysfunction. Participants completed three learning tasks targeting sensitivity to positive versus negative outcomes, Pavlovian bias (automatic/innate versus instrumental/arbitrary), and model-based control (goal-directed planning). Factor analysis of self-report questionnaires identified four symptom dimensions: rumination, physiological anxiety, impulsivity/dysregulation, and anhedonia. Symptom expression varied with age, and each symptom domain demonstrated a distinct computational signature."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A cognitive map of a subjective value space",
    "presenter": "Mark A Orloff",
    "poster_id": "C79",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Individuals are thought to make choices based on subjective valuations of options that combine multiple attributes into a unified subjective value (SV) signal in the brain. However, it is not yet known how the multiple attributes of choice options are transformed into SV. One possible mechanism is the cognitive mapping system in the entorhinal cortex (EC) and medial prefrontal cortex (mPFC), which efficiently represents relational, multi-dimensional information. Here, we develop  a novel risky decision-making task and use fMRI to show that a two dimensional (2D) SV space of reward probability and amount is represented in the cognitive mapping system as both a grid-like representation of decision vectors and a 2D ‘positional’ code of options. Further preliminary work using the same task in intracranial EEG (iEEG) shows theta coupling between medial temporal lobe (MTL) and PFC and a grid-like representation of decision vectors in mPFC theta power. These findings connect the brain’s cognitive mapping and valuation systems and provide a possible mechanism by which individuals convert an options’ multiple attributes into an  SV signal, suggesting a new framework for understanding how the brain constructs and compares values."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "An Excessive-Demand Measure Outperforms Other Demand Proxies in Explaining Lab Asset-Market Price Changes: Toward a Brain Biomarker of Excessive Demand",
    "presenter": "John L. Haracz",
    "poster_id": "C80",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Dynamic stochastic general equilibrium models have been criticized for failing to forecast the Global Financial Crisis. This and other flaws of neoclassical economics are proposed to arise partly from the failure of equilibrium-based models to capture excessive demand, which exceeds the balanced excess demand in general equilibrium theory. Excessive demand is defined as demand that promotes disequilibria in asset or goods markets and drives prices above fundamental values (e.g., an asset-price bubble). Neuroimaging studies are elucidating the neuroeconomics of asset-price bubbles. However, these studies have been limited in characterizing individual-level brain-behavior relationships due to the lack of a subject-level excessive-demand measure (EDM). The present study makes such a measure available. In a standard lab asset-market design, 9 experiments each included 9 subjects. Experiments consisted of 15 2.5-minute periods of trading cash and a risky asset. To capture excessive demand, the end of each Period 1-14 was followed by a survey that elicited each subject’s number of asset shares that they want to hold at the end of the next period. This measure was designed to tap into anticipatory affect that may drive price bubbles. Two other predictive measures included excess bids and price-change momentum. A market-level EDM, which explained 34.5% of the variance in asset-price changes, outperformed the excess-bids and momentum measures, which each explained less than 10% of this variance. The EDM’s outperformance in predicting price changes aligns with numerous other findings that underscore the predictive power of measures related to anticipatory affect. For example, neuroimaging-measured activity in nucleus accumbens, an area implicated in anticipatory affect, performed better than choice behavior in forecasting crowdfunding outcomes. Similarly, the survey-elicited EDM, which may reflect anticipatory affect, was a better price-change predictor than the behavioral excess-bids measure. Therefore, the presently introduced EDM may facilitate finding an excessive-demand biomarker with market-price predictive power."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Social context affects policy and counterfactual learning in a two-armed bandit task",
    "presenter": "Amric Trudel",
    "poster_id": "C81",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Social contexts shape decision making under uncertainty. While reinforcement learning models explain how individuals learn from rewards, it remains unclear how this process is affected when rewards come from other agents. Here we introduce a novel social decision-making task based on the two-armed bandit paradigm, which allows the isolation of social learning mechanisms while keeping reward structure exactly matched between social and non-social contexts. Model-free analyses revealed increased behavioral switching and reduced blind repetition in the social condition. A reinforcement learning model incorporating a counterfactual learning parameter revealed that the social context primarily altered policy parameters and counterfactual updating, suggesting participants imagined a competitive dynamic between their partners. Our findings indicate that while learning mechanisms about chosen options remain relatively stable across conditions, social framing seems to shift how people explore and infer from unchosen outcomes. This advances our understanding of the cognitive architecture underpinning human social learning."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Affective features drive human representations of social touch expressions during naturalistic viewing",
    "presenter": "Haemy Lee Masson",
    "poster_id": "C82",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Touch is a potent communication tool. It has been suggested that a wide range of factors impact how we perceive social touch. No study has investigated which features we attend to when observing complex, naturalistic social touch expressions. To address this question, we curated 125 video clips showing a wide range of social interactions from the American TV series, Modern Family. Eighty participants watched 45 pseudo-randomly selected videos and performed a multiple arrangement task. This procedure produced the group-averaged pairwise similarity judgements for social touch expressions. Visual, social, and affective features were extracted from each video clip using artificial neural networks (ANN), behavioral experiments, and human annotations. The combination of multiple regression and variance partitioning analyses revealed that selected affective, social, high-level visual, and ANN features collectively explained 52% of the variance in the perception of social touch expressions. Among these, affective features uniquely accounted for 33% of the variance. The current findings suggest that affective features, specifically whether the touch is used to convey positive or negative emotions, drive human perceptions of social touch during naturalistic viewing. Conversely, ANN features explained the least variance, suggesting that the models, trained on action perception and facial expression recognition, may not be sufficient to decode social touch expressions."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Attention in value-based choice: Active and passive uncertainty reduction mechanisms",
    "presenter": "Ruby Schlayer-Westbrook",
    "poster_id": "C83",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "The more one attends to an option, the more likely one is to choose it. This gaze benefit has been formalized as a discounting of unattended options in influential models of decision-making.Recent work shows that the attention benefit vanishes when attention is manipulated externally rather than freely allocated. Here, we explicitly compare free viewing condition, and externally manipulated attention (sequential presentation). Consistent with model predictions, we find strong gaze effects on choice only in free viewing, where people prioritized higher value options. Importantly, this attention prioritization is associated with greater choice efficiency, with more consistent and faster choices. Taken together, our results highlight two ways in which attention shapes choice. First, attention affords option-level value uncertainty reduction\nby supporting information sampling and value estimation on any attended options. Second, attention affords efficient choice-level uncertainty reduction through prioritization of goal-relevant options."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Context-dependent modulations of sensory adaptation and pupil-linked arousal as substrates of flexible decision-making",
    "presenter": "Kara D. McGaughey",
    "poster_id": "C84",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Visual decisions typically require the accumulation of uncertain sensory evidence over time. To be effective in the real world, where evidence is often uncertain and unstable, the dynamics of this accumulation process must be flexible. Here, we examined sources of this flexibility by leveraging simultaneous psychophysics, electrophysiology, and pupillometry in non-human primates performing a visual decision-making task. We found that the rate of change of recent stimulus statistics (i.e., context stability) affected both the degree of sensory adaptation in single neurons encoding relevant sensory evidence and the dynamics of pupil diameter. These changes in evidence encoding and pupil-linked arousal depended on whether or not the monkeys tended to use flexible evidence accumulation during a given session, indicating a relationship between sensory adaptation, arousal, and evidence-integration behavior. Collectively, these findings demonstrate that context stability can affect both “bottom-up” evidence encoding in cortex and “top-down” arousal-related modulations that work together to support flexible decision-making behavior."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Frontostriatal dynamics modelling obsessions and compulsions",
    "presenter": "Sebastien Naze",
    "poster_id": "C85",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "The frontostriatal system support several processes related to attention, habits, emotional regulation and goal directed behaviours. Preclinical and clinical work suggests that obsessive-compulsive disorder (OCD) maps onto a functional imbalance in the ventral and dorsal frontostriatal circuits. However, the neural mechanisms supporting these dysregulations remain elusive, their association with symptom severity is unclear, and therapeutic interventions are limited. We here combined neuroimaging and behavioural data from individuals with OCD and controls with computational modelling. We found that bidirectionally decreasing neural coupling in the ventromedial circuit while concurrently increasing dorsolateral cortico-striatal coupling delivered the highest functional improvements in OCD. The analysis of longitudinal changes in obsessions and compulsions with respect to modelled neural interventions supported our predictions. This study highlights behaviourally meaningful neural mechanisms hidden from traditional neuroimaging analysis to better understand the neural basis of OCD and provide new therapeutic targets for obsessions and compulsions."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Exploring the Neural Basis of Stimulus-Driven Overconfidence",
    "presenter": "Ema Zezelic",
    "poster_id": "C86",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "In decision-making, confidence is the ability to judge how likely our choices are to be correct or incorrect. Whilst this internal sense of accuracy can be a useful tool with which to adapt our behaviour, it is not always reliable. In fact, certain visual manipulations can lead observers to feel more confident even when the likelihood of correct choices remains the same.  Here we used such a manipulation combined with  magnetoencephalography, to investigate the neural basis of the “positive-evidence bias”.  Participants performed a visual decision-making task with confidence judgements, in which we induced overconfidence in one of two conditions, while keeping accuracy between the conditions the same.  We found evidence that the observed behavioral overconfidence could be explained by increased separation and variance of neural evidence representations and is not necessarily due to a higher-level cognitive bias."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Drift Rate Reflects Value, Response Bias Reflects Habit: A DDM Analysis of the Reward Pairs Task",
    "presenter": "Viktor Timokhov",
    "poster_id": "C87",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Human behavior is guided by both goal-directed and habitual systems. While reinforcement learning (RL) models have captured these influences using choice data, integrating RL with sequential sampling models like the drift-diffusion model (DDM) enables modeling of both choices and response times (RTs). In this study, we tested whether value differences modulate the DDM drift rate and whether prior choice frequency affects response bias simultaneously. Using data of 213 participants in the Reward Pairs Task - an instrumental learning paradigm that independently manipulates stimulus value and choice frequency - we applied hierarchical DDM modeling with collapsing boundaries. Results showed that the best-fitting model captured both value-based and habit-based influences: drift rate scaled with value differences, and response bias reflected differences in choice frequencies. Posterior predictive checks confirmed alignment with observed behavior. These findings support a dual-process view of decision-making, showing that goal-directed and habitual factors influence choice and decision speed via distinct mechanisms."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Simultaneous modeling of behavior and dopamine with disentangled RNNs",
    "presenter": "Siddhant Jain",
    "poster_id": "C88",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Understanding how neural activity relates to cognitive processes during learning requires methods that can jointly model brain signals and behavior. Here, we extend Disentangled RNNs (DisRNN), an approach for using constrained recurrent neural networks to discover cognitive models, to the case of jointly modeling behavioral data and measurements of neural activity. We augment a DisRNN trained on choice prediction with a separate subnetwork to predict neural activity. We apply this approach to datasets from a simple reward-learning task, consisting of choices, rewards, and a scalar measure of dopamine responses to reward. First, using synthetic data from a Q-learning agent, we demonstrate the approach is able to capture both choices and reward prediction errors with a single set of internal variables, consistent with the groundtruth. Next, we apply this approach to laboratory data from mice performing a similar task, successfully modeling both choice behavior and nucleus accumbens dopamine responses. Analysis of the fit DisRNNs confirms that the same interpretable latent variables are utilized for both choice prediction and dopamine signal prediction, demonstrating the model's potential to uncover cognitive models that bridge behavior and neural data through shared representations."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Working Memory-Supported Reinforcement Learning Related to Mental Health Phenotypes in a Representative Sample",
    "presenter": "Laura Ana Bustamante",
    "poster_id": "C90",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Working memory-supported reinforcement learning (RL-WM) may differ between participants with and without schizophrenia and mood disorders. It remains unexplored whether such processes are related to distinct dimensional phenotypes. This study tests whether RL-WM parameters are associated with self-reported mental health phenotypes (anxiety, depression, drug use, impulsivity, mania, motivation/pleasure, schizotypy) in a US representative online sample (N=2,300 exploratory). Participants completed a stimulus-response learning task that manipulates WM demands (set size, delay) to disentangle WM and RL contributions to performance (N=1,665 post-exclusion, ages 18-65 years). The RL-WM model estimated participants' reliance on WM (v. RL) reliance, WM capacity, WM decay (interference from intervening trials), RL learning rate, perseveration (negative feedback neglect), and undirected noise (attention lapse) parameters. We tested associations between 8 RL-WM measures and survey items using multivariate sparse partial least squares regression (m-SPLS). m-SPLS Cross-validation identified 1 component with 31 survey items predicting 7 RL-WM measures. Items reflecting schizotypy, mania, and others, were related to reduced RL and WM performance (increased perseveration, WM decay, undirected noise, decreased WM reliance). Results support RL-WM utility for computationally mechanistic community mental health research. Replicability will be tested in an independent sample (N=6,725 confirmatory)."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Analogous neural representations underlying risky decision making in deep reinforcement learning agents and humans",
    "presenter": "T. Alexander Price",
    "poster_id": "C89",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "We employed deep reinforcement learning to discover behavioral and neural strategies underlying a spectrum of performance on a risky decision- making task. Working backwards, we identified analogous behavior from a large cohort of neurosurgical patients from whom we recorded single neuron activity in decision making circuits. Examining low dimensional factors in neuron population activity uncovered temporal and trial factors differentiating task performance groups, with improved task performance being associated with more nonlinear neural representations of reward prediction."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Structural abstraction of emotion knowledge in hippocampal-prefrontal systems",
    "presenter": "Yumeng Ma",
    "poster_id": "C91",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Hippocampal-prefrontal systems organize knowledge in a map-like way across multiple domains, from physical space to abstract concepts. It is well known that knowledge about emotional events is organized in a low-dimensional space, raising the question of whether the brain uses cognitive maps to represent emotion concepts. Using functional magnetic resonance imaging while participants viewed emotionally evocative film clips, we decoded patterns of neural activity in hippocampal-prefrontal systems to predict representations of emotion concepts in a computational model of relational memory inspired by the hippocampal formation. Our findings demonstrate that hippocampal-prefrontal systems contain map-like representations of emotion concepts at multiple levels of granularity. These findings provide new insight into how the brain organizes knowledge about emotional events to react adaptively in a complex environment."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Neural Representation of Social Relationship Graphs through Multidimensional Modeling of Dynamic Social Interactions",
    "presenter": "Dasom Kwon",
    "poster_id": "C93",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Social interactions continuously evolve, shaping our understanding of interpersonal relationships. Yet, how does the human brain construct relational knowledge from such dynamics? Prior research has primarily relied on unidimensional co-occurrence metrics, failing to capture the complexity of real-world social dynamics. Here, we introduce a multidimensional modeling framework characterizing dynamic social interactions as valence-weighted graphs. Using fMRI data collected during movie-viewing and subsequent relationship rating tasks, we show that distributed brain regions track dynamic interactions and represent social relationship graphs. These representations were preserved across tasks, with higher dimensionality observed in the medial prefrontal cortex (mPFC) and lower dimensionality in the posterior superior temporal sulcus (pSTS). These findings bridge online social perception and structured relational knowledge, elucidating how the brain organizes dynamic social interactions into multi-layered interpersonal relationship graphs."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A single general strategy supports flexible behavior in response to both gradual and sudden changes",
    "presenter": "Jessica Passlack",
    "poster_id": "C94",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Most decisions we make occur in an ever-changing world, where flexibility is crucial for making intelligent decisions. However, despite a large array of different behavioral tasks being used to investigate flexible behavior, existing research has focused primarily on individual tasks in isolation, preventing understanding of whether there is a general strategy underlying flexible behavior. Here, we designed a set of tasks that spanned two hallmarks of behaviors that require flexible decision making: gradual and sudden changes. We found that behavioral correlates were similar across both types of changes and that a neural network could learn a general strategy to support behavior. Our results indicate that a neural network approach in combination with across-feature task design allows for determining whether a general underlying strategy supports different aspects of flexible behavior. This opens the door for interrogating the building blocks of the general strategy underlying flexible behavior."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Neural computations underlying human social evaluations from visual stimuli",
    "presenter": "Manasi Malik",
    "poster_id": "C95",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Humans easily make social evaluations from visual scenes, but the computational mechanisms in the brain that support this ability remain unknown. Here, we test two hypotheses raised by prior work: one proposes that people recognize and evaluate social interactions by inverting a generative model of the world and reasoning about others’ mental states; the other suggests that this process relies on bottom-up visual perception without explicit mental state inference. In this preregistered study, we collected fMRI responses from participants watching videos of social interactions and compared these neural responses to computational models that instantiate these different theories: a generative inverse planning model (SIMPLE) and a relational bottom-up visual model (SocialGNN). Using representational similarity analysis, we find that perceptual social processing regions — such as regions in pSTS and LOTC — are significantly similar to SocialGNN, even after controlling for SIMPLE and low-level motion features. Further, a non-relational visual control model failed to explain neural responses in these regions. SIMPLE also explained neural responses in similar regions, but effects were weaker and largely accounted for by SocialGNN. These findings suggest that regions in pSTS and LOTC may support relational bottom-up computations during social interaction recognition."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Emergent Reciprocity Through Temporal Credit Assignment in Reinforcement Learning Agents",
    "presenter": "Le Thuy Duong Nguyen",
    "poster_id": "C92",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Humans and animals have complex social and cultural systems that can span large distances and times. In North America, Plains Indigenous nations practiced reciprocity through Manitokan, effigies placed at fixed locations, where leaving surplus goods was seen as cooperative acts of resource sharing or caching. In multi-agent reinforcement learning (MARL), reciprocity has largely been defined as an emergent property through tit-for-tat policies or reputation scores that establish social norms. These perspectives fail to consider the temporal structure of rewards or the criticality of certain actions necessary for success. We present a novel MARL environment to investigate the emergence of reciprocal prosocial behaviours in reinforcement learning agents. Baseline experiments show that agents consistently converged to suboptimal policies favoring individual resource maximization, despite the potential for improved collective outcomes. These findings highlight a critical gap in existing MARL methods, suggesting the need for new algorithms capable of supporting temporal credit assignment in artificial agents."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Reward-Prediction-Error-Guided Attention Explains Behavioral Learning Curves",
    "presenter": "Mingze Li Leukos",
    "poster_id": "C96",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Attention and learning are highly intertwined: past experiences determine where attention is focused at present and the focus of attention guides future experiences. In the context of reinforcement learning (RL), previous work has demonstrated how reward feedback can be used to learn a value function-based attention template (Jahn et al., 2024). Many open questions remain, however, regarding the exact way in which internal value estimates guide attentional modulation in the visual system. We explore these questions by building a perceptual model where top-down feature-selective attention is determined by an internal value function. We explore several different forms the relationship between value and attention can take in this model. We find that, to fit the unique features of the behaviorally-observed learning curve, attention should be focused on the color with highest estimated value and its strength should be inverted after large negative prediction errors. This work gives us a compact description of a latent process relating two important cognitive variables and sets the groundwork for exploring how the relationship between reward feedback and attention may vary under different tasks."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Feedback signals can explain spike-silent working memory traces in visual cortex",
    "presenter": "Noa N. Krause",
    "poster_id": "C104",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Storage of visual memories is known to engage early visual cortex (EVC), where mnemonic representations are presumed to be spike-silent. Possibly, such spike-silent representations rely on short-term synaptic plasticity (STSP) traces laid down by previous sensory inputs. However, STSP cannot account for selection of information from working memory on the basis of external information, and is not robust against visual distraction. As such, it fails to explain data from neuroimaging studies in humans, which shows that representations recovered from EVC are mnemonic in nature, that is, only task-relevant information can be recovered. Additionally, these traces return even after temporary quenching from visual distraction. We show that feedback projections from more anterior cortical sites, known to reflect memories via sustained spiking, can explain these findings that STSP cannot account for."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A computational model of reward learning and habits on social media",
    "presenter": "Georgia Turner",
    "poster_id": "C97",
    "topic_area": "Reward, Value and Social Decision Making",
    "abstract": "Social media have fundamentally transformed how we live and communicate. However, the methods to study how our cognitive systems interact with technology platforms are very limited. Computational modelling represents a new avenue to uncover the finegrained cognitive processes driving social media behaviour. Here, we develop a novel computational model of real-world social media posting data, adapted from the animal reward learning literature. We fit seven models to a Twitter dataset (n=2,696 users), including a preregistered replication, and show that a hybrid goal-directed and habitual reward-seeking process underlies social media posting behaviour. More frequent posters show signs of more habitual behaviour. Our model paves the way for large-scale investigation into the cross-species cognitive processes motivating social media behaviours, and their downstream impacts on individuals and society."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Sample, Don't Assume: Unconstrained Receptive Field Estimation",
    "presenter": "Niklas Müller",
    "poster_id": "C105",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "The spatial tuning of (populations of) neurons, as reflected in their (population) receptive field (pRF), is one of the most fundamental properties determining neural responses in visual cortex. pRF geometry is typically modeled as a 2D isotropic Gaussian, effectively assuming the pRF samples a circular 'aperture' in the visual field. However, it has been found that using a more complex geometry can improve neural predictions. Thus, it remains unclear what assumptions to make about the geometry of pRFs. Here, we show that removing any geometrical assumptions, and instead estimating pRFs in a fully data-driven way, leads to significant improvements in neural predictions. We combine linear encoding models with random sampling of pixels from feature maps of convolution deep neural networks to estimate unconstrained pRFs from monkey multi-unit electrophysiology recordings. Our new method not only improves neural predictions but also allows for both quantitative pRF mapping (parameter estimation) and qualitative inspection of the pRF geometry from the obtained pixel importance maps."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images",
    "presenter": "Jesus Malo",
    "poster_id": "C107",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "A number of scientists suggested that human visual perception may emerge from image statistics, shaping efficient neural representations in early vision. In this work, a bio-inspired architecture that can accommodate several known facts in the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for different tasks related to image reconstruction: autoencoding, denoising, deblurring, and sparsity regularization. Our results show that the encoder stage (V1-like layer) consistently exhibits the highest correlation with human perceptual judgments on image distortion despite not using perceptual information in the initialization or training. This alignment exhibits an optimum for moderate noise, blur and sparsity. These findings suggest that the visual system may be tuned to remove those particular levels of distortion with that level of sparsity and that biologically inspired models can learn perceptual metrics without human supervision."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A multimodal encoding model for predicting human brain responses to complex naturalistic movies",
    "presenter": "Viacheslav Fokin",
    "poster_id": "C106",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Accurately modeling human brain responses to complex, multimodal sensory inputs remains a core challenge in computational neuroscience. The Algonauts Project 2025 provides a large whole-brain fMRI dataset collected during naturalistic movie viewing, presenting the challenge of improving multimodal encoding models. Here, we propose a biologically informed encoding model that integrates visual, auditory, and language features, extracted using established deep learning and signal processing approaches, within predefined functional connectivity (FC) networks. By applying predictive modeling within an FC-based cortical mask, we achieve a 45.39% performance gain over the full cortex baseline. Our findings demonstrate the value of incorporating functional brain organization into encoding models and lays a foundation for future biologically grounded AI systems that integrate sensory information across domains."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Evaluating Human-Machine Representational Alignment in Hyperbolic Space",
    "presenter": "Otto Béla Márton",
    "poster_id": "C109",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "While humans naturally organize concepts hierarchically, this characteristic remains poorly represented in many deep neural networks (DNNs). This may lower generalisability and could cause DNNs to fail in unexpected ways. Virtually all DNNs make use of Euclidean geometry, but hyperbolic geometry is more naturally suitable for hierarchical structures. Using the THINGS dataset of\nhuman similarity judgments of object triplets, we examine the alignment between humans and Euclidean versus hyperbolic models, including both a hyperbolic version of a task-optimized DNN (CLIP) and a hyperbolic adaptation of sparse positive embeddings trained directly on the human behavioural data. Confirming the suitability of hyperbolic geometry, we find that the hyperbolic models predict human behavioural similarity judgments significantly better than their Euclidean counterparts."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Differentiating image representations in terms of their local geometry",
    "presenter": "David Lipshutz",
    "poster_id": "C108",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Similarity between neural representations is often quantified by measuring alignment of the representations over a set of natural stimuli that are relatively far apart in stimulus space. However, systems with similar global structure can have strikingly different sensitivities to local stimulus distortions, suggesting a need for metrics that compare local sensitivities of representations. We propose a framework for comparing a set of image representations in terms of their sensitivities to local distortions. We quantify the local geometry of a representation using the Fisher information matrix, a standard statistical tool for characterizing the sensitivity to local stimulus perturbations, and use this to define a metric on the local geometry of representations near a base image. This metric may then be used to differentiate a set of representations, by finding a pair of ``principal distortions'' that maximize the variance of the representations under the metric. We apply our method to models of the early visual system and to a set of deep neural network (DNN) models."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Deep layers of primary visual cortex encode postdictive perception",
    "presenter": "Pieter Barkema",
    "poster_id": "C111",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Neural representations in primary visual cortex (V1) are not solely determined by bottom-up retinal inputs, but also reflect top-down modulations, where bottom-up and top-down signals are reflected in different cortical layers. Subjective perception is thought to reflect the integration of these signals. Here, we investigated whether this mechanism generalizes to postdictive perception, when later information affects the perception of earlier sensory input. A feedback hypothesis predicts this temporal integration reaches early sensory cortex, while a feedforward hypothesis predicts that this sensory input is integrated downstream. We induced a postdictive visual illusion with sound and hypothesized that multisensory regions would feed back postdictive information into V1. We tested this hypothesis using layer-specific 7T fMRI (N=24), retinotopic mapping and a postdictive illusion paradigm. We validated the illusion and retinal effects and found no evidence for univariate BOLD increase. Using multivariate analysis, however, we found that activity patterns in the deep, but not the middle, layers of V1 reflected the contents of illusory percepts, in line with the feedback hypothesis. Informational connectivity analyses revealed that this information was shared with the Superior Temporal Gyrus, a multisensory hub. These results reveal that perceptual inference in primary visual cortex can be modulated by top-down information arriving after the fact."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Learning to cluster neuronal function",
    "presenter": "Nina S. Nellen",
    "poster_id": "C110",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Deep predictive models have recently shown great potential to create digital twins to predict neuronal activity in the visual cortex.\nThese models provide per-neuron embeddings, which have been proposed as a basis to identify functional cell types. \nHowever, so far no clear clusters have been observed in the mouse visual cortex and the structure of the embedding space is not highly reproducible across independent model fits.\nTo address these problems, we build upon state-of-the-art predictive networks and introduce an explicit inductive bias to enhance cluster separability.\nIf functional cell types exist, such a clustering bias should improve model performance and consistency of clustering.\nOur approach is based on training a predictive model and adding an auxiliary loss function that encourages the per-neuron embeddings to be distributed according to a $t$ mixture model.\nWe jointly optimize both neuronal feature embeddings and clustering parameters.\nOur approach improves consistency of clusters and therefore leads to more consistent embedding spaces across models."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Predictive remapping and allocentric coding as consequences of energy efficiency in recurrent neural network models of active vision",
    "presenter": "Philip Sulewski",
    "poster_id": "C112",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Despite moving our eyes from one location to another, our perception of the world is stable - an aspect thought to rely on predictive computations that use efference copies to predict the upcoming foveal input. Are these complex computations genetically hard-coded, or can they emerge from simpler principles? Here we consider the organism's limited energy budget as a potential origin. We expose a recurrent neural network to sequences of fixation patches and saccadic efference copies, training the model to minimise energy consumption (preactivation). We show that targeted inhibitory predictive remapping emerges from this energy efficiency optimization alone. As furthermore demonstrated, this computation relies on the model's learned ability to re-code egocentric eye-coordinates into an allocentric (image-centric) reference frame. Together, our findings suggest that both allocentric coding and predictive remapping can emerge from energy efficiency constraints during active vision, demonstrating how complex neural computations can arise from simple physical principles."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Foveated sensing in KNN-convolutional neural networks based on isotropic cortical magnification",
    "presenter": "Nicholas Blauch",
    "poster_id": "C114",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Human vision prioritizes the center of gaze through spatially-variant retinal sampling, leading to magnification of the fovea in cortical visual maps. In contrast, deep neural network models (DNNs) typically operate on spatially uniform inputs, limiting their use in understanding the active and foveated nature of human vision. Prior works exploring foveated sampling in DNNs have introduced anisotropy in their attempts to wrangle retinal samples into a grid-like representation, sacrificing faithful cortical retinotopy and creating undesirable warped receptive field shapes that depend on eccentricity. \nHere, we offer an alternative approach by adapting the model architecture to enable realistic foveated sensing. First, we develop a spatially-variant input sensor derived from the assumption of isotropic cortical magnification. Second, as this produces a curved sensor manifold, we devise a novel method for hierarchical convolutional processing that defines receptive fields as k-nearest-neighborhoods on the sensor manifold. This approach allows us to build hierarchical KNN convolutional neural networks (KNN-CNNs) closely matched to their CNN counterparts. \nArchitecturally, these models have more realistic cortical retinotopy and desirable receptive field properties, such as increasing size and approximately constant shape as a function of eccentricity. \nTraining foveated KNN-CNNs end-to-end over natural images on a categorization task, we find that they provide improved performance relative to non-foveated CNNs when retinal resources are constrained relative to the native image resolution. Moreover, they exhibit increasing performance with multiple fixations that encode different parts of the image in high-resolution. \nBroadly, this model class offers a more biologically-aligned sampling of the visual world, enabling future computational work to model the active and spatial nature of human vision, and to build more neurally mappable models."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Inverse receptive field attention for naturalistic image reconstruction from the brain",
    "presenter": "Marcel van Gerven",
    "poster_id": "C113",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Visual perception in the brain largely depends on the organization of neuronal receptive fields. Although extensive research has delineated the coding principles of receptive fields, most studies have been constrained by their foundational assumptions. Moreover, while machine learning has successfully been used to reconstruct images from brain data, this approach faces significant challenges, including inherent feature biases in the model and the complexities of brain structure and function. In this study, we introduce an inverse receptive field attention (IRFA) model, designed to reconstruct naturalistic images from neurophysiological data in an end-to-end fashion. This approach aims to elucidate the tuning properties and representational transformations within the visual cortex. The IRFA model incorporates an attention mechanism that determines the inverse receptive field for each pixel, weighting neuronal responses across the visual field and feature spaces. This method allows for an examination of the dynamics of neuronal representations across stimuli in both spatial and feature dimensions. Our results show highly accurate reconstructions of naturalistic data, independent of pre-trained models. Notably, IRF models trained on macaque V1, V4, and IT regions yield remarkably consistent spatial receptive fields across different stimuli, while the features to which neuronal representations are selective exhibit significant variation. Additionally, we propose a data-driven method to explore representational clustering within various visual areas, further providing testable hypotheses."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Getting into Shape: The Impact of Early Visual Development on Object Recognition",
    "presenter": "Zejin Lu",
    "poster_id": "C116",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "A prolonged period of immaturity is a key feature distinguishing humans from artificial neural networks (ANNs) and many other animals. For example, various aspects of the visual experience of babies are rather poor and only slowly improve over time. In stark contrast, AI vision models are presented with mature, adult-like input from the start. Here we wondered whether there is a computational advantage to this developmental trajectory, and how far it would impact artificial vision models. Indeed, recent studies indicate that injecting several fixed levels of blur into training can improve robustness and shape bias — longstanding challenges for vision models in object recognition. However, a large margin to human visual robustness remains for all publicly available vision systems. To further explore the possibilities of a human-adjusted visual diet, we introduce a visual training trajectory that simulates the progressive developmental visual diet (DVD) of humans, spanning from newborns to 25-year-old adults. Three aspects of vision are considered: visual acuity, chromatic sensitivity, and contrast sensitivity. We demonstrate that training on vision tasks with DVD yields models with near-human-level shape bias, better alignment with robust human perception under signal deterioration, and much enhanced robustness to a variety of adversarial attacks. Importantly, the DVD improvements are observed on regular vision models, such as ResNet, trained on regular vision tasks, such as ecoset or ILSVRC, thus enabling robust visual inference outside of large-scale, large-data, multimodal models. DVD thereby offers a promising approach to bridging the gap between artificial neural networks and human visual systems."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Which Neural Networks best model the human perception of geometric shapes?",
    "presenter": "Maxence Pajot",
    "poster_id": "C115",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "While impressive in many vision tasks, artificial neural networks are limited dealing with geometric shapes. In this study, we systematically evaluate the ability of Convolutional Neural Networks (CNNs) and Vision Transformers - varying in sizes and training datasets- to recognize and process geometric shapes. We compare the models’ internal representations to human data collected from an outlier detection task involving quadrilaterals. We find that networks trained on large scale datasets, develop shape representations that closely resemble those of humans in some tasks."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "MIRAGE: Robust multi-modal architectures translate fMRI-to-image models from vision to mental imagery",
    "presenter": "Reese Kneeland",
    "poster_id": "C118",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "To be practically useful, vision decoding models trained on perceived images must generalize effectively to mental images—internally generated visual representations. Analysis of the NSD-Imagery dataset revealed that achieving state-of-the-art (SOTA) results on seen images does not guarantee similar performance on mental images. To address this gap, we developed **MIRAGE**, a model explicitly designed to generalize from visually perceived data to mental imagery decoding. \\textbf{MIRAGE} employs a robust ridge regression approach, utilizes multi-modal conditioning with text and small-dimension image embeddings, and leverages the Stable Cascade diffusion model. Extensive human evaluations and image metrics establish **MIRAGE** as the SOTA method for mental image reconstruction on NSD-Imagery. Our ablation studies emphasize that mental imagery decoding benefits from simple architectures robust to low signal-to-noise conditions, explicit low-level guidance, multi-modal semantic features, and lower-dimensional embeddings compared to typical vision decoders. These findings highlight the potential of existing visual datasets for training models capable of effective mental imagery decoding."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "CorText-AMA: brain-language fusion as a new tool for probing visually evoked brain responses",
    "presenter": "Victoria Bosch",
    "poster_id": "C119",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Cognitive computational neuroscience embraces machine learning techniques to gain insight into how the brain represents and transforms visual information. Recent advances have allowed the field to move from classic category inventory approaches to more contextualized, semantic aspects, e.g. by mapping visual responses to natural scenes to corresponding language embeddings of scene captions. While the latter is powerful, single embedding vectors or captions may not fully capture the distributed cortical feature selectivity or complex spatial and semantic interactions in natural scenes. To go beyond passive representation analysis and develop interactive approaches to neural data interpretation, we extend large language models to combine a natural language interface with brain data. The resulting framework, CorText-AMA, provides an interactive chat interface that enables researchers to interrogate neural representations of natural scenes. This approach preserves semantic context while simultaneously allowing us to isolate and examine specific dimensions of brain representations. To make this possible, we combine a transformer-based multimodal model and functional brain alignment with a large instruction-finetuning dataset of question-answer pairs defined on natural scenes. The current model enables flexible probing of decodable information in visual cortex and outperforms control models. Future work will further investigate the usage of CorText-AMA as an interactive diagnostic readout that allows contrasting which questions can be answered based on neural responses in specific brain regions, and which cannot."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Target Detection as an Online Measure of Adapting to Changing Statistical Regularities",
    "presenter": "Brent Vernaillen",
    "poster_id": "C117",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Our environments are inherently structured, with certain stimuli or events having a higher probability of co-occurring. Statistical learning (SL), the ability to extract such regularities, is a powerful cognitive mechanism. The majority of SL studies to date, however, have treated our sensory environments as stable, assuming only a single set of to-be-learned regularities. In doing so, they overlooked the flexibility humans need to process and represent changes in the statistical patterns that make up our environments. In the current study, we exposed participants to visual and auditory sequences containing statistical regularities that changed throughout exposure. Our online learning measure, based on the reaction-time benefit in detecting predictable stimuli, showed that participants successfully learned both the initial and updated structure in the visual modality. Our offline test, by contrast, only provided evidence for learning of the first structure in the auditory modality."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "The geometry of primary visual cortex representations is dynamically adapted to task performance",
    "presenter": "Leyla Roksan Caglar",
    "poster_id": "C120",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Perceptual learning optimizes perception by reshaping sensory representations to enhance discrimination and generalization. Previous work has shown that learning a visual orientation discrimination task  reshapes the population feature representations in the primary visual cortex (V1) via suppressive mechanisms. Although the computational importance of these changes has not yet been elucidated, it has been proposed that they optimize the geometry of the representation to be readout. Are these feature-encoding changes paired with changes to the representational geometry? To answer this, we investigated the relationship between V1 feature representation, behavioral performance, and neural manifold geometry in trained and naïve mice. Response dimensionality showed increases with task difficulty but was lower in trained animals, suggesting that successful learning reduces dimensionality. Based on manifold capacity, dimensionality, and radius, we further found that representational separability is a stronger predictor of individual behavioral performance. These results confirm that learning alters the geometric properties as early as early sensory representations, optimizing them for linear readout and improving perceptual decision-making."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Replicating Posner and Keele (1968) with Standard Convolutional Neural Networks",
    "presenter": "Milan Van Maldegem",
    "poster_id": "C121",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "This study tests whether standard convolutional neural networks (CNNs) replicate the human prototype effect in category learning. Using a setup based on Experiment III of Posner \u0026 Keele (1968), three CNN architectures were trained on abstract dot patterns. While none matched the human results, AlexNet and DenseNet-121 showed better accuracy for unseen prototypes than for new exemplars, suggesting a weak prototype bias. These results provide a foundation for further research on category learning in humans and CNNs."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Decoding Colour Information from EEG Signals in Natural Scenes",
    "presenter": "Arash Akbarinia",
    "poster_id": "C123",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Recent years have seen major advances in decoding perceptual information from EEG signals, driven by developments in artificial intelligence—particularly large datasets, transformer-based models, and contrastive learning. Here, we investigate whether colour information can be decoded from EEG signals recorded while participants viewed natural scenes. While earlier work shows that colour features (e.g., the hue circle) are decodable under controlled conditions, it is unclear whether such information persists in complex, real-world settings. Using the THINGS-EEG2 dataset, we analyse EEG recordings from a 64-electrode cap as participants viewed natural images for 100 ms each in a rapid serial visual presentation (RSVP) paradigm. To define colour ground truth, we apply the Segment Anything Model (SAM) to segment images into foreground and background, quantifying each segment's colour using common categories from human colour naming studies. An artificial neural network is trained to predict scene colour content from EEG signals alone, and performance is evaluated by comparing predicted and ground-truth colours for each region. Our findings show that EEG signals retain decodable colour information even in object recognition tasks without explicit colour references, offering new insights into the brain's colour representation and opening doors for naturalistic brain-computer interfaces and neuroimaging research."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Hues and Cues: Human vs. CLIP",
    "presenter": "Nuria Alabau-Bosque",
    "poster_id": "C122",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Playing games is inherently human, and a lot of games are created to challenge different human characteristics. However, these tasks are often left out when evaluating the human-like nature of artificial models. The objective of this work is proposing a new approach to evaluate artificial models via board games. To this effect, we test the color perception and color naming capabilities of CLIP by playing the board game Hues \u0026 Cues and assess its alignment with humans. Our experiments show that CLIP is generally well aligned with human observers, but our approach brings to light certain cultural biases and inconsistencies when dealing with different abstraction levels that are hard to identify with other testing strategies. Our findings indicate that assessing models with different tasks like board games can make certain deficiencies in the models stand out in ways that are difficult to test with the commonly used benchmarks."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Connectome-Constrained Unsupervised Learning Reveals Emergent Visual Representations in the Drosophila Optic Lobe",
    "presenter": "Keisuke Toyoda",
    "poster_id": "C124",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Understanding how brain structure enables visual processing is crucial. While \\textit{Drosophila} offers a complete connectome, computational models often use biologically implausible supervised signals. We address this by building a large-scale autoencoder constrained by the complete \\textit{Drosophila} right optic lobe connectome ($\\sim$45k neurons, FlyWire dataset). Using photoreceptors (R1-R6) as both input and output, the model incorporates anatomical feedforward and feedback loops and was trained unsupervised on naturalistic video stimuli to minimize reconstruction error. Temporal offsets were included to probe predictive capacity. The autoencoder accurately reconstructed photoreceptor inputs with high fidelity. Deeper layer neurons (medulla, lobula) showed moderate, stable activity under sustained input, consistent with efficient engagement and functional recurrent loops. Temporal offsets improved short-term prediction, indicating learned input dynamics. We demonstrate that a connectome-based autoencoder can learn meaningful visual representations via biologically plausible unsupervised learning. This highlights how anatomical structure shapes emergent function and provides a digital twin framework for studying visual processing beyond task-specific supervised approaches, suggesting complex representations can arise from self-organization on detailed neural circuits."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Rethinking Representational Alignment: Linear Probing Fails to Identify the Ground-Truth Model",
    "presenter": "Itamar Avitan",
    "poster_id": "C126",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Linearly transforming stimulus representations of deep neural networks yields performant models of human similarity judgments. But can the predictive accuracy of such models identify genuine representational alignment? We conducted a model recovery study to test this empirically. We aligned 20 diverse pretrained models to 4.2 million human judgments from the THINGS-odd-one-out dataset, generated synthetic data conforming to the predictions of one of the models, and tested whether this model would re-emerge as the best predictor of the simulated data, as measured by linear probing. We found that even with large datasets, linear probing can systematically fail to recover ground-truth models. Our findings call for a reconsideration of the flexibility of model-human alignment metrics and the design of model comparison studies.\n\n$\\textbf{Keywords:}$ Representational Alignment ; Model Recovery; Deep Neural Networks; Similarity Judgments"
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Examining the potential functional significance of initially poor temporal acuity",
    "presenter": "Marin Vogelsang",
    "poster_id": "C127",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "The human visual system is remarkably immature at birth, exhibiting initially degraded spatial and temporal vision. While early spatial degradations have been proposed to provide important benefits to the developing visual system, less is known about the potential adaptive significance of early temporal immaturities. Here, we investigated this possibility computationally, using 3D convolutional neural networks trained on a temporally meaningful classification task. We systematically manipulated spatial and temporal blur when training on the Something-Something V2 dataset, which critically depends on temporal order. Analysis of learned receptive fields revealed that initial exposure to temporal blur led to longer-range temporal processing, persisting even after transitioning to clear temporal inputs. Such developmental trajectory commencing with initial temporal blur also significantly enhanced generalization performance compared to training with high temporal resolution input or corresponding spatial blur alone. These findings extend the concept of adaptive developmental degradations into the temporal domain, suggesting that immaturities in temporal vision may instantiate important mechanisms for robust perception later in life."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision",
    "presenter": "Alessandro Thomas Gifford",
    "poster_id": "C128",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic’s fMRI responses are OOD with respect to NSD, that brain encoding models exhibit reduced performance when tested OOD on NSD-synthetic compared to when tested in-distribution (ID) on NSD, and that OOD tests on NSD-synthetic reveal differences between encoding models not detected by ID tests—specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Developmental plasticity rules facilitate representation learning in a model of visual ventral stream",
    "presenter": "Ariane Delrocq",
    "poster_id": "C129",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "It is known that different cortical areas have different critical periods for the most fundamental learning. However, the type of developmental plasticity rules that lead to high-level representations of objects are unknown. Here, we study a model of the visual ventral stream trained by a generalized Hebbian plasticity rule. The learning rule uses only quantities that are locally available at the site of the synapse, is consistent with recent plasticity experiments in pyramidal neurons,  and, as opposed to the backpropagation algorithm, does not need a detailed feedback architecture. Our model shows that limiting plasticity in time to critical periods of development improves the quality of learned representation. Our model achieves state-of-the-art performance for bio-plausible plasticity models on the STL10 large image dataset designed for unsupervised learning."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A hierarchy of spatial predictions across human visual cortex during natural vision",
    "presenter": "Wieger H. Scheurer",
    "poster_id": "C125",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "The predictive processing framework posits that the brain constantly compares incoming sensory signals with self-generated predictions. However, evidence for prediction in sensory cortex mostly comes from artificial paradigms with simple, highly predictable stimuli, leaving it unclear whether the reported sensory prediction effects generalise to perception more broadly. Here, we probe predictions in naturalistic perception, analysing high-resolution 7T functional magnetic resonance imaging (fMRI) responses of human participants viewing tens of thousands natural scenes. We use deep generative models to quantify the inherent spatial predictability of image patches, and relate resulting estimates to brain activity. Our results reveal robust and widespread predictability modulations of BOLD responses across the visual cortex. Higher visual areas were sensitive to more high-level predictability, forming a prediction hierarchy. Effects were stronger in voxels with higher eccentricity receptive fields, aligning with predictive coding and Bayesian theories. These results demonstrate the ubiquity of prediction in vision and inform neurocomputational models of predictive coding and self-supervised learning in the brain."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Models trained on infant views are more predictive of infant visual cortex",
    "presenter": "Cliona O'Doherty",
    "poster_id": "C131",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "The perspective of a developing infant offers unique potential when training a neural network. Egocentric video from a young child can provide ample data for representation learning in vision and language models, to only some expense of model performance. It is known that pre-trained DNNs optimised for object classification are good models of the ventral visual stream in adults, but would the same be true prior to the onset of classification behaviour? Here, we explore whether models trained on infant views are more predictive of category responses in infant ventrotemporal cortex (VTC). Using awake fMRI in a large cohort of 2-month-olds, we find that - unlike adults - features from neural networks pre-trained on infant headcam data are better models of infant VVC."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Incorporating foveal sampling and integration to model 3D shape inferences",
    "presenter": "Stephanie Fu",
    "poster_id": "C130",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Human vision is inherently sequential. This is largely because of foveal constraints on the retina, which demand\nthat we shift our gaze to collect high-resolution information from throughout the environment. Within the domain\nof visual object perception, the neural substrates that support these sequential visual inferences have been well\ncharacterized: ventral temporal cortex (VTC) rapidly extracts visual features at each spatial location, while medial\ntemporal cortex (MTC) integrates over the sequential outputs of VTC. This neurocomputational motif is absent in\ncontemporary deep learning models of human vision. Not\nsurprisingly, contemporary models approximate the rapid\nvisual inferences that depend on VTC, but not those behaviors that depend on MTC (e.g., novel 3D shape inference).\nHere we develop a modeling framework that embodies\nthe sequential sampling/integration strategy emblematic\nof human vision. Given an image, this model first determines relevant locations to attend to, sequentially processes these locations as ‘foveated’ inputs using a VTC-like model, then integrates over these sequential visual\nfeatures within a MTC-like model. Here we report preliminary results on the design choices that lead to stable\nmodel optimization and subsequent model behaviors."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Investigation of Numerosity Representation in Convolution Neural Networks",
    "presenter": "Nhut Truong",
    "poster_id": "C132",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Convolutional neural networks (CNNs) have emerged as powerful models for predicting neural activity and behavior in visual tasks. Recent studies suggest that number-detector units—analogous to number neurons—can emerge in CNNs, both in trained networks optimized for object recognition and in untrained networks. In this work, we extend previous studies by investigating whether CNNs encode numerosity at the population level and by examining how the statistical distribution of numerical and non-numerical features in the training dataset influences their internal representations. Recognizing that perceptual systems are finely tuned to the statistical properties of their sensory environment, we compare CNNs trained on both synthetic datasets and a naturalistic dataset that better reflects the real-world conditions shaping human number sense. By systematically manipulating these statistical properties, we assess their impact on the encoding of both numerical and non-numerical features. Finally, we compare these computational representations with those observed in the human brain, highlighting both shared characteristics and key differences that provide deeper insights into the mechanisms underlying numerosity perception in biological and artificial systems."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "How to sample the world for understanding the visual system",
    "presenter": "Johannes Roth",
    "poster_id": "C134",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Understanding vision requires capturing the vast diversity of the visual world we experience. How can we sample this diversity in a manner that supports robust, generalizable inferences? While widely-used, massive neuroimaging datasets have strongly contributed to our understanding of brain function, their ability to comprehensively capture the diversity of visual and semantic experiences remains largely untested. More broadly, the factors required for diverse and generalizable datasets have remained unknown. To address these gaps, we introduce LAION-natural, a curated subset of 120 million natural photographs filtered from LAION-2B, and use it as a proxy of the breadth of our visual experience in assessing visual-semantic coverage. Our analysis of CLIP embeddings of these images reveals significant representational gaps in existing datasets, demonstrating that they cover only a restricted subset of the space spanned by LAION-natural. Simulations and analyses of functional MRI data further show that these gaps lead to impaired out-of-distribution generalization. Importantly, our results reveal that even moderately sized stimulus sets can achieve strong generalization if they are sampled from a diverse stimulus pool, and that this diversity is more important than the specific sampling strategy employed. These findings not only highlight limitations of existing datasets in generalizability and model comparison, but also provide clear strategies for future studies to support the development of stronger computational models of the visual system and generalizable inferences."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Memory diffusion: Using generative AI to create an image database  for memory research",
    "presenter": "Fabian Kamp",
    "poster_id": "C135",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "When we memorize visual stimuli, their content is processed at multiple levels, ranging from the fine-grained perceptual details to the semantic concepts and categories. However, it is unclear to which extend low- and high-level information is maintained in memory over time. Real-world stimuli are not ideal for investigating this question, as they often exhibit strong correlations between processing levels: Conceptually similar objects tend to share similar visual features. Using generative AI we created a new database of 496 image pairs orthogonalizing semantic (word2vec) and perceptual (CoreNet-S) information. Specifically, we generated image pairs that either (a) depict objects from distinct semantic concepts but are perceptually similar, or (b) show the same object but are perceptually dissimilar."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Model-brain comparison using inter-animal transforms",
    "presenter": "Imran Thobani",
    "poster_id": "C133",
    "topic_area": "Visual Processing and Computational Vision",
    "abstract": "Artificial neural network models have emerged as promising mechanistic models of brain function, but there is little consensus on the correct method for comparing activation patterns in these models to brain responses. Drawing on recent work on mechanistic models in philosophy of neuroscience, we propose that a good comparison method should mimic the Inter-Animal Transform Class (IATC) - the strictest set of functions needed to accurately map neural responses between subjects in a population for the same brain area. Using the IATC, we can map bidirectionally between model responses and brain data, assessing how well the model can masquerade as a typical subject using the same kinds of transforms needed to map across animal subjects. We attempt to empirically identify the IATC in three settings: a simulated population of neural network models, a population of mouse subjects, and a population of human subjects. In each setting, we find that the empirically identified IATC enables accurate neural predictions while also achieving high specificity (i.e. distinguishing response patterns from different areas while strongly aligning same-area responses between subjects). In some settings, we find evidence that the IATC is shaped by specific aspects of the neural mechanism, such as the non-linear activation function. Using IATC-guided transforms, we obtain new evidence, convergent with previous findings, in favor of topographical deep neural networks (TDANNs) as models of the visual system."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A Multiple Trace Theory of Statistical Learning",
    "presenter": "Dock H Duncan",
    "poster_id": "C136",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "The attentional effects of statistical learning and intertrial priming have long been treated as separate selection history effects. This is despite their many similarities, improving performance in the same direction. There is thus motivation to either formally dissociate these two effects, or unify them under a single theoretical framework. We suggest that multiple trace theory is the framework to unify these two cognitive effects. We used a Kalman filter approach to model reaction times while participants performed the additional singleton task with biased distractor presentations - a paradigm known to engender both statistical learning and intertrial effects. Initial results suggest this by-trial modelling approach aptly captures learning effects and their effect on reaction times. Subsequent steps will now compare unified versus divided models of intertrial priming and statistical learning to provide evidence for their dissociation or union."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Perceiving patterns under uncertainty:  the role of conspiratorial thinking",
    "presenter": "Yi-Chuang Lin",
    "poster_id": "C137",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Conspiratorial thinking, among other things, is associated with a tendency to perceive patterns in ambiguity and jump to conclusions. In a visual pattern detection task (N = 406), participants were faster and more accurate when identifying object images compared to noise, confirming sensitivity to true signals. Participants with higher conspiracy beliefs identified more objects in noise trials, and responded faster in both object and noise trials. Drift diffusion modeling revealed that higher conspiracy scores were associated with reduced boundary separation, indicating lower decision thresholds. These results suggest that individuals who believe in conspiracy theories tend to accumulate less evidence before decision-making, confirming a tendency to jump to conclusions."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Attention rules Episodic Memory",
    "presenter": "Zahra Fayyaz",
    "poster_id": "C138",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Attention plays a crucial role in memory and learning by prioritizing relevant information and filtering out redundant input. This study explores how attention, guided by semantic memory, enhances memory encoding and retrieval. We present a neural network model to simulate generative episodic memory, comprising a VQ-VAE encoder, an attention module, and a transformer-based semantic decoder. Three attention strategies (random, selective, and additive) were evaluated. Random attention, lacking prioritization, led to lowest memory accuracy. Selective attention, informed by semantic prediction, improved performance by focusing on novel, informative inputs. Additive attention, inspired by biological saccades, offered the highest performance through iterative, predictive refinement of input encoding, albeit at a higher computational cost. Furthermore, experiments on both MNIST and ImageNet datasets demonstrate that semantically-guided attention leads to more structured and less prototypical memory traces. These findings underscore the dynamic interplay between attention and memory, suggesting that attentional mechanisms shaped by prior knowledge significantly optimize learning and memory."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Multimodal Human Perception of Object Dimensions: Evidence from Deep Neural Networks And Large Language Models",
    "presenter": "Florian Burger",
    "poster_id": "C139",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Human object recognition relies on both perceptual and semantic dimensions. Here, we examined how deep neural networks (DNNs) and large language models (LLMs) capture and integrate human-derived dimensions of object similarity. We extracted layer activations from CORnet-S and obtained BERT embeddings for 1853 images from the THINGS dataset. We used support vector regression (SVR) to quantify explained variance in human-derived dimensions. Results showed that multimodal integration improved predictions in early visual processing but offers limited additional benefits at later stages, suggesting that deep perceptual processing already encodes meaningful object representations."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Seen2Scene: a generative model of fixation-by-fixation scene understanding",
    "presenter": "Ritik Raina",
    "poster_id": "C140",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Human scene understanding dynamically evolves over the course of sequential viewing fixations from a gist-level understanding to a more detailed comprehension of the scene. Each fixation provides rich visual information about objects and their spatial relationships and we model this incremental process by introducing Seen2Scene, a framework for modeling human scene understanding by controlling the visual inputs available for scene generation. Seen2Scene uses a self-supervised encoder to extract features from fixated scene regions, which guides a pre-trained text-to-image latent diffusion model through a modular adapter framework. As fixations accumulate, the model iteratively refines its visual hypotheses, filling in unseen areas with contextually plausible content. We evaluated Seen2Scene on COCO-FreeView using two experimental conditions: fixation-only conditioning to isolate the contribution of foveal information, and fixation+gist conditioning to examine how peripheral scene information integrates with foveal details. Results show that initial fixations drive the greatest gains in semantic and perceptual fidelity and that the fixation+gist condition reached the high-fidelity scene understanding with the fewest fixations, thus demonstrating the importance of integrating peripheral gist information visual details collected foveally."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Auditory Object Formation in Temporally Complex Scenes",
    "presenter": "Berfin Bastug",
    "poster_id": "C141",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "The auditory system decomposes boundary-less\nsensory input into meaningful units through\nAuditory Scene Analysis (ASA) (Bregman, 1990).\nRepetition helps listeners segregate overlapping\nsounds and identify distinct auditory objects\n(McDermott et al., 2011). Previous studies suggest\nthat repeated units in noisy or ambiguous contexts\ncan eventually be perceived as stable auditory\nobjects (Barczak et al., 2018; McDermott et al., 2011),\nthough the behavioral dynamics of this process\nremain unclear. We investigated this build-up\nprocess using 'tone cloud' stimuli. By manipulating\nrepetition strength and unit duration of tone cloud\nunits, we created auditory analogues of the motion\ncoherence paradigm. Participants completed\nrepetition detection and sensorimotor\nsynchronization tasks, allowing us to examine how\nthe accumulation of sensory evidence supports the\nemergence and stabilization of auditory objects.\nResults reveal sigmoidal, quasi-categorical\nperformance in both tasks. In detection,\nperformance improves earlier for shorter durations.\nInterestingly, In synchronization, performance\nconverges across durations, showing that once an\nobject emerges, it can be tracked equally well\nregardless of duration. Our results suggest a\ncategorical shift in perception, with stabilization\noccurring after sufficient repetition."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Quantifying infants’ everyday experiences with objects in a large corpus of egocentric videos",
    "presenter": "Jane Yang",
    "poster_id": "C143",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "While modern vision-language models are typically trained on millions of curated photographs, infants learn visual categories and the words that refer to them from very different training data. Here, we investigate which objects infants actually encounter in their everyday environments, and how often they encounter them. We use a large corpus of egocentric videos taken from the infant perspective (N = 868 hours, N = 31 participants), applying and validating a recent object detection model (YOLOE) to detect a set of categories that are frequently named in children’s early vocabulary. We find that infants’ visual experience is dominated by a small set of objects, with differences in individual children’s home environments driving variability. We also find that young children tend to learn words earlier for more frequently encountered categories. These results suggest that visual experience scaffolds young children’s early category and language learning and highlight that ecologically valid computational models of category learning must be able to accommodate skewed input distributions."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Representational drift across the macaque ventral stream does not affect all timepoints and stimuli alike: first evidence for a sequence of three different, yet comparatively stable clusters in V4.",
    "presenter": "Daniel Anthes",
    "poster_id": "C142",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Neural representations supporting stable behavior have been shown to drift on the timescale of days to weeks. With some exceptions, most studies investigating this phenomenon analyze populations of single neurons in mice. Here, we add to the growing evidence for representational drift by showing that the phenomenon is also found in LFPs recorded from the ventral stream of a macaque monkey during a passive object viewing task. Additionally, we show that as representations evolve over the trial time course, the main axes of the representational geometry are relatively stable, suggesting that drift is not uniform over time and stimuli."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A Goal-driven Model of Visual Search in Natural Scenes Replicates Human Behavior While Relying on Similar Neural Representations",
    "presenter": "Motahareh Pourrahimi",
    "poster_id": "C145",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Visual search, the process of locating a specific item among multiple objects, is a key paradigm in studying visual attention. Due to eccentricity-dependent visual acuity, many animals constantly selectively sample from their environment by moving their gaze location, leading to the formation of search scanpaths, a hallmark of visual search behavior. While much is known about the brain networks involved in visual search, our understanding of the neural computations driving this behavior is limited, leading to challenges in simulating such behavior in-silico. To address this gap, we trained an image-computable artificial neural network to perform visual search from pixels in natural scenes. Model’s search scanpaths (spatiotemporal sequence of fixations) were highly consistent with those of humans. It captured the human information integration behavior and relied on neural representations similar to those observed in the primate fronto-parietal attentional control network. Examining the model’s latent space revealed how it uses its internal state to construct and update a priority map of the visual space, enabling efficient visual search. Our model provides concrete predictions about the neural computations underlying visual search in the primate brain."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Leveraging Vision Transformers to Propose a Context-Dependent Computational Mechanism for the Holistic Process of Faces",
    "presenter": "Srijani Saha",
    "poster_id": "C146",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Holistic processing, or the integration of facial features to build an identity representation, has offered a clever solution to a critical problem - how can we seamlessly tell apart faces when there is little inter-class variability? While seminal work in psychology has demonstrated the behavioral consequences of holistic processing where individual features and identities appear different as a result of the facial context (e.g., the Composite Face Effect, Thatcher Illusion), it has been difficult to identify a computational mechanism that operationalizes these context- dependent perceptual effects. Here, we leverage the vision transformer’s architecture to show how local perturbations in a face can update the representations of other face features, thereby affecting the identity representation. The interactions between the perturbed feature and the context updates the representation of the unchanged facial features and identity, the latter towards a different, new identity. The shift in identity primarily occurs when the local changes\nare naturalistic."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Tracking covert attention over space and time using RIFT",
    "presenter": "Kabir Arora",
    "poster_id": "C144",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "When navigating through the visual world, we constantly shift our attention from one point to the other. We often do this in a covert manner, without any clearly visible signature (such as moving our eyes). This makes these shifts difficult to study, and therefore we know relatively little about how they actually operate. How does our attention move from one point to another? Here, we use Rapid Invisible Frequency Tagging (RIFT) in an EEG experiment to track the allocation of spatial attention over time and space during covert attentional shifts, showing a suppression of attention at the location between the shifts around 150ms after it begins."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Brain-Aligned Category-Selective Features from Contrastive Learning",
    "presenter": "Daniel Janini",
    "poster_id": "C147",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Researchers have long debated the origins of category-selective visual cortex. Recently, some have argued that face- and scene-selective cortex can naturally emerge from contrastive self-supervised learning instead of domain-specific learning objectives. Here, we aggregated an image set for testing classic effects of the FFA and PPA. We ran replication fMRI experiments for these effects, characterizing the FFA and PPA’s distinct feature tuning. We then applied this test battery to a self-supervised vision model, finding that its face- and scene-selective features naturally exhibit many of these effects as well. Our findings support the argument that properties of human category-selective cortex can emerge from contrastive learning objectives, though our test battery also revealed specific shortcomings that could be improved in future models."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Competitive Retraining Reveals the Resilience and Reallocation of Functional Specialization in Deep Neural Networks",
    "presenter": "Zhengqing Miao",
    "poster_id": "C149",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Cognitive neuroscientists have long documented functional specialization in the brain for tasks such as face, body or scene recognition, and recent computational studies reveal that deep neural networks (DNNs) spontaneously develop specialized populations of units for the same tasks. But are these specialized units necessary for performance, and how plastic are they? Here, we combine lesioning approaches with competitive retraining in DNNs to address these questions. In a dual-task network with localized specialized units in the final convolutional layer for face and object tasks, we ablated those units either at the onset or continuously throughout retraining. To modulate competition, we retrained the networks on a single task or both tasks simultaneously. Our findings reveal that retraining restores network performance even when these layer-specific units remain permanently disrupted, indicating they are not strictly necessary. Moreover, the extent and pattern of unit reallocation vary with retraining conditions, demonstrating substantial plasticity and suggesting that the reallocation process is an intrinsic outcome of rapid network optimization."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Parametric control along the encoding axes of IT neurons uncovers hidden differences in model-brain alignment",
    "presenter": "Jacob S. Prince",
    "poster_id": "C148",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "As model-brain alignment scores increasingly saturate under current assessment methods, new approaches are needed to test whether there are actually hidden differences in how well models capture biological feature tuning. To this end, we introduce a paradigm for comparing deep encoding models based on their ability to *control* neural responses along their hypothesized encoding axes. Using recordings from macaque inferotemporal cortex, we compared two DNN-based encoding models: a standard ResNet-50 and an adversarially robust variant.  These models achieved comparable performance in predicting neural responses over a wide range of natural images. However, we found they differed substantially when subjected to a test of “parametric control.” Leveraging an explainable AI technique called feature accentuation, we synthesized image sets that varied systematically in precise intervals along each encoding axis, based on the hierarchical computations of each model. We found that accentuated stimuli from the robust model achieved superior control of neural firing. We then synthesized “controversial” stimuli that further validated the brain alignment of RN50-robust over the baseline model. Our framework offers a new means to arbitrate between models, requiring a more precise characterization of feature tuning in targeted local regions of image space."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Quantifying the Role of Perceived Curvature in the Processing of  Natural Object Images",
    "presenter": "Laura Mai Stoinski",
    "poster_id": "C151",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Curvature has been suggested as a fundamental organizational dimension of object responses. Despite its prominence, there is no consensus on how to define this measure for naturalistic object images. Here, we aimed to quantify the perceived curvature of natural images, clarify its relationship to spatial and temporal patterns of brain activity, and identify what features in an image contribute to perceived curvature. To address this, we collected extensive curvature ratings of 27,961 natural images and tested how they explain neural responses compared to computed curvature measures. Leveraging large-scale fMRI and MEG datasets, perceived curvature best explained broad occipitotemporal patterns in fMRI data and was decodable across an extended time period in MEG. To identify the object-features contributing to people’s perception of curvature, we used an image-generative approach based on deep neural networks, suggesting that people considered the curvature of more global object contours in their judgements. Given the apparent validity of perceived curvature, we offer an image-computable model to quantify perceived curvature for novel images. Together, our results highlight the importance of perceived curvature as a mid-level summary statistic and provide an approach for the automated quantification of perceived curvature in natural object images."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Representational Geometry Dynamics in Networks After Long-Range Modulatory Feedback",
    "presenter": "Kexin Cindy Luo",
    "poster_id": "C152",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "The human visual system employs extensive long-range feedback circuitry, where feedforward and feedback connections iteratively refine interpretations through reentrant loops (Di Lollo, 2012). Inspired by this neuroanatomy, a recent computational model incorporated long-range modulatory feedback into a convolutional neural network (Konkle \u0026 Alvarez, 2023). While this prior work focused on injecting an external goal signal to leverage feedback for category-based attention, here we investigated its default operation: how learned feedback intrinsically reshapes representational geometry without top-down goals. Analyzing activations from this model across two passes—feedforward versus modulated—on ImageNet data, we examined local (within-category) and global (between-category) structure. Our results demonstrate that feedback significantly compacts category clusters: exemplars move closer to prototypes, and the local structure improves as more near neighbors fall within the same category. Notably, this occurs while largely preserving global structure, as between-category distances remain relatively stable. An exploratory analysis linking local and global changes suggested a positive relationship between local compaction and prototype shifts. These findings reveal an emergent \"prototype effect\" where fixed long-range feedback automatically refines local representations, potentially enhancing categorical processing efficiency without disrupting overall representational organization. This suggests intrinsic feedback dynamics might contribute fundamentally to perceptual organization."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Encoding of Fixation-Specific Visual Information: No Evidence of Information Carry-Over between Fixations",
    "presenter": "Carmen Amme",
    "poster_id": "C153",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Humans make multiple eye movements each second to sample visual information from different locations in space. This information is integrated to form a single, coherent percept. Here, we investigate whether fixation-specific visual information is encoded in the corresponding neural data, and whether this information is carried over to the subsequent fixation. We successfully encoded fixation-specific neural responses using deep neural network features extracted from fixation patches, with encoding performance peaking at around 125 ms after fixation onset in occipital sensors. Encoding in source space revealed a peak encoding performance along the left dorsal visual path at 100 ms. Incorporating model representations from both the previous and current fixation patches did not improve encoding performance, suggesting no carry-over of visual information between fixations. By demonstrating the feasibility of encoding naturalistic stimulus features during active vision in humans, we open new avenues for investigating how the brain constructs coherent percepts despite processing visual information in discrete, fixation-specific fragments."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Shared high-dimensional latent structure in the neural and mental representations of objects",
    "presenter": "Raj Magesh Gauthaman",
    "poster_id": "C150",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Recent work has demonstrated that visual cortex representations of natural scenes are high-dimensional, with a power-law spectrum of stimulus-related variance. However, the statistical structure of the mental representations underlying visual behavior remains unknown — is there a limited subset of latent dimensions that fully captures human behavior on a visual task? Here, we investigate the dimensionality of visual object representations in the human mind and brain by analyzing behavioral and fMRI responses from the large-scale THINGS-data collection using spectral decomposition methods. First, we find that neural representations of objects have a high-dimensional power-law structure throughout visual cortex, replicating previous findings for natural scenes. Next, we show that mental representations of objects, inferred directly from human similarity judgments, have an underlying power-law covariance spectrum, consistent with the power-law structure observed in neural representations of these stimuli. Finally, we show that the dimensionality of shared mental and neural representations increases systematically over stages of visual processing from V1 to hV4 to LOC. Our results suggest that a shared high-dimensional latent structure underlies both mental and neural representations of objects."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Small-Worlds Memory Generation and Capacity in the Neuroidal Model",
    "presenter": "Mugizi Robert Rwebangira",
    "poster_id": "C155",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Understanding higher level cognitive processes is a central problem in neuroscience. The Neuroidal model provides a useful framework for posing these problems in a computer science context. There has been significant recent work trying to understand memory capacity in the Neuroidal model but this work was done assuming that the network of neurons was an Erdős-Rényi (ER) random graph. However, the network of neurons in the brain has been shown to exhibit small-world properties, which are not present in ER graphs. In this research we explore replacing ER graphs with Watts-Strogatz and Barabási-Albert small-worlds graphs in order to more accurately model the biological reality. We aim to investigate the implications for memory capacity and interference within the Neuroidal model. We show that the algorithm JOIN can function with small-worlds graph structures and allow the Neuroidal model to reach capacity."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Robustness to 3D Object Transformations in Humans and Image-Based Deep Neural Networks",
    "presenter": "Haider Al-Tahan",
    "poster_id": "C154",
    "topic_area": "Object Recognition and Visual Attention",
    "abstract": "Recent work at the intersection of psychology, neuroscience, and computer vision has advocated for the use of more realistic visual tasks in modeling human vision. Deep neural networks have become leading models of the primate visual system. However, their behavior under identity-preserving 3D object transformations, such as translation, scaling, and rotation, has not been thoroughly compared to humans. Here, we evaluate both humans and image-based deep neural networks, including vision-only and vision-language models trained with supervised, self-supervised, or weakly supervised objectives, on their ability to recognize objects undergoing such transformations. Humans (n=220) and models (n=169) were asked to categorize images of 3D objects, generated with a custom pipeline, into 16 object categories recognizable by both. Humans were time-limited to reduce reliance on recurrent processing. We find that both humans and models are robust to translation and scaling, but models struggle more with object rotation and are more sensitive to contextual changes. Humans and models agree on which in-depth object rotations are most challenging -- when humans struggle, models do too -- but humans are more robust and show more consistent category confusions with one another than with any model. By testing model families trained on different amounts of data and with different learning objectives, we show that data richness plays a substantial role in supporting robustness -- potentially more so than vision-language alignment. Our benchmark  excludes models trained on video, multiview, or 3D data, but is in principle compatible with such models and may support their evaluation in future work. This study underscores the importance of using naturalistic visual tasks to model human object perception in complex, real-world scenarios, and introduces a benchmark - ORBIT (Object Recognition Benchmark for Invariance to Transformations) - for evaluating and developing computational models of human object recognition. Code and data for ORBIT are available at: https://github.com/haideraltahan/ORBIT."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Biologically informed cortical models predict optogenetic perturbations",
    "presenter": "Guillaume Bellec",
    "poster_id": "C156",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "A recurrent neural network fitted to large electrophysiological datasets may help us understand the chain of cortical information transmission. In particular, successful network reconstruction methods should enable a model to predict the response to optogenetic perturbations. We test recurrent neural networks (RNNs) fitted to electrophysiological datasets on unseen optogenetic interventions, and measure that generic RNNs used predominantly in the field generalize poorly on these perturbations. Our alternative RNN model adds biologically informed inductive biases like structured connectivity of excitatory and inhibitory neurons, and spiking neuron dynamics. We measure that some biological inductive biases improve the model prediction on perturbed trials in a simulated dataset, and a dataset recorded in mice in vivo. Furthermore, we show in theory and simulations that gradients of the fitted RNN can predict the effect of micro-perturbations in the recorded circuits, and discuss potentials for measuring brain gradients or using gradient-targeted stimulation to bias an animal behavior."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Anatomy-based estimates indicate that cortical stimulation can only sparsely affect long-range connections",
    "presenter": "Dora Hermes",
    "poster_id": "C157",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Knowledge of how human neuroanatomy influences stimulation induced neural signal propagation is essential for understanding brain network function and advancing neuromodulation technologies. Here, we investigate how neuroanatomical properties of the ventral and dorsal visual pathways guide signal propagation when stimulating ventral temporal cortex. We calculate that the convoluted dorsal and ventral surface areas in the human brain span centimeters of gray matter cortex, while the smallest cross sectional area of the white matter pathway connecting these areas only spans millimeters. Using single pulse electrical stimulation of the ventral temporal cortex, we find that evoked responses measured with intracranial EEG (iEEG) follow this neuroanatomical layout. Ventral temporal gray matter stimulation evokes little responses in dorsal visual electrodes, whereas adjacent white matter stimulation evokes strong responses in many dorsal electrodes. The influence of cortical stimulation on connected areas in the human brain thus seems limited by inherently sparse cortico-cortical connectivity, whereas white matter stimulation may provide more widespread influence."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Spike Synchrony Resolves Stimulus Saliency and Familiarity",
    "presenter": "Viktoria Zemliak",
    "poster_id": "C158",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "The function of temporal coding in the brain remains controversial, with debate centering on the following question: does spike timing information, such as their temporal synchrony, play a meaningful role in neural computation which cannot be attributed to firing rate? We propose the solution to this dilemma: spike synchrony provides crucial information about stimulus familiarity under conditions when the firing rate alone is insufficient − namely, when the input stimulus is varied in saliency. Using simulations of recurrent spiking networks, we show that synchrony is particularly effective in distinguishing familiar stimuli of low saliency from novel stimuli of high saliency − an important distinction for both biological perception and artificial agents navigating dynamic environments. Synchrony is more sensitive to recurrent connectivity, that encodes prior experiences, compared to input firing rate. This highlights the relevance of synchrony for familiarity encoding in a scenario of realistic input variability."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Occipital alpha oscillations negatively correlate with the vividness of mental imagery in healthy participants",
    "presenter": "Aitor Morales-Gregorio",
    "poster_id": "C159",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Mental imagery is the ability to evoke visualizations in the absence of external stimuli. Some individuals lack mental imagery (aphantasia), while others have the ability to evoke hyperrealistic imagery (hyperphantasia). The vividness of imagery depends on top-down communication to occipital areas. However, the neuronal correlates of mental imagery are still not fully understood. Here, we found a strong and significant linear correlation between mental imagery and EEG eyes-closed resting-state occipital alpha power. This finding suggests that alpha oscillations may be a biomarker for suppressed communication from occipital to frontal areas leading to less vivid or absent imagery."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Unsupervised Identification of Behaviorally-relevant States in Biological and Artificial Neural Systems",
    "presenter": "Arman Behrad",
    "poster_id": "C160",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Identifying distinct neural dynamics corresponding to cognitive states and their transitions is crucial for understanding the neural machinery of cognitive functions in both biological and artificial intelligent systems. However, conventional methods for state identification constrain the analysis by relying on predefined state labels. Here, we introduce a novel unsupervised approach to robustly detect behaviorally-relevant state transitions without prior assumptions or knowledge about behavioral labels. We assume that each state has a characteristic dynamics within each state (with minimal variation), but triggered by behavioral demands, transitions to other states (with different characteristic dynamics). Therefore, comparing neural dynamics across time, should provide us key information about state transitions. Based on this idea, we developed Moving Window Dynamical Similarity Analysis (MoDSA) for an unbiased detection of state transitions in neural systems. We validated our method on biological neural data recorded from macaque area V4 during selective attention tasks, and data from diverse recurrent neural networks trained on context-dependent decision-making tasks. We demonstrate that our method can identify behaviorally meaningful states purely based on neural dynamics, in both domains of artificial and biological neural systems."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Langevin Flows for Modeling Neural Latent Dynamics",
    "presenter": "T. Anderson Keller",
    "poster_id": "C162",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "We propose LangevinFlow, a sequential variational model for neural population activity, where latent dynamics are governed by the underdamped Langevin equation. This framework captures both intrinsic neural dynamics and external unobserved inputs through physically grounded priors -- incorporating inertia, damping, stochasticity, and a learned potential landscape. The potential is parameterized as a locally coupled oscillator network, biasing the model toward oscillatory and flow-like behaviors observed in real neural circuits. Our architecture combines a recurrent encoder, a one-layer Transformer decoder, and structured Langevin dynamics in the latent space. LangevinFlow achieves strong empirical results: it closely tracks ground-truth firing rates on synthetic data driven by a Lorenz attractor, and outperforms prior methods on the Neural Latents Benchmark across four datasets in terms of both bits-per-spike and forward prediction. It also matches or exceeds baselines in decoding behavioral variables such as hand velocity. This work introduces a compact, physics-inspired, interpretable, and high-performing model for neural population dynamics."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "CNeuroMod Data Collection Complete: 200h of individual fMRI Across Diverse Naturalistic and Controlled Tasks to build NeuroAI models",
    "presenter": "Julie A. Boyle",
    "poster_id": "C161",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Abstract\n\nDense fMRI datasets are gaining popularity as a powerful resource to build integrative NeuroAI models to understand the brain. The Courtois Project on Neuronal Modelling (CNeuroMod) has now completed a 5-years data collection process resulting in 200 hours of fMRI data per subject using diverse naturalistic and controlled cognitive tasks. CNeuroMod is the largest dense fMRI dataset currently available to support the development of individualized and generalizable AI models of complex brain processes.\n\nKeywords: dense fMRI dataset, multimodal datasets, NeuroAI, individual brain models, Algonauts 2025 \n\nIntroduction\n\nSeveral large individual fMRI datasets have emerged to train artificial intelligence (AI) models on specific cognitive processes like natural image (NSD: Allen et al. 2021; BOLD5000: Chang, et al. 2019) and movies (Dr Who: Seeliger et al. 2019) viewing. However, a key feature of the brain is the capacity to integrate and switch between specialized processes and cognitive contexts.The CNeuroMod project, which just completed its data collection, is the largest individual fMRI dataset to date. We collected a rich neuroimaging dataset that probes numerous cognitive domains in the same subjects (N=6) using both carefully controlled and engaging naturalistic tasks in order to build versatile and complex Neuro-AI models. Additionally, the overlap between CNeuroMod tasks and other open data resources opens the possibility to test NeuroAI models transferability and generalization across subjects and datasets. \n\nMethods\n\nParticipants (aged 31 to 47 at the time of recruitment in 2018); 3 women and 3 men; 3 native French \u0026 1 native English speakers, 2 bilingual; all right-handed) consented to participate for at least 5 years of data collection. All participants had good general health and normal hearing for their age. Four subjects were scanned 80h+ / year and two were scanned 40h+ / year. FMRI data were acquired with a 3T scanner). Setup included physiological signal recording, headcases to minimize motion, and a custom-built fiber optic controller to play videogames (Harel et al., 2023). Data were preprocessed with the fMRIprep pipeline LTS (Esteban et al, 2019). \n\nThe CNeuroMod Databank\n\nThe databank includes 29 fMRI datasets targeting several cognitive domains and modalities. Per subject, there are 197h of fMRI data, 17h of anatomical MRI data (Boudreau et al, 2025), 29h of data collected outside the scanner, including longitudinal hearing measures (Fortier et al, 2025) and videogame (Shinobi) training. The following is a breakdown of the fMRI datasets by cognitive domains, per subject (Ss), Table 1).\nVision. 22 datasets (175h) including movies (10h), 7 seasons of TV-show Friends (70h), \u0026 functional localizers (Stigliani et al, 2015; Kay et al. 2013). Language. 19 datasets (73h), including listening to Le Petit Prince (3h; Li et al, 2022) in 3 languages, reading a chapter of harrypotter (Wehbe et al 2014), triplets is a  semantic association task involving triplets of words (7h), functional localizers (Scott et al, 2017 \u0026 Malik-Moralda et al 2021). Memory. 5 datasets (44h), including a memory task (18h) using THINGS (18h; Hebart et al. 2019), and  multfs (8h), which is a  working-memory task. Emotions. 13 datasets (147h), including gifs (5h) that evoke varying emotional dimensions (Cowen \u0026 Keltner, 2017). Auditory. 18 datasets (146h), including Mutemusic, which is an auditory imagery task, and narratives, a listening task (Nastase et al., 2018) with verbal recal inside the scanner. Videogames (48h). In-scan playing of videogames including Shinobi, SuperMario, Mariostars, and Mario3. OOD Algonauts (2h/Ss). A secret dataset acquired for the Algonauts 2025 challenge (Gifford et al., 2025).\n\nData access \u0026 release\n\nRaw and preprocessed fMRI data, behavioral responses and physiological recordings are formatted in BIDS (Gorgolewski, et al, 2016) and available via DataLad version control. After processing and quality checks, each dataset will be released independently with an accompanying data paper in the coming years. Data for 4 subjects is accessible without any restrictions (CC0 license) on the Canadian Open Neuroscience Platform's portal (https://portal.conp.ca/), while data for all subjects is available via registered access at https://www.cneuromod.ca/. \n\nConclusion\n\nCNeuroMod has assembled an unprecedented resource to model individual brain function using a wide range of controlled and naturalistic tasks. Data from the movie watching tasks are currently in use to assess the robustness of brain encoding models for the Algonauts 2025 challenge (Gifford et al., 2025). The wealth of additional data that will be released in the coming years will fuel novel insights into the ways human brains process complex stimuli.\n\nAcknowledgements\n\nThe Courtois NeuroMod project was made possible by a grant from la Fondation Courtois given to LPB."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "What governs the emergence of brain-like specialized neurons in artificial neural networks?",
    "presenter": "Brian S Robinson",
    "poster_id": "C164",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Neurons with specialized properties have been widely characterized across the brain. It is a broad question as to why this occurs, with computational theories often assuming a central role of nonlinear neuron activation functions and connectivity constraints. In this work, we characterize neuron-specialization in artificial neural networks by extending regression-based approaches for predicting experimentally recorded neural activation patterns. When investigating a range of performant artificial neural network architectures, we demonstrate that (1) brain-aligned specialized neurons can emerge in layers without nonlinear neuron activation functions, and (2) the emergence of brain-aligned specialized neurons depends on training properties, not strictly on architecture. Overall, this work suggests that new and complementary explanations for the emergence of specialized neurons in biological brains may be needed, such as processes underlying learning and optimization. Furthermore, this work motivates brain-to-model comparison techniques that respect and further investigate properties of neuron specialization. These results may additionally inform general interpretability approaches for artificial neural networks, where methods for obtaining units for inspection is an active area of research."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Common representations underlie idiosyncratic neural topographies",
    "presenter": "Bogdan Petre",
    "poster_id": "C165",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Primary sensory brain areas contain innate stereotyped topographies like retinotopy and somatotopy, but association cortex shows more idiosyncratic organization. Using a twin sample from the human connectome project and functional magnetic resonance imaging, we test the hypothesis that idiosyncrasies are the byproduct of convergent learning and subserve common functional representations.\n\nWe combined representational similarity analysis, full brain neuromaps and twin heritability models to investigate the spatial profiles and sources of representational and topographic similarity across diverse task conditions and at rest. We found common representational geometry with idiosyncratic topographies in both task evoked responses and resting state network organization. Common representations with idiosyncratic topographies were especially common in transmodal brain areas, late in the cortical hierarchy, but did not show consistent associations with neuromaps of genetic or developmental markers. Additionally, while topography was heritable, response geometry was not, indicating it was learned.\n\nThese findings are consistent with experience dependent but convergent circuit organization during development, and echoes learning principles in artificial neural networks where circuit weights are always idiosyncratic but learned representations are nevertheless predictable. This shows how similar principles affect brain organization. Differing topographies in association cortex are at least in part subtle implementation differences that underlie shared representations."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Comparing Brain-Score and ImageNet performance with responses to the scintillating grid illusion",
    "presenter": "Martin Kent Kraus",
    "poster_id": "C163",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Perceptual illusions are widely used to study brain processing, and are essential for elucidating underlying function. Successful brain models should then also be able to reproduce these illusions. Some of the most successful models for vision are several variants of Deep Neural Networks (DNNs). These models can classify images with human-level accuracy, and many behavioral and activation measurements correlate well with humans and animals. For several networks it was also shown that they can reproduce some human illusions. However, this was typically done for a limited number of networks. In addition, it remains unclear whether the presence of illusions is linked to either how accurate or brain-like the DNNs are. Here, we consider the scintillating grid illusion, to which two DNNs have been shown to respond as if they are impacted by the illusion. We develop a measure for measuring Illusion Strength based on model activation correlations, which takes into account the difference in Illusion Strength between illusion and control images. We then compare the Illusion Strength to both model performance (top-1 ImageNet), and how well the model explains brain activity (Brain-score). We show that the illusion was measurable in a wide variety of networks (41 out of 51). However, we do not find a strong correlation between Illusion Strength and Brain-Score, nor performance. Some models have strong illusion scores but not Brain-Score, or vice-versa, but no model does both well. Finally, this differs strongly between model types, particularly between convolutional and transformer-based architectures, with transformers having low illusion scores. Overall, our work shows that Illusion Strength measures an important metric, which is important to consider for assessing brain models, and that some models could still be missing out on some processing important for brain functioning."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "From simple to complex: shared learning dynamics in humans and neural networks",
    "presenter": "Christopher Summerfield",
    "poster_id": "C166",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Deep neural networks demonstrate a well-documented simplicity bias—the tendency to learn simple functions before acquiring more complex ones. This bias towards simplicity is thought to enable overparameterized models to successfully generalize to unseen data rather than overfitting to examples seen in training. Complementary work in psychology has demonstrated human simplicity biases in several domains. Here, we aim to unite these two streams by comparing human and neural network simplicity biases side-by-side in a Boolean classification task. We demonstrate that both humans and models initially learn simple rules before mastering a more complex function. We also provide evidence that human learners rely on the simple functions they learned early on to classify out-of-distribution examples, suggesting that dynamical simplicity biases are important for generalization."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Brain-Like Pathways Form in Models With Heterogeneous Experts",
    "presenter": "Jack Cook",
    "poster_id": "C167",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "The brain is made up of a vast set of heterogeneous brain regions that organize themselves into sub-networks and processing pathways to respond to task demands. Examples of such pathways can be seen in the ventral and dorsal visual streams, the Multiple-Demand Network during task execution, and the interaction between cortical and subcortical networks during learning. In this work we ask how do these processing pathways develop from a set of heterogeneous brain regions. Do regions automatically group into systems or are additional priors required? We study this by using neural networks, specifically by extending the Mixture-of-Expert architecture. We show that heterogeneous regions do not automatically form processing pathways by themselves. Training with a processing-complexity routing cost, when scaled based on task performance, results in the development of replicable processing pathways. When comparing our model to the brain, we observe that these pathways match how the brain utilizes different systems to learn and execute tasks of varying task complexities. Our findings establish specific biases that may underlie the formation of processing pathways observed in the brain. At the same time, our model allows us to conduct fine-grained analyses of how sets of pathways interact during problem solving across domains of neuroscience."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Arousal dynamics predict transitions in engagement state",
    "presenter": "Philippa A. Johnson",
    "poster_id": "C168",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "When completing a task for a prolonged period, animals switch between being engaged and disengaged in what they are doing. What neural and physiological processes trigger these behavioural state transitions? We estimate the engagement state of mice using a hidden Markov model of response times and found that intermediate arousal was associated with more engagement in the task. Additionally, we show that changes in arousal predict subsequent changes in behavioural state. To explain this, we propose a double-well model, in which arousal causes behavioural state transitions by reshaping the attractor landscape of population neural activity. These results highlight a possible mechanism of arousal-related changes in behaviour and suggest the presence of early warning signals for behavioural state switches."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Auditory stimuli extend the temporal window of visual integration by modulating alpha-band oscillations",
    "presenter": "Mengting Xu",
    "poster_id": "C169",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "In multisensory environments, how inputs from different sensory modalities interact to shape perception is not fully understood. In this study, we investigated how auditory stimuli influence the temporal dynamics of visual processing using electroencephalography (EEG). We found that the presence of auditory stimuli led to poststimulus alpha frequency degradation, which positively correlated with the prolonged temporal window of visual integration. This was accompanied by a diminished predictive role of prestimulus alpha frequency while enhancing the predictive role of prestimulus alpha phase in shaping perceptual outcomes. To probe the underlying mechanisms, we developed a computational model that successfully replicated the core findings and revealed that auditory input extends the temporal window of visual integration by resetting alpha oscillations in the visual cortex, leading to alpha frequency reduction and an altered perception of visual events."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Early learning of the optimal constant solution in neural networks and humans",
    "presenter": "Jirko Rubruck",
    "poster_id": "C170",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Deep neural networks learn increasingly complex functions over the course of training. Here, we show both empirically and theoretically that learning of the target function is preceded by an early phase in which networks learn the optimal constant solution (OCS) – that is, initial model responses mirror the distribution of target labels, while entirely ignoring information provided in the input. Using a hierarchical category learning task, we derive exact solutions for learning dynamics in deep linear networks trained with bias terms. Even when initialized to zero, this simple architectural feature induces substantial changes in early dynamics. We identify hallmarks of this early OCS phase and illustrate how these signatures are observed in deep linear networks and larger, nonlinear convolutional neural networks solving a hierarchical learning task based on MNIST and CIFAR10. We train human learners over the course of three days on a structurally equivalent learning task. We then identify qualitative signatures of this early OCS phase in terms of true negative rates. Surprisingly, we find the same early reliance on the OCS in the behavior of human learners. Finally, we show that learning of the OCS can emerge even in the absence of bias terms and is equivalently driven by generic correlations in the input data. Overall, our work suggests the OCS is a common phenomenon in biological and artificial, supervised, error-corrective learning, and suggests possible factors for its prevalence."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Traveling Waves Integrate Spatial Information Through Time",
    "presenter": "Mozes Jacobs",
    "poster_id": "C171",
    "topic_area": "Brain Networks and Neural Dynamics",
    "abstract": "Traveling waves of neural activity are widely observed in the brain, but their precise computational function remains unclear. One prominent hypothesis is that they enable the transfer and integration of spatial information across neural populations. However, few computational models have explored how traveling waves might be harnessed to perform such integrative processing. Drawing inspiration from the famous “Can one hear the shape of a drum?” problem -- which highlights how normal modes of wave dynamics encode geometric information -- we investigate whether similar principles can be leveraged in artificial neural networks. Specifically, we introduce convolutional recurrent neural networks that learn to produce traveling waves in their hidden states in response to visual stimuli, enabling spatial integration. By then treating these wave-like activation sequences as visual representations themselves, we obtain a powerful representational space that outperforms local feed-forward networks on tasks requiring global spatial context. In particular, we observe that traveling waves effectively expand the receptive field of locally connected neurons, supporting long-range encoding and communication of information. We demonstrate that models equipped with this mechanism solve visual semantic segmentation tasks demanding global integration, significantly outperforming local feed-forward models and rivaling non-local U-Net models with fewer parameters. As a first step toward traveling-wave-based communication and visual representation in artificial networks, our findings suggest wave-dynamics may provide efficiency and training stability benefits, while simultaneously offering a new framework for connecting models to biological recordings of neural activity."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "The Landscape of Neuroscience",
    "presenter": "Mario Senden",
    "poster_id": "C172",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Neuroscience emerged as a distinct academic discipline during the 20th century and has undergone rapid expansion and diversification. This study leverages text-embedding and clustering techniques together with large language models to analyze 461,316 articles published between 1999 and 2023 to provide a snapshot of neuroscience's landscape. Inter-cluster citation analysis uncovers a surprisingly integrated picture. An analysis of how research clusters align with pre-defined dimensions demonstrates a strong experimental focus, widespread reliance on specific mechanistic explanations rather than unifying theoretical frameworks, and a growing shift towards applied research."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Efficient Regularization of High-Dimensional Cerebellar Representations by Sparse Parallel Fiber Inputs: A Virtual Sample-based L2 Regularization Perspective",
    "presenter": "Tae-Rim Yun",
    "poster_id": "C173",
    "topic_area": "Methods and Computational Tools",
    "abstract": "The cerebellum generates extremely high-dimensional representations through parallel fibers (PF) originating from the granule cell layer, enabling precise motor learning and predictive control. However, this excessive dimensionality expansion potentially risks overfitting due to surpassing the intrinsic dimensionality of the input data. This study proposes that the spontaneous and highly sparse PF inputs serve as explicit virtual data samples that effectively implement an L2 regularization mechanism analogous to multiple linear regression. Specifically, the sparse PF inputs mathematically resemble virtual samples, each having a single feature with value $\\sqrt{\\lambda}$ and target output y=0. Improper activation of Purkinje cells (PC) by these sparse inputs triggers error signals via climbing fibers (CF), consequently inducing long-term depression (LTD) at PF-PC synapses. This interpretation extends traditional adaptive filter theories based on the delta learning rule and integrates recent perspectives of spontaneous activity-driven pruning in generative models. Experimental validations using optogenetics and electrophysiology are proposed."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Constructing Representational Similarity Metrics Through Linear Decoding",
    "presenter": "Sarah E Harvey",
    "poster_id": "C174",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Neural responses encode information that is useful for a variety of downstream tasks; however, many methods for comparing neural representations do not explicitly leverage this perspective and instead highlight geometric invariances. Here, we show that many representational similarity measures can be equivalently motivated from a decoding perspective. Specifically, measures like CKA and CCA are shown to quantify the average alignment between optimal linear readouts across a distribution of decoding tasks. This approach suggests a metric on neural representations in which the distance between representations directly quantifies differences in the decoding of neural data. We demonstrate this in an ensemble of DNNs trained for image classification and human fMRI representations from the Natural Scenes Dataset. Our work demonstrates a tight link between the geometry of neural representations and the ability to linearly decode information. This perspective suggests new ways of measuring similarity between neural systems and also provides novel, unifying interpretations of existing measures."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "A hierarchical multivariate copula-based framework for cognitive modeling",
    "presenter": "Jesper Fischer Ehmsen",
    "poster_id": "C176",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Computational cognitive models provide an approach to understanding behavior and cognition by formalizing latent parameters underlying decision-making and learning. Many existing models take a univariate approach, analyzing single measures in isolation, while others incorporate multiple measures but impose specific process assumptions that constrain how these measures relate i.e. drift diffusion models. Here, we introduce a hierarchical multivariate modeling framework that uses copulas to flexibly combine independent likelihood functions, enabling joint modeling of multiple measures without imposing restrictive assumptions. Through simulations and empirical applications, we assess the reliability, discriminability, and advantages of copula-based modeling (CBM). Model validation via simulation-based calibration, model recovery, and sensitivity analyses demonstrate that CBM is computationally robust and accurately recovers latent parameters and their uncertainty. When applied to psychophysical and probabilistic learning tasks, CBM can be empirically distinguished from DDMs, even with limited data. We show that this framework enables efficient use of available data by integrating multiple sources of information, while enhancing model accuracy and efficiency of parameter estimation."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Longitudinal Multimodal Data Fusion Reveals Brain–Symptom Change Patterns in Depression",
    "presenter": "Effat Salehi Far",
    "poster_id": "C175",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Understanding how brain structure and psychopathology co-evolve over time is central to unravel individual variability in affective disorders. We applied multiset canonical correlation analysis followed by joint independent component analysis (mCCA + jICA) to longitudinal data from 105 participants (48 healthy controls, 57 with major depressive disorder, MDD), using cortical surface area (SA), cortical thickness (CT), and symptom checklist-90-revised (SCL-90) symptom scores as input modalities. Five joint components were extracted. Two components revealed significant brain–symptom change associations involving depression-dominant SCL-90 profiles. Regions showing progressive reduction in SA and CT overlapped with those reported by ENIGMA studies in adolescent and adult MDD, respectively. Correlations between structural and symptom component loadings were stronger in the MDD subgroup than in controls, highlighting diagnosis-specific change trajectories. Our results support multimodal fusion as a promising approach to identify clinically meaningful brain–symptom change patterns and better understand brain–behavior coupling in affective disorders."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Variance explained by different model components does not behave like a Venn diagram: Why variance decomposition provides misleading intuitions",
    "presenter": "Jinkang Derrick Xiang",
    "poster_id": "C177",
    "topic_area": "Methods and Computational Tools",
    "abstract": "When explaining brain responses $Y$ with a set of predictors, the variance of $Y$ is often decomposed into portions explained by each predictor, as a reflection of their contribution. The explained variance is commonly visualized using Venn diagrams. This approach originates from Fisher's ANOVA where some of the variance of a variable $Y$ can be explained by orthogonal predictors, and the variance explained by the predictors together is the sum of the variance explained by each one alone. However, in neuroscience applications, the predictors are often correlated, which could cause the variance explained by two predictors to be smaller than, equal to or greater than the sum of each alone. Variance is not a fixed quantity of the data that can be decomposed, but should be considered in the context of all model components. We provide an alternative to the commonly used Venn diagram to visualize variance explained, and will provide an analytical framework to quantitatively conduct model selection and comparison for RSA, PCM and encoding models."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Comparing Variance Partitioning and the Residual Method for Interpreting Brain Recordings",
    "presenter": "Fatma Deniz",
    "poster_id": "C179",
    "topic_area": "Methods and Computational Tools",
    "abstract": "A shift toward more naturalistic experiments in computational cognitive neuroscience has enabled a richer analysis of brain recordings. These naturalistic experiments often allow for the extraction of multiple feature spaces from stimuli, helping better explain variance in voxelwise encoding models. Two key methods for determining the unique contribution of each feature space to the variance explained are variance partitioning and the residual method. However, no systematic comparison has been conducted to assess their suitability and properties. To address that gap, this work compares both methods by evaluating them in simulated and real-world experiments and comparing their results. Our findings reveal that both variance partitioning and the residual method can effectively determine the unique variance a feature space explains. However, the residual method requires careful verification of the linear dependence between feature spaces, a step that variance partitioning does not need."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Boosting Hyperalignment Performance with Age-specific Templates",
    "presenter": "Yuqi Zhang",
    "poster_id": "C178",
    "topic_area": "Methods and Computational Tools",
    "abstract": "The brain undergoes significant developmental and functional changes over the lifespan, and certain features in brain functional organization may be more prominent in certain age groups than others. Due to individual differences in functional–anatomical correspondence, features encoded in fine-grained spatial patterns need to be functionally aligned using hyperalignment. In this work, we examine whether age-specific functional templates improve hyperalignment. We built age-specific templates using the Cambridge Centre for Ageing and Neuroscience (Cam-CAN) dataset (18–87 yo) and evaluated their performance. We found that congruent age-specific templates improve the performance of (a) predicting individualized connectomes, (b) predicting individualized brain responses during movie watching, and (c) inter-subject correlation of connectivity profiles. This work enhances our understanding of age-related differences in brain function, highlights the benefits of creating age-specific templates to refine hyperalignment model performance, and may contribute to the development of age-sensitive diagnostic tools and interventions for neurological disorders."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Intrinsic dimensionality of brain activity manifolds across tasks and development",
    "presenter": "Erica Lindsey Busch",
    "poster_id": "C181",
    "topic_area": "Methods and Computational Tools",
    "abstract": "To investigate cognitive processing in the brain, researchers often focus on the localization or amplitude of fMRI activity. Intrinsic dimensionality – the degrees of freedom needed to characterize complex signals – may offer additional insights into where and how computations occur across the brain. Here, we investigated how tasks and development alter the intrinsic dimensionality of brain activity. We used a nonlinear manifold learning method to estimate the intrinsic dimensionality of fMRI activity in whole-brain searchlights from infant (3–24 months), child (3–12 years), and adult datasets during resting and naturalistic fMRI tasks (i.e., movies or narratives). These analyses revealed two principles of intrinsic dimensionality in the human brain: task processing reduces the intrinsic dimensionality of task-irrelevant brain regions relative to rest and development increases the intrinsic dimensionality of task-relevant brain regions. These findings hold implications for understanding information processing in the brain and studying the development of adult-like brain functioning."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "SenseNet: Neural Architecture Search Inspired by Adaptive Biological Sensing for Transparency and Adaptability",
    "presenter": "Kirtana Sunil Phatnani",
    "poster_id": "C182",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Neural Architecture Search (NAS) has emerged as a key method for automating neural network design. However, most existing approaches rely on static optimization strategies that struggle to adapt to new tasks, incorporate real-time feedback, or explain their decision processes—limitations that hinder performance in dynamic environments. Inspired by evolutionary phenomena and human learning — where organisms develop or adapt sensory systems to interpret and act on environmental cues—this work explores how NAS can incorporate similar mechanisms to improve adaptability and dynamic search strategy. This paper presents a hybrid approach called SenseNet that combines large language model (LLM)-driven explainability, dynamic optimization, and real-time adaptability to enhance NAS decision-making. At its core, SenseNet features a meta-controller that dynamically selects high-level strategy - exploration, exploitation, or balanced — based on sensing environmental cues, while an ML strategist translates these high-level strategies into tailored crossover and mutation operations that guide architecture evolution effectively. We evaluate SenseNet comprehensively on NATS-Bench and TransNAS-Bench, demonstrating its adaptability and effectiveness. Our experiments show that SenseNet achieves state-of-the-art results on NATS-Bench and performs competitively on TransNAS-Bench. By embedding sensing and response mechanisms into NAS, SenseNet enhances both efficiency and transparency in neural network optimization, shifting the paradigm from rigid search techniques to biologically inspired, self-adaptive, and transparent neural network optimization."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Using transfer learning to identify a neural network's algorithm",
    "presenter": "John Morrison",
    "poster_id": "C183",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Algorithms generate input-output mappings through operations on representations. In cognitive science, we use algorithms to explain cognitive processes. For example, we use tree-search algorithms to explain planning, reinforcement learning algorithms to explain exploration, and Bayesian algorithms to explain categorization. To what extent do these algorithms describe processes in the brain? The standard method is to look for parts in the brain that correspond to the parts of an algorithm. However, we haven't found many algorithms using this method. This has led some to view cognitive science algorithms as merely normative, indicating the ideal input-output mapping without describing operations in the brain. It has led others to view these algorithms are nothing more than useful fictions; useful insofar as they allow us to predict behavior, but fictional insofar as they inaccurately describe the causes of that behavior. As an alternative, we suggest identifying a neural system's algorithm by assessing how quickly it learns alternative input-output mappings, that is, its transfer learning profile. The basic idea is that, depending on which algorithm is being used, different input-output mappings will be easier to learn, allowing us to recover its original algorithm from its transfer learning profile. We use artificial neural networks to demonstrate that this proposal productively applies to multiple networks and tasks. We conclude that transfer learning is a promising approach for integrating algorithms with neural networks and thus for integrating cognitive science with systems neuroscience and machine learning."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Estimating Neural Representation Alignment from Sparsely Sampled Inputs and Features",
    "presenter": "Chanwoo Chun",
    "poster_id": "C184",
    "topic_area": "Methods and Computational Tools",
    "abstract": "In both artificial and biological systems, the centered kernel alignment (CKA) has become a widely used tool for quantifying neural representation similarity. While current CKA estimators typically correct for the effects of finite stimuli sampling, the effects of sampling a subset of neurons are overlooked, introducing notable bias in standard experimental scenarios. Here, we provide a theoretical analysis showing how this bias is affected by the representation geometry. We then introduce a novel estimator that corrects the bias for both input and feature sampling. We use our method for evaluating both brain-to-brain and model-to-brain alignments and show that it delivers reliable comparisons even with very sparsely sampled neurons. We perform within-animal and across-animal comparisons on electrophysiological data from visual cortical areas V1, V4, and IT, and use these as benchmarks to evaluate model-to-brain alignment. \nWe also apply our method to reveal how object representations become progressively disentangled across layers in both biological and artificial systems. These findings underscore the importance of correcting feature-sampling biases in CKA and demonstrate that our bias-corrected estimator provides a more faithful measure of representation alignment. The improved estimates increase our understanding of how neural activity is structured across both biological and artificial systems."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Evaluating Representational Similarity Measures from the Lens of Functional Correspondence",
    "presenter": "Yiqing Bo",
    "poster_id": "C185",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Neuroscience and artificial intelligence (AI) both grapple with the challenge of interpreting high-dimensional neural data. Comparative analysis of such data is essential to uncover shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the ever-growing landscape of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? Prior work often evaluates metrics by their ability to differentiate models with varying origins (e.g., different architectures), but an alternative—and arguably more informative—approach is to assess how well these metrics distinguish models with distinct behaviors. This is crucial as representational comparisons are frequently interpreted as indicators of functional similarity in NeuroAI. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes in a suite of different downstream data distributions and tasks. We compared eight commonly used metrics in the visual domain, including alignment-based, CCA-based, inner product kernel-based, and nearest-neighbor-based methods, using group statistics and a comprehensive set of behavioral metrics. We found that metrics like the Procrustes distance and linear Centered Kernel Alignment (CKA), which emphasize alignment in the overall shape or geometry of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These findings highlight that some widely used representational similarity metrics may not directly map onto functional behaviors or computational goals, underscoring the importance of selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research."
  },
  {
    "start": "2025-08-15T02:00:00Z",
    "end": "2025-08-15T17:00:00Z",
    "title": "Learning Latent Spaces for Individualized Functional Neuroimaging with Variational Autoencoders",
    "presenter": "Kajal Singla",
    "poster_id": "C180",
    "topic_area": "Methods and Computational Tools",
    "abstract": "Functional Magnetic Resonance Imaging (fMRI) studies often use dimensionality reduction methods like independent component analysis or diffusion map embedding to identify group-level brain networks and dynamics. These approaches struggle to capture individual-specific differences. To address this gap, we explore the use of variational autoencoders (VAEs) to model Blood Oxygen Level Dependent (BOLD) signals in a subject-specific latent space. Our approach effectively denoises fMRI data using a compressed, low-dimensional latent representation, enhancing the separation of signals from distinct functional networks without directly aligning them to specific latent axes. While direct alignment of latent dimensions across subjects is not straightforward, we observe shared geometric patterns across subjects’ latent spaces, enabling meaningful cross-subject comparisons. Deep latent modeling offers a promising avenue for individualized fMRI analysis, providing new insights into the brain’s complex functional architecture."
  }
]